{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Grapher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1805a-55dc-409c-9ccf-512eb1f88667",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8ec1d-73be-4b8c-95dc-79489e84998b",
   "metadata": {},
   "source": [
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located.\n",
    "\n",
    "- `pos`: This is the position of the genetic variant on the chromosome.\n",
    "\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant.\n",
    "\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population.\n",
    "\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group.\n",
    "\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group.\n",
    "\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0). \n",
    "\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 64 bit (AMD64)]\n",
      "NumPy version: 1.24.3\n",
      "Pandas version: 2.0.1\n",
      "Matplotlib version: 3.7.1\n",
      "Scikit-learn version: 1.2.2\n",
      "Torch version: 2.0.0+cu118\n",
      "Torch Geometric version: 2.3.1\n",
      "NetworkX version: 3.0\n",
      "Using NVIDIA GeForce RTX 3060 Ti (cuda)\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import expit\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfa8cb-01df-491f-94f0-c8c228256b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "data = pd.read_csv('~/Desktop/gwas-graph/FinnGen/data/gwas-finemap.csv', dtype=dtypes)\n",
    "\n",
    "# Assert column names\n",
    "expected_columns = ['#chrom', 'pos', 'ref', 'alt', 'rsids', 'nearest_genes', 'pval', 'mlogp', 'beta',\n",
    "                    'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls', 'finemapped',\n",
    "                    'id', 'trait']\n",
    "assert set(data.columns) == set(expected_columns), \"Unexpected columns in the data DataFrame.\"\n",
    "\n",
    "# Assert data types\n",
    "expected_dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "for col, expected_dtype in expected_dtypes.items():\n",
    "    assert data[col].dtype == expected_dtype, f\"Unexpected data type for column {col}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d6d595-4ef8-41d5-b0a5-dbdc9f2d9b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values in each column:\n",
      "#chrom                   0\n",
      "pos                      0\n",
      "ref                      0\n",
      "alt                      0\n",
      "rsids              1366396\n",
      "nearest_genes       727855\n",
      "pval                     0\n",
      "mlogp                    0\n",
      "beta                     0\n",
      "sebeta                   0\n",
      "af_alt                   0\n",
      "af_alt_cases             0\n",
      "af_alt_controls          0\n",
      "id                       0\n",
      "finemapped               0\n",
      "trait                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for total number of null values in each column\n",
    "null_counts = data.isnull().sum()\n",
    "\n",
    "print(\"Total number of null values in each column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513cafa0-f3f8-47b5-9d22-96062449edde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fab1f-46fa-40a5-a8c5-aa746b1db7eb",
   "metadata": {},
   "source": [
    "### Find nearest gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assert column 'nearest_genes' is a string\n",
    "assert data['nearest_genes'].dtype == 'object', \"Column 'nearest_genes' is not of string type.\"\n",
    "\n",
    "# Get the length of the data before transformation\n",
    "original_length = len(data)\n",
    "\n",
    "# Extract the first gene name from the 'nearest_genes' column\n",
    "data['nearest_genes'] = data['nearest_genes'].str.split(',').str[0]\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Assert the length of the data remains the same\n",
    "assert len(data) == original_length, \"Length of the data has changed after transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "Here's the adjusted spec in markdown format:\n",
    "\n",
    "### Data\n",
    "\n",
    "`data` Pandas DataFrame:\n",
    "\n",
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located.\n",
    "- `pos`: This is the position of the genetic variant on the chromosome (int: 1-200,000).\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant (string).\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population (float: 0-1.\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group (float: 0-1).\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group (float: 0-1).\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0) (int).\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs.\n",
    "\n",
    "### Task Overview\n",
    "\n",
    "The objective is to design and implement a binary node classification GNN model to predict whether variants are included after post-finemapping or not based on `finemapping`.\n",
    "\n",
    "### Nodes and Their Features\n",
    "\n",
    "There is one type of node: SNP nodes.\n",
    "\n",
    "- **SNP Nodes**: Each SNP Node is characterized by various features, including `id`, `nearest_genes`, `#chrom`, `pos`, `ref`, `alt`, `mlogp`, `beta`, `sebeta`,  `af_alt`, `af_alt_cases`, and `af_alt_controls` columns.\n",
    "\n",
    "### Edges, Their Features, and Labels\n",
    "\n",
    "Edges represent relationships between SNP nodes in the graph.\n",
    "\n",
    "1. **Type 1 Edges: LD-based edges**\n",
    "\n",
    "   - For each pair of SNPs (row1 and row2) that exist on the same chromosome (`#chrom`), an edge is created if the absolute difference between their positions (`pos`) is less than or equal to 500,000 and greater than 1 (no loops).\n",
    "   - The weight of the edge is determined by the following formula:\n",
    "     \n",
    "```\n",
    "    weights = (average_mlogp / (1 + pos_diff_abs * \\\n",
    "                      af_alt_diff_abs * \\\n",
    "                      af_alt_cases_diff_abs * \\\n",
    "                      af_alt_controls_diff_abs))\n",
    "```\n",
    "\n",
    "    - For each chromosome, standardize the edge weights between 0 and 1 after all weights have been computed.\n",
    "    - Prune any edges that have a weight of less than `1e-3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51485cfb-989a-430f-874b-af2d5f79d9f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68257919-e9e4-4aa7-8b45-d0de2dbd765b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 20170\n",
      "Number of edges: 36767\n",
      "Node feature dimension: 26\n",
      "Execution time: 55.78683924674988 seconds\n",
      "         112926481 function calls (110616940 primitive calls) in 55.778 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1279 to 3 due to restriction <3>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       14    0.000    0.000   55.786    3.985 C:\\Users\\falty\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3472(run_code)\n",
      "       14    0.000    0.000   55.786    3.985 {built-in method builtins.exec}\n",
      "        1    0.750    0.750   55.512   55.512 C:\\Users\\falty\\AppData\\Local\\Temp\\ipykernel_2092\\966270683.py:44(preprocess_positive_edges)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edge_weight_cutoff = 1e-3  # set the cutoff value, this is an example and you can choose any suitable value\n",
    "\n",
    "\n",
    "def get_unique_snps(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Function to create mappings for SNPs to integer indices.\n",
    "    \"\"\"\n",
    "    return {snp: idx for idx, snp in enumerate(data['id'].unique())}\n",
    "\n",
    "\n",
    "def preprocess_snp_features(data: pd.DataFrame, snp_to_idx: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to create node feature vectors for SNPs and preprocess categorical and numerical features.\n",
    "    \"\"\"\n",
    "    # Ensure that 'id' exists in the data and 'id' and 'nearest_genes' are not null\n",
    "    assert 'id' in data.columns and 'nearest_genes' in data.columns, \"Columns 'id' or 'nearest_genes' do not exist in the dataframe\"\n",
    "    assert data[['id', 'nearest_genes']].isnull().sum().sum() == 0, \"Columns 'id' or 'nearest_genes' contain null values\"\n",
    "    \n",
    "    # Columns to be extracted from the original dataframe\n",
    "    cols_to_extract = ['id', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls']\n",
    "    \n",
    "    snp_features = data.loc[data['id'].isin(snp_to_idx.keys()), cols_to_extract].set_index('id').sort_index()\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Frequency encoding for 'nearest_genes' using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    snp_features['nearest_genes'] = label_encoder.fit_transform(snp_features['nearest_genes'])\n",
    "    \n",
    "    categorical_cols = ['ref', 'alt']\n",
    "\n",
    "    # BinaryEncoder for 'ref', 'alt'\n",
    "    binary_encoder = ce.BinaryEncoder(cols=categorical_cols, drop_invariant=True)\n",
    "    snp_features = binary_encoder.fit_transform(snp_features)\n",
    "\n",
    "    numerical_cols = list(set(snp_features.columns) - set(categorical_cols))\n",
    "    snp_features[numerical_cols] = scaler.fit_transform(snp_features[numerical_cols])\n",
    "    \n",
    "    # Filling 0 values for all columns\n",
    "    snp_features = snp_features.fillna(0)\n",
    "\n",
    "    return snp_features\n",
    "\n",
    "\n",
    "def preprocess_positive_edges(data: pd.DataFrame, snp_to_idx: dict) -> torch.Tensor:\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Sort data once before grouping\n",
    "    data = data.sort_values(by=['#chrom', 'pos'])\n",
    "\n",
    "    # Create new column for SNP index\n",
    "    data['snp_idx'] = data['id'].map(snp_to_idx)\n",
    "\n",
    "    positive_edges_snp_snp = []\n",
    "    snp_weights = []\n",
    "\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        # Skip if group is empty\n",
    "        if group.empty:\n",
    "            continue\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "            pos_diff = group['pos'] - row['pos']\n",
    "            mask = (pos_diff > 1) & (pos_diff <= 500000)\n",
    "            filtered_group = group[mask]\n",
    "\n",
    "            if filtered_group.empty:\n",
    "                continue\n",
    "            \n",
    "            af_alt_diff = abs(row['af_alt'] - filtered_group['af_alt'])\n",
    "            pos_diff_abs = abs(pos_diff[mask])\n",
    "            average_mlogp = (row['mlogp'] + filtered_group['mlogp']) / 2\n",
    "            weights = (average_mlogp / (1 + pos_diff_abs * af_alt_diff  * \\\n",
    "                      abs(row['af_alt_cases'] - filtered_group['af_alt_cases']) * \\\n",
    "                      abs(row['af_alt_controls'] - filtered_group['af_alt_controls'])))\n",
    "\n",
    "            # Skip if weights are empty\n",
    "            if len(weights) == 0:\n",
    "                continue\n",
    "\n",
    "            positive_edges_snp_snp.extend(zip([row['snp_idx']] * len(filtered_group), filtered_group['snp_idx']))\n",
    "            snp_weights.extend(weights)\n",
    "\n",
    "    # Normalize weights\n",
    "    snp_weights = scaler.fit_transform(np.array(snp_weights).reshape(-1, 1)).flatten()\n",
    "    mask_cutoff = snp_weights >= edge_weight_cutoff\n",
    "    filtered_edges = np.array(positive_edges_snp_snp)[mask_cutoff]\n",
    "    filtered_weights = snp_weights[mask_cutoff]\n",
    "\n",
    "    return torch.tensor(filtered_edges, dtype=torch.long).t().contiguous(), torch.tensor(filtered_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "def create_pytorch_graph(features: torch.Tensor, edges: torch.Tensor, edge_weights: torch.Tensor) -> Data:\n",
    "    return Data(x=features, edge_index=edges, edge_attr=edge_weights)\n",
    "\n",
    "\n",
    "# Add profiling\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Main\n",
    "snp_to_idx = get_unique_snps(data)\n",
    "labels = data['finemapped'].map(lambda x: 1 if x > 0 else 0)\n",
    "snp_features = preprocess_snp_features(data, snp_to_idx)\n",
    "features = torch.tensor(snp_features.values, dtype=torch.float)\n",
    "\n",
    "positive_edges_snp_snp, snp_weights = preprocess_positive_edges(data, snp_to_idx)\n",
    "graph = create_pytorch_graph(features, positive_edges_snp_snp, snp_weights)\n",
    "graph.y = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats(3)  # Only print the top 5 lines\n",
    "print(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save/Load graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd9e725-2545-4820-903c-a68a9a9d73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PyTorch Geometric graph\n",
    "\n",
    "#torch.save(graph, \"pytorch_geometric_graph.pt\")\n",
    "#graph = torch.load(\"pytorch_geometric_graph.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4462c-4130-4242-9a7d-09c6ed71b613",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba52c6ce-3f9b-43f5-b3d9-b12fef9ef3ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph stats:\n",
      "Number of nodes: 20170\n",
      "Number of edges: 36767\n",
      "Node feature dimension: 26\n",
      "Number of finemapped nodes: 3020\n",
      "Average Degree: 1.8228557109832764\n",
      "Median Degree: 2.0\n",
      "Standard Deviation of Degree: 1.7116793394088745\n",
      "Density: 0.0001807582\n",
      "Features with NaN values:\n",
      "[]\n",
      "Edge weight stats:\n",
      "Number of edges: 36767\n",
      "Average edge weight: 1.2632436752319336\n",
      "Median edge weight: 0.9518038034439087\n",
      "Standard deviation of edge weights: 1.2150322198867798\n",
      "Maximum edge weight: 34.015201568603516\n",
      "Minimum edge weight: 0.0010152175091207027\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "def print_graph_stats(graph, positive_edges_snp_snp, features_list):\n",
    "    print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "    print(f\"Number of edges: {graph.num_edges}\")\n",
    "    print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "    print(f\"Number of finemapped nodes: {data['finemapped'].sum()}\")\n",
    "\n",
    "    # Compute and print degree-related stats\n",
    "    degrees = degree(graph.edge_index[0].long(), num_nodes=graph.num_nodes)\n",
    "    average_degree = degrees.float().mean().item()\n",
    "    median_degree = np.median(degrees.numpy())\n",
    "    std_degree = degrees.float().std().item()\n",
    "\n",
    "    print(f\"Average Degree: {average_degree}\")\n",
    "    print(f\"Median Degree: {median_degree}\")\n",
    "    print(f\"Standard Deviation of Degree: {std_degree}\")\n",
    "\n",
    "    # Density is the ratio of actual edges to the maximum number of possible edges\n",
    "    num_possible_edges = graph.num_nodes * (graph.num_nodes - 1) / 2\n",
    "    density = graph.num_edges / num_possible_edges\n",
    "\n",
    "    print(f\"Density: {density:.10f}\")\n",
    "\n",
    "    # Check for NaN values in features\n",
    "    nan_mask = torch.isnan(graph.x)\n",
    "    nan_features = []\n",
    "    for feature_idx, feature_name in enumerate(features_list):\n",
    "        if nan_mask[:, feature_idx].any():\n",
    "            nan_features.append(feature_name)\n",
    "\n",
    "    print(\"Features with NaN values:\")\n",
    "    print(nan_features)\n",
    "\n",
    "def print_edge_weight_stats(edge_weights):\n",
    "    print(f\"Number of edges: {edge_weights.size(0)}\")\n",
    "    print(f\"Average edge weight: {edge_weights.float().mean().item()}\")\n",
    "    print(f\"Median edge weight: {np.median(edge_weights.numpy())}\")\n",
    "    print(f\"Standard deviation of edge weights: {edge_weights.float().std().item()}\")\n",
    "    print(f\"Maximum edge weight: {edge_weights.max().item()}\")\n",
    "    print(f\"Minimum edge weight: {edge_weights.min().item()}\")\n",
    "\n",
    "# Print graph stats\n",
    "print(\"Graph stats:\")\n",
    "print_graph_stats(graph, positive_edges_snp_snp, snp_features.columns)\n",
    "\n",
    "print(\"Edge weight stats:\")\n",
    "print_edge_weight_stats(graph.edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51e96d-660c-4b4f-8ac4-9a07dba5bcc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c79fb79d-4729-4d4d-8dff-229108b6cac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes per class in each set:\n",
      "Train set:\n",
      "Class 0: 10248 nodes\n",
      "Class 1: 1854 nodes\n",
      "Validation set:\n",
      "Class 0: 3434 nodes\n",
      "Class 1: 600 nodes\n",
      "Test set:\n",
      "Class 0: 3468 nodes\n",
      "Class 1: 566 nodes\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "transform = RandomNodeSplit(split=\"train_rest\", num_val=0.2, num_test=0.2, key='y')\n",
    "graph = transform(graph)\n",
    "\n",
    "# Count the number of nodes per class in each set\n",
    "train_class_counts = Counter(graph.y[graph.train_mask].numpy())\n",
    "val_class_counts = Counter(graph.y[graph.val_mask].numpy())\n",
    "test_class_counts = Counter(graph.y[graph.test_mask].numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of nodes per class in each set:\")\n",
    "print(\"Train set:\")\n",
    "for class_label, count in train_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Validation set:\")\n",
    "for class_label, count in val_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Test set:\")\n",
    "for class_label, count in test_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665050a-5cd9-4960-a2ef-8a4cf2cb5e40",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358aee2-41a3-4301-ab75-23ccb0ff75a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "860691e0-192e-4cac-b43a-8a104748c126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1532, 0.8468], device='cuda:0')\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 1, Loss: 0.0403727069, Train ROC AUC: 0.4882356870, Val ROC AUC: 0.5003069792, Test ROC AUC: 0.4829738121\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 2, Loss: 0.0402203985, Train ROC AUC: 0.4884991636, Val ROC AUC: 0.5001739953, Test ROC AUC: 0.4835459282\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 3, Loss: 0.0400897264, Train ROC AUC: 0.4887786403, Val ROC AUC: 0.5000400408, Test ROC AUC: 0.4842693521\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 4, Loss: 0.0399211049, Train ROC AUC: 0.4891442496, Val ROC AUC: 0.5000550864, Test ROC AUC: 0.4848957251\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 5, Loss: 0.0398151912, Train ROC AUC: 0.4895148063, Val ROC AUC: 0.5000342167, Test ROC AUC: 0.4853789416\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 6, Loss: 0.0396142043, Train ROC AUC: 0.4898943631, Val ROC AUC: 0.5002227723, Test ROC AUC: 0.4857992407\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 7, Loss: 0.0394791439, Train ROC AUC: 0.4902524459, Val ROC AUC: 0.5004778198, Test ROC AUC: 0.4859627753\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 8, Loss: 0.0393279120, Train ROC AUC: 0.4905565282, Val ROC AUC: 0.5003664337, Test ROC AUC: 0.4858878856\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 9, Loss: 0.0392133333, Train ROC AUC: 0.4908788738, Val ROC AUC: 0.5002137934, Test ROC AUC: 0.4857584844\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 10, Loss: 0.0391126201, Train ROC AUC: 0.4912407199, Val ROC AUC: 0.4996177927, Test ROC AUC: 0.4856211867\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 11, Loss: 0.0388299525, Train ROC AUC: 0.4915293283, Val ROC AUC: 0.4989633081, Test ROC AUC: 0.4855287209\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 12, Loss: 0.0387370512, Train ROC AUC: 0.4917767784, Val ROC AUC: 0.4984148709, Test ROC AUC: 0.4856576127\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 13, Loss: 0.0387439877, Train ROC AUC: 0.4920895450, Val ROC AUC: 0.4977615997, Test ROC AUC: 0.4857279172\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 14, Loss: 0.0385426991, Train ROC AUC: 0.4925241287, Val ROC AUC: 0.4969697632, Test ROC AUC: 0.4858534975\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 15, Loss: 0.0383853279, Train ROC AUC: 0.4932186889, Val ROC AUC: 0.4960908076, Test ROC AUC: 0.4858506955\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 16, Loss: 0.0382746048, Train ROC AUC: 0.4940099607, Val ROC AUC: 0.4950982819, Test ROC AUC: 0.4860335893\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Classes predicted: [0 1]\n",
      "Epoch: 17, Loss: 0.0381613001, Train ROC AUC: 0.4947896798, Val ROC AUC: 0.4942710153, Test ROC AUC: 0.4865211362\n",
      "Early stopping!\n",
      "Classes predicted: [0 1]\n",
      "After early stopping, Test ROC AUC: 0.4859627753\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_layers, num_classes):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_node_features, hidden_layers[0])\n",
    "        self.conv2 = SAGEConv(hidden_layers[0], hidden_layers[1])\n",
    "        self.conv3 = SAGEConv(hidden_layers[1], num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        return torch.sigmoid(x.view(-1))\n",
    "\n",
    "# Use the GraphSAGE model\n",
    "hidden_layers = [64, 64]  \n",
    "model = GraphSAGEModel(graph.num_node_features, hidden_layers, 1).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.2)\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets.long()].view(-1, 1)\n",
    "            logpt = -F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "            logpt = logpt * alpha_t\n",
    "        else:\n",
    "            logpt = -F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        pt = torch.exp(logpt)\n",
    "        F_loss = -((1 - pt) ** self.gamma) * logpt\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "class_counts = graph.y[graph.train_mask].bincount(minlength=2).float()\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = class_weights.detach().to(device)  # Add to(device)\n",
    "print(class_weights)\n",
    "\n",
    "loss_fn = FocalLoss(alpha=class_weights, gamma=2)\n",
    "\n",
    "def train(data):\n",
    "    data = data.to(device)  # Add this line\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)[data.train_mask].squeeze()\n",
    "    loss = loss_fn(out, data.y[data.train_mask].float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()  # Update learning rate\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(data, mask):\n",
    "    data = data.to(device)  # Add this line\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)[mask].squeeze()  # Predicted probabilities\n",
    "        preds = (out > 0.5).long()  # Binary predictions\n",
    "        print(f\"Classes predicted: {preds.unique().cpu().numpy()}\")  # Move to CPU before converting to NumPy\n",
    "        accuracy = preds.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        precision = precision_score(data.y[mask].cpu().numpy(), preds.cpu().numpy(), zero_division=1)  # Move to CPU\n",
    "        recall = recall_score(data.y[mask].cpu().numpy(), preds.cpu().numpy(), zero_division=1)  # Move to CPU\n",
    "        f1 = f1_score(data.y[mask].cpu().numpy(), preds.cpu().numpy(), zero_division=1)  # Move to CPU\n",
    "        roc_auc = roc_auc_score(data.y[mask].cpu().numpy(), out.cpu().numpy())  # Use out, not preds\n",
    "    return accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "best_model = None\n",
    "best_val_roc_auc = 0\n",
    "patience = 10\n",
    "epochs_no_improve = 0\n",
    "early_stop_threshold = 0.0001\n",
    "\n",
    "for epoch in range(150):\n",
    "    loss = train(graph)\n",
    "    train_metrics = evaluate(graph, graph.train_mask)\n",
    "    val_metrics = evaluate(graph, graph.val_mask)\n",
    "    test_metrics = evaluate(graph, graph.test_mask)\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss:.10f}, Train ROC AUC: {train_metrics[4]:.10f}, Val ROC AUC: {val_metrics[4]:.10f}, Test ROC AUC: {test_metrics[4]:.10f}')\n",
    "\n",
    "    # Check if the validation ROC AUC score is higher than what we've seen so far\n",
    "    if val_metrics[4] > best_val_roc_auc + early_stop_threshold:\n",
    "        # We've seen a model performance improvement!\n",
    "        best_val_roc_auc = val_metrics[4]\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        epochs_no_improve = 0  # Reset the count\n",
    "    else:\n",
    "        # We did not see any improvement this epoch\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # If we've had too many epochs with no improvement, stop training early\n",
    "    if epochs_no_improve == patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "# Load the best model back in\n",
    "model.load_state_dict(best_model)\n",
    "test_metrics = evaluate(graph, graph.test_mask)\n",
    "print(f'After early stopping, Test ROC AUC: {test_metrics[4]:.10f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
