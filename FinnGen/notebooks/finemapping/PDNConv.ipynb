{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Grapher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1805a-55dc-409c-9ccf-512eb1f88667",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8ec1d-73be-4b8c-95dc-79489e84998b",
   "metadata": {},
   "source": [
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located.\n",
    "\n",
    "- `pos`: This is the position of the genetic variant on the chromosome.\n",
    "\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant.\n",
    "\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population.\n",
    "\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group.\n",
    "\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group.\n",
    "\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0). \n",
    "\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 64 bit (AMD64)]\n",
      "NumPy version: 1.24.3\n",
      "Pandas version: 2.0.1\n",
      "Matplotlib version: 3.7.1\n",
      "Scikit-learn version: 1.2.2\n",
      "Torch version: 2.0.0+cu118\n",
      "Torch Geometric version: 2.3.1\n",
      "NetworkX version: 3.0\n",
      "Using NVIDIA GeForce RTX 3060 Ti (cuda)\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import expit\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from collections import Counter\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfa8cb-01df-491f-94f0-c8c228256b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "data = pd.read_csv('~/Desktop/gwas-graph/FinnGen/data/gwas-finemap.csv', dtype=dtypes)\n",
    "\n",
    "# Assert column names\n",
    "expected_columns = ['#chrom', 'pos', 'ref', 'alt', 'rsids', 'nearest_genes', 'pval', 'mlogp', 'beta',\n",
    "                    'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls', 'finemapped',\n",
    "                    'id', 'trait']\n",
    "assert set(data.columns) == set(expected_columns), \"Unexpected columns in the data DataFrame.\"\n",
    "\n",
    "# Assert data types\n",
    "expected_dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "for col, expected_dtype in expected_dtypes.items():\n",
    "    assert data[col].dtype == expected_dtype, f\"Unexpected data type for column {col}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d6d595-4ef8-41d5-b0a5-dbdc9f2d9b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values in each column:\n",
      "#chrom                   0\n",
      "pos                      0\n",
      "ref                      0\n",
      "alt                      0\n",
      "rsids              1366396\n",
      "nearest_genes       727855\n",
      "pval                     0\n",
      "mlogp                    0\n",
      "beta                     0\n",
      "sebeta                   0\n",
      "af_alt                   0\n",
      "af_alt_cases             0\n",
      "af_alt_controls          0\n",
      "id                       0\n",
      "finemapped               0\n",
      "trait                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for total number of null values in each column\n",
    "null_counts = data.isnull().sum()\n",
    "\n",
    "print(\"Total number of null values in each column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513cafa0-f3f8-47b5-9d22-96062449edde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fab1f-46fa-40a5-a8c5-aa746b1db7eb",
   "metadata": {},
   "source": [
    "### Find nearest gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assert column 'nearest_genes' is a string\n",
    "assert data['nearest_genes'].dtype == 'object', \"Column 'nearest_genes' is not of string type.\"\n",
    "\n",
    "# Get the length of the data before transformation\n",
    "original_length = len(data)\n",
    "\n",
    "# Extract the first gene name from the 'nearest_genes' column\n",
    "data['nearest_genes'] = data['nearest_genes'].str.split(',').str[0]\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Assert the length of the data remains the same\n",
    "assert len(data) == original_length, \"Length of the data has changed after transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "`data` Pandas DataFrame:\n",
    "\n",
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located (int).\n",
    "- `pos`: This is the position of the genetic variant on the chromosome (int: 1-200,000).\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant (string).\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population (float: 0-1.\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group (float: 0-1).\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group (float: 0-1).\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0) (int).\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs.\n",
    "\n",
    "### Task Overview\n",
    "\n",
    "The objective is to design and implement a binary node classification GNN model to predict whether variants are included after post-finemapping or not based on `finemapping`.\n",
    "\n",
    "### Nodes and Their Features\n",
    "\n",
    "There is one type of node: SNP nodes.\n",
    "\n",
    "- **SNP Nodes**: Each SNP Node is characterized by various features, including `id`, `nearest_genes`, `#chrom`, `pos`, `ref`, `alt`, `mlogp`, `beta`, `sebeta`,  `af_alt`, `af_alt_cases`, and `af_alt_controls` columns.\n",
    "\n",
    "### Edges, Their Features, and Labels\n",
    "\n",
    "Edges represent relationships between SNP nodes in the graph.\n",
    "\n",
    "1. **Type 1 Edges: LD-based edges**\n",
    "\n",
    "   - For each pair of SNPs (row1 and row2) that exist on the same chromosome (`#chrom`), an edge is created if the absolute difference between their positions (`pos`) is less than or equal to 500,000 and greater than 1 (no loops).\n",
    "   - The edge attributes (`edge_attr`) are:\n",
    "       - `nearest_genes` \n",
    "       - `#chrom`\n",
    "       - `pos`\n",
    "       - `af_alt`, \n",
    "       - `af_alt_cases` \n",
    "       - `af_alt_controls`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51485cfb-989a-430f-874b-af2d5f79d9f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68257919-e9e4-4aa7-8b45-d0de2dbd765b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 20170\n",
      "Number of edges: 291740\n",
      "Node feature dimension: 11\n",
      "Execution time: 46.77100658416748 seconds\n",
      "         126559278 function calls (124402504 primitive calls) in 46.758 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1145 to 3 due to restriction <3>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       13    0.000    0.000   46.771    3.598 C:\\Users\\falty\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3472(run_code)\n",
      "       13    0.000    0.000   46.771    3.598 {built-in method builtins.exec}\n",
      "        1    0.976    0.976   46.516   46.516 C:\\Users\\falty\\AppData\\Local\\Temp\\ipykernel_23832\\966054611.py:32(preprocess_positive_edges)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edge_weight_cutoff = 1e-3  # set the cutoff value, this is an example and you can choose any suitable value\n",
    "\n",
    "\n",
    "def get_unique_snps(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Function to create mappings for SNPs to integer indices.\n",
    "    \"\"\"\n",
    "    return {snp: idx for idx, snp in enumerate(data['id'].unique())}\n",
    "\n",
    "\n",
    "def preprocess_snp_features(data: pd.DataFrame, snp_to_idx: dict) -> pd.DataFrame:\n",
    "    assert 'id' in data.columns and 'nearest_genes' in data.columns\n",
    "    assert data[['id', 'nearest_genes']].isnull().sum().sum() == 0\n",
    "\n",
    "    cols_to_extract = ['id', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls']\n",
    "    snp_features = data.loc[data['id'].isin(snp_to_idx.keys()), cols_to_extract].set_index('id').sort_index()\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    categorical_cols = ['ref', 'alt', 'nearest_genes']\n",
    "    count_encoder = ce.CountEncoder(cols=categorical_cols)\n",
    "    snp_features = count_encoder.fit_transform(snp_features)\n",
    "\n",
    "    numerical_cols = list(set(snp_features.columns) - set(categorical_cols))\n",
    "    snp_features[numerical_cols] = scaler.fit_transform(snp_features[numerical_cols])\n",
    "\n",
    "    snp_features = snp_features.fillna(0)\n",
    "\n",
    "    return snp_features\n",
    "\n",
    "\n",
    "def preprocess_positive_edges(data: pd.DataFrame, snp_to_idx: dict) -> torch.Tensor:\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    data = data.sort_values(by=['#chrom', 'pos'])\n",
    "    data['snp_idx'] = data['id'].map(snp_to_idx)\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    data['nearest_genes'] = label_encoder.fit_transform(data['nearest_genes'])\n",
    "\n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        if group.empty:\n",
    "            continue\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "            pos_diff = abs(group['pos'] - row['pos'])\n",
    "            mask = (pos_diff > 1) & (pos_diff <= 1_000_000)\n",
    "            filtered_group = group[mask]\n",
    "\n",
    "            if filtered_group.empty:\n",
    "                continue\n",
    "\n",
    "            edge_list.extend(zip([row['snp_idx']] * len(filtered_group), filtered_group['snp_idx']))\n",
    "\n",
    "            for _, edge_row in filtered_group.iterrows():\n",
    "                edge_attr = (edge_row['nearest_genes'], edge_row['#chrom'], edge_row['pos'], edge_row['af_alt'], edge_row['af_alt_cases'], edge_row['af_alt_controls'])\n",
    "                edge_attributes.append(edge_attr)\n",
    "\n",
    "    edge_attributes = scaler.fit_transform(np.array(edge_attributes))\n",
    "\n",
    "    return torch.tensor(edge_list, dtype=torch.long).t().contiguous(), torch.tensor(edge_attributes, dtype=torch.float)\n",
    "\n",
    "\n",
    "def create_pytorch_graph(features: torch.Tensor, edges: torch.Tensor, edge_attr: torch.Tensor) -> Data:\n",
    "    return Data(x=features, edge_index=edges, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "snp_to_idx = get_unique_snps(data)\n",
    "snp_features = preprocess_snp_features(data, snp_to_idx)\n",
    "features = torch.tensor(snp_features.values, dtype=torch.float)\n",
    "\n",
    "positive_edges_snp_snp, edge_attr = preprocess_positive_edges(data, snp_to_idx)\n",
    "graph = create_pytorch_graph(features, positive_edges_snp_snp, edge_attr)\n",
    "graph.y = torch.tensor(data['finemapped'].values, dtype=torch.long)\n",
    "\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats(3)  # Only print the top 3 lines\n",
    "print(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save/Load graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd9e725-2545-4820-903c-a68a9a9d73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PyTorch Geometric graph\n",
    "\n",
    "#torch.save(graph, \"pytorch_geometric_graph.pt\")\n",
    "#graph = torch.load(\"pytorch_geometric_graph.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4462c-4130-4242-9a7d-09c6ed71b613",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba52c6ce-3f9b-43f5-b3d9-b12fef9ef3ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph stats:\n",
      "Number of nodes: 20170\n",
      "Number of edges: 291740\n",
      "Node feature dimension: 11\n",
      "Number of finemapped nodes: 3020\n",
      "Average Degree: 14.464055061340332\n",
      "Median Degree: 14.0\n",
      "Standard Deviation of Degree: 4.666636943817139\n",
      "Density: 0.0014342858\n",
      "Features with NaN values:\n",
      "[]\n",
      "Edge attribute stats:\n",
      "Number of edge attributes: 6\n",
      "Stats for attribute 1:\n",
      "Average: -0.003921892493963242\n",
      "Standard deviation: 0.5780200362205505\n",
      "Maximum: 1.013943076133728\n",
      "Minimum: -0.9576422572135925\n",
      "\n",
      "Stats for attribute 2:\n",
      "Average: 0.12059140205383301\n",
      "Standard deviation: 0.6776004433631897\n",
      "Maximum: 1.6666666269302368\n",
      "Minimum: -0.7777777910232544\n",
      "\n",
      "Stats for attribute 3:\n",
      "Average: 0.12703663110733032\n",
      "Standard deviation: 0.7035937905311584\n",
      "Maximum: 2.2977404594421387\n",
      "Minimum: -0.8305438160896301\n",
      "\n",
      "Stats for attribute 4:\n",
      "Average: 0.9953055381774902\n",
      "Standard deviation: 1.9785510301589966\n",
      "Maximum: 8.822497367858887\n",
      "Minimum: -0.057887930423021317\n",
      "\n",
      "Stats for attribute 5:\n",
      "Average: 0.9975663423538208\n",
      "Standard deviation: 1.9826009273529053\n",
      "Maximum: 8.841273307800293\n",
      "Minimum: -0.05794169008731842\n",
      "\n",
      "Stats for attribute 6:\n",
      "Average: 0.9971073269844055\n",
      "Standard deviation: 1.9823918342590332\n",
      "Maximum: 8.839381217956543\n",
      "Minimum: -0.05810800567269325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "def print_graph_stats(graph, positive_edges_snp_snp, features_list):\n",
    "    print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "    print(f\"Number of edges: {graph.num_edges}\")\n",
    "    print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "    print(f\"Number of finemapped nodes: {data['finemapped'].sum()}\")\n",
    "\n",
    "    # Compute and print degree-related stats\n",
    "    degrees = degree(graph.edge_index[0].long(), num_nodes=graph.num_nodes)\n",
    "    average_degree = degrees.float().mean().item()\n",
    "    median_degree = np.median(degrees.numpy())\n",
    "    std_degree = degrees.float().std().item()\n",
    "\n",
    "    print(f\"Average Degree: {average_degree}\")\n",
    "    print(f\"Median Degree: {median_degree}\")\n",
    "    print(f\"Standard Deviation of Degree: {std_degree}\")\n",
    "\n",
    "    # Density is the ratio of actual edges to the maximum number of possible edges\n",
    "    num_possible_edges = graph.num_nodes * (graph.num_nodes - 1) / 2\n",
    "    density = graph.num_edges / num_possible_edges\n",
    "\n",
    "    print(f\"Density: {density:.10f}\")\n",
    "\n",
    "    # Check for NaN values in features\n",
    "    nan_mask = torch.isnan(graph.x)\n",
    "    nan_features = []\n",
    "    for feature_idx, feature_name in enumerate(features_list):\n",
    "        if nan_mask[:, feature_idx].any():\n",
    "            nan_features.append(feature_name)\n",
    "\n",
    "    print(\"Features with NaN values:\")\n",
    "    print(nan_features)\n",
    "\n",
    "def print_edge_attr_stats(edge_attrs):\n",
    "    num_attrs = edge_attrs.size(1)\n",
    "    print(f\"Number of edge attributes: {num_attrs}\")\n",
    "\n",
    "    for i in range(num_attrs):\n",
    "        attr_values = edge_attrs[:, i]\n",
    "        print(f\"Stats for attribute {i + 1}:\")\n",
    "        print(f\"Average: {attr_values.float().mean().item()}\")\n",
    "        print(f\"Standard deviation: {attr_values.float().std().item()}\")\n",
    "        print(f\"Maximum: {attr_values.max().item()}\")\n",
    "        print(f\"Minimum: {attr_values.min().item()}\")\n",
    "        print()\n",
    "\n",
    "# Print graph stats\n",
    "print(\"Graph stats:\")\n",
    "print_graph_stats(graph, positive_edges_snp_snp, snp_features.columns)\n",
    "\n",
    "# Print edge attribute stats\n",
    "print(\"Edge attribute stats:\")\n",
    "print_edge_attr_stats(graph.edge_attr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51e96d-660c-4b4f-8ac4-9a07dba5bcc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c79fb79d-4729-4d4d-8dff-229108b6cac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes per class in each set:\n",
      "Train set:\n",
      "Class 0: 6789 nodes\n",
      "Class 1: 1279 nodes\n",
      "Validation set:\n",
      "Class 0: 5199 nodes\n",
      "Class 1: 852 nodes\n",
      "Test set:\n",
      "Class 0: 5162 nodes\n",
      "Class 1: 889 nodes\n",
      "Percentage of Class 0 in test set: 85.31%\n",
      "Percentage of Class 1 in test set: 14.69%\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "transform = RandomNodeSplit(split=\"train_rest\", num_val=0.3, num_test=0.3, key='y')\n",
    "graph = transform(graph)\n",
    "\n",
    "# Count the number of nodes per class in each set\n",
    "train_class_counts = Counter(graph.y[graph.train_mask].numpy())\n",
    "val_class_counts = Counter(graph.y[graph.val_mask].numpy())\n",
    "test_class_counts = Counter(graph.y[graph.test_mask].numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of nodes per class in each set:\")\n",
    "print(\"Train set:\")\n",
    "for class_label, count in train_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Validation set:\")\n",
    "for class_label, count in val_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Test set:\")\n",
    "for class_label, count in test_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "\n",
    "# Calculate and print the percentage of class 1 vs. class 0 in the test set\n",
    "total_test_nodes = sum(test_class_counts.values())\n",
    "class_0_nodes = test_class_counts[0]\n",
    "class_1_nodes = test_class_counts[1]\n",
    "class_0_percentage = (class_0_nodes / total_test_nodes) * 100\n",
    "class_1_percentage = (class_1_nodes / total_test_nodes) * 100\n",
    "print(f\"Percentage of Class 0 in test set: {class_0_percentage:.2f}%\")\n",
    "print(f\"Percentage of Class 1 in test set: {class_1_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665050a-5cd9-4960-a2ef-8a4cf2cb5e40",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358aee2-41a3-4301-ab75-23ccb0ff75a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GATv2Conv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8720d7b-7115-4f2b-adca-cd5f70d4e564",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn.functional import binary_cross_entropy, dropout, leaky_relu\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# suppose y is your target vector\n",
    "y = graph.y.cpu().numpy()\n",
    "\n",
    "# Count number of occurrences of each class\n",
    "class_counts = np.bincount(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = 1. / class_counts\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "\n",
    "\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_layers, num_classes, dropout_rate=0.5):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GATv2Conv(num_node_features, hidden_layers[0], aggr=\"mean\"))\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.layers.append(GATv2Conv(hidden_layers[i - 1], hidden_layers[i], aggr=\"mean\"))\n",
    "        self.layers.append(GATv2Conv(hidden_layers[-1], num_classes, aggr=\"mean\"))\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.bn_layers = torch.nn.ModuleList([torch.nn.BatchNorm1d(size) for size in hidden_layers])\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for i, conv in enumerate(self.layers[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.bn_layers[i](x)\n",
    "            x = leaky_relu(x)\n",
    "            x = dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return torch.sigmoid(x.view(-1))\n",
    "\n",
    "    \n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets.long()].view(-1, 1)\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "            logpt = logpt * alpha_t\n",
    "        else:\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(logpt)\n",
    "        F_loss = -((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "        \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\\\n",
    "            \n",
    "            \n",
    "def train(model, data, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)[data.train_mask].squeeze()\n",
    "    loss = loss_fn(out, data.y[data.train_mask].float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data, mask, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)[mask].squeeze()\n",
    "        preds = (out > 0.5).long()\n",
    "        loss = loss_fn(out, data.y[mask].float())\n",
    "        accuracy = preds.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        precision, recall, _ = precision_recall_curve(data.y[mask].cpu(), out.cpu())\n",
    "        auprc = auc(recall, precision)\n",
    "    return loss.item(), accuracy, auprc\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialization\n",
    "    hidden_layers = [256, 256] \n",
    "    model = GATModel(graph.num_node_features, hidden_layers, 1, dropout_rate=0.5).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    loss_fn = FocalLoss(alpha=class_weights, gamma=2)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.0001)\n",
    "\n",
    "    # Move your graph data to device\n",
    "    graph.x = graph.x.to(device)\n",
    "    graph.edge_index = graph.edge_index.to(device)\n",
    "    graph.y = graph.y.to(device)\n",
    "    graph.train_mask = graph.train_mask.to(device)\n",
    "    graph.val_mask = graph.val_mask.to(device)\n",
    "    graph.test_mask = graph.test_mask.to(device)\n",
    "\n",
    "    for epoch in range(150):\n",
    "        train_loss = train(model, graph, loss_fn, optimizer)\n",
    "        _, train_acc, _ = evaluate(model, graph, graph.train_mask, loss_fn)\n",
    "        val_loss, val_acc, val_auprc = evaluate(model, graph, graph.val_mask, loss_fn)\n",
    "        print(f'Epoch: {epoch+1}, Loss: {train_loss:.10f}, Train Acc: {train_acc:.10f}, Val Loss: {val_loss:.10f}, Val Acc: {val_acc:.10f}, Val AUPRC: {val_auprc:.10f}')\n",
    "\n",
    "        # Reduce learning rate when validation loss plateaus\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping.step(-val_auprc)  # Pass -val_auprc because we want to maximize it\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "        # Save checkpoint if is a new best\n",
    "        if -val_auprc == early_stopping.best_score:  # Save the best model based on AUPRC\n",
    "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "\n",
    "    # Load the best model back in\n",
    "    if os.path.isfile('checkpoint.pt'):\n",
    "        model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    else:\n",
    "        print(\"No checkpoint found. Creating a new file checkpoint.pt.\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "    test_loss, test_acc, test_auprc = evaluate(model, graph, graph.test_mask, loss_fn)\n",
    "    print(f'After early stopping, Test AUPRC: {test_auprc:.10f}')\n",
    "\n",
    "# Call the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3dcdc90-311d-4a37-afc0-c52931471daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1562482738"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecb64f-6c08-4790-962b-902e46249c4f",
   "metadata": {},
   "source": [
    "### PDNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ba196f1-ae20-4b88-80be-a3b791223d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.0258789752, Train Acc: 0.1585275161, Val Loss: 1.6935904026, Val Acc: 0.1408031730, Val AUPRC: 0.1481309036\n",
      "Epoch: 2, Loss: 0.0241101496, Train Acc: 0.1585275161, Val Loss: 0.6308483481, Val Acc: 0.1408031730, Val AUPRC: 0.1467076587\n",
      "Epoch: 3, Loss: 0.0233423393, Train Acc: 0.1585275161, Val Loss: 0.2104600817, Val Acc: 0.1408031730, Val AUPRC: 0.1429365914\n",
      "Epoch: 4, Loss: 0.0221711323, Train Acc: 0.1596430342, Val Loss: 0.0438982435, Val Acc: 0.1422905305, Val AUPRC: 0.1389791453\n",
      "Epoch: 5, Loss: 0.0211917255, Train Acc: 0.7973475459, Val Loss: 0.0127125243, Val Acc: 0.8173855561, Val AUPRC: 0.1344627712\n",
      "Epoch: 6, Loss: 0.0206264984, Train Acc: 0.8380019831, Val Loss: 0.0145638064, Val Acc: 0.8575442076, Val AUPRC: 0.1320792869\n",
      "Epoch: 7, Loss: 0.0202427581, Train Acc: 0.8409766981, Val Loss: 0.0206357073, Val Acc: 0.8588663031, Val AUPRC: 0.1316917406\n",
      "Epoch: 8, Loss: 0.0185478069, Train Acc: 0.8415964303, Val Loss: 0.0250618402, Val Acc: 0.8593620889, Val AUPRC: 0.1322863366\n",
      "Epoch: 9, Loss: 0.0200662613, Train Acc: 0.8414724839, Val Loss: 0.0275995471, Val Acc: 0.8591968270, Val AUPRC: 0.1333884617\n",
      "Epoch: 10, Loss: 0.0192820914, Train Acc: 0.8414724839, Val Loss: 0.0282986052, Val Acc: 0.8591968270, Val AUPRC: 0.1347233490\n",
      "Epoch: 11, Loss: 0.0191935301, Train Acc: 0.8414724839, Val Loss: 0.0277301669, Val Acc: 0.8591968270, Val AUPRC: 0.1352759808\n",
      "Early stopping!\n",
      "No checkpoint found. Creating a new file checkpoint.pt.\n",
      "After early stopping, Test AUPRC: 0.1525066896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn.functional import binary_cross_entropy, dropout, leaky_relu\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# suppose y is your target vector\n",
    "y = graph.y.cpu().numpy()\n",
    "\n",
    "# Count number of occurrences of each class\n",
    "class_counts = np.bincount(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = 1. / class_counts\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "\n",
    "\n",
    "from torch_geometric.nn import PDNConv\n",
    "\n",
    "class PDNModel(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_layers, num_classes, edge_dim, dropout_rate=0.5):\n",
    "        super(PDNModel, self).__init__()\n",
    "        self.edge_transform = torch.nn.Linear(6, edge_dim)  # Add the edge transformation layer\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(PDNConv(num_node_features, hidden_layers[0], edge_dim, hidden_layers[0]))\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.layers.append(PDNConv(hidden_layers[i - 1], hidden_layers[i], edge_dim, hidden_layers[i]))\n",
    "        self.layers.append(PDNConv(hidden_layers[-1], num_classes, edge_dim, hidden_layers[-1]))\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.bn_layers = torch.nn.ModuleList([torch.nn.BatchNorm1d(size) for size in hidden_layers])\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        edge_attr = self.edge_transform(edge_attr)  # Apply transformation to the edge attributes\n",
    "        for i, conv in enumerate(self.layers[:-1]):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = self.bn_layers[i](x)\n",
    "            x = leaky_relu(x)\n",
    "            x = dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index, edge_attr)\n",
    "        return torch.sigmoid(x.view(-1))\n",
    "\n",
    "\n",
    "    \n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets.long()].view(-1, 1)\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "            logpt = logpt * alpha_t\n",
    "        else:\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(logpt)\n",
    "        F_loss = -((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "        \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\\\n",
    "            \n",
    "            \n",
    "def train(model, data, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)[data.train_mask].squeeze()\n",
    "    loss = loss_fn(out, data.y[data.train_mask].float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data, mask, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)[mask].squeeze()\n",
    "        preds = (out > 0.5).long()\n",
    "        loss = loss_fn(out, data.y[mask].float())\n",
    "        accuracy = preds.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        precision, recall, _ = precision_recall_curve(data.y[mask].cpu(), out.cpu())\n",
    "        auprc = auc(recall, precision)\n",
    "    return loss.item(), accuracy, auprc\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialization\n",
    "    hidden_layers = [256, 256] \n",
    "    edge_dim = 256  # specify the edge dimensionality\n",
    "    model = PDNModel(graph.num_node_features, hidden_layers, 1, edge_dim, dropout_rate=0.5).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    loss_fn = FocalLoss(alpha=class_weights, gamma=2)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.0001)\n",
    "\n",
    "    # Move your graph data to device\n",
    "    graph.x = graph.x.to(device)\n",
    "    graph.edge_index = graph.edge_index.to(device)\n",
    "    graph.y = graph.y.to(device)\n",
    "    graph.train_mask = graph.train_mask.to(device)\n",
    "    graph.val_mask = graph.val_mask.to(device)\n",
    "    graph.test_mask = graph.test_mask.to(device)\n",
    "    graph.edge_attr = graph.edge_attr.to(device) \n",
    "\n",
    "    for epoch in range(150):\n",
    "        train_loss = train(model, graph, loss_fn, optimizer)\n",
    "        _, train_acc, _ = evaluate(model, graph, graph.train_mask, loss_fn)\n",
    "        val_loss, val_acc, val_auprc = evaluate(model, graph, graph.val_mask, loss_fn)\n",
    "        print(f'Epoch: {epoch+1}, Loss: {train_loss:.10f}, Train Acc: {train_acc:.10f}, Val Loss: {val_loss:.10f}, Val Acc: {val_acc:.10f}, Val AUPRC: {val_auprc:.10f}')\n",
    "\n",
    "        # Reduce learning rate when validation loss plateaus\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping.step(-val_auprc)  # Pass -val_auprc because we want to maximize it\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "        # Save checkpoint if is a new best\n",
    "        if -val_auprc == early_stopping.best_score:  # Save the best model based on AUPRC\n",
    "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "\n",
    "    # Load the best model back in\n",
    "    if os.path.isfile('checkpoint.pt'):\n",
    "        model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    else:\n",
    "        print(\"No checkpoint found. Creating a new file checkpoint.pt.\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "    test_loss, test_acc, test_auprc = evaluate(model, graph, graph.test_mask, loss_fn)\n",
    "    print(f'After early stopping, Test AUPRC: {test_auprc:.10f}')\n",
    "\n",
    "# Call the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa56de1-e78e-496c-ab71-acf46ead78b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
