{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Grapher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1805a-55dc-409c-9ccf-512eb1f88667",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8ec1d-73be-4b8c-95dc-79489e84998b",
   "metadata": {},
   "source": [
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located.\n",
    "\n",
    "- `pos`: This is the position of the genetic variant on the chromosome.\n",
    "\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant.\n",
    "\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population.\n",
    "\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group.\n",
    "\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group.\n",
    "\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0). \n",
    "\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 64 bit (AMD64)]\n",
      "NumPy version: 1.24.3\n",
      "Pandas version: 2.0.1\n",
      "Matplotlib version: 3.7.1\n",
      "Scikit-learn version: 1.2.2\n",
      "Torch version: 2.0.0+cu118\n",
      "Torch Geometric version: 2.3.1\n",
      "NetworkX version: 3.0\n",
      "Using NVIDIA GeForce RTX 3060 Ti (cuda)\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import expit\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfa8cb-01df-491f-94f0-c8c228256b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "data = pd.read_csv('~/Desktop/gwas-graph/FinnGen/data/gwas-finemap.csv', dtype=dtypes)\n",
    "\n",
    "# Assert column names\n",
    "expected_columns = ['#chrom', 'pos', 'ref', 'alt', 'rsids', 'nearest_genes', 'pval', 'mlogp', 'beta',\n",
    "                    'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls', 'finemapped',\n",
    "                    'id', 'trait']\n",
    "assert set(data.columns) == set(expected_columns), \"Unexpected columns in the data DataFrame.\"\n",
    "\n",
    "# Assert data types\n",
    "expected_dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "for col, expected_dtype in expected_dtypes.items():\n",
    "    assert data[col].dtype == expected_dtype, f\"Unexpected data type for column {col}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d6d595-4ef8-41d5-b0a5-dbdc9f2d9b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values in each column:\n",
      "#chrom                   0\n",
      "pos                      0\n",
      "ref                      0\n",
      "alt                      0\n",
      "rsids              1366396\n",
      "nearest_genes       727855\n",
      "pval                     0\n",
      "mlogp                    0\n",
      "beta                     0\n",
      "sebeta                   0\n",
      "af_alt                   0\n",
      "af_alt_cases             0\n",
      "af_alt_controls          0\n",
      "id                       0\n",
      "finemapped               0\n",
      "trait                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for total number of null values in each column\n",
    "null_counts = data.isnull().sum()\n",
    "\n",
    "print(\"Total number of null values in each column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513cafa0-f3f8-47b5-9d22-96062449edde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fab1f-46fa-40a5-a8c5-aa746b1db7eb",
   "metadata": {},
   "source": [
    "### Find nearest gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assert column 'nearest_genes' is a string\n",
    "assert data['nearest_genes'].dtype == 'object', \"Column 'nearest_genes' is not of string type.\"\n",
    "\n",
    "# Get the length of the data before transformation\n",
    "original_length = len(data)\n",
    "\n",
    "# Extract the first gene name from the 'nearest_genes' column\n",
    "data['nearest_genes'] = data['nearest_genes'].str.split(',').str[0]\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Assert the length of the data remains the same\n",
    "assert len(data) == original_length, \"Length of the data has changed after transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "`data` Pandas DataFrame:\n",
    "\n",
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located.\n",
    "- `pos`: This is the position of the genetic variant on the chromosome (int: 1-200,000).\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant (string).\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population (float: 0-1.\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group (float: 0-1).\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group (float: 0-1).\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0) (int).\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs.\n",
    "\n",
    "### Task Overview\n",
    "\n",
    "The objective is to design and implement a binary node classification GNN model to predict whether variants are included after post-finemapping or not based on `finemapping`.\n",
    "\n",
    "### Nodes and Their Features\n",
    "\n",
    "There is one type of node: SNP nodes.\n",
    "\n",
    "- **SNP Nodes**: Each SNP Node is characterized by various features, including `id`, `nearest_genes`, `#chrom`, `pos`, `ref`, `alt`, `mlogp`, `beta`, `sebeta`,  `af_alt`, `af_alt_cases`, and `af_alt_controls` columns.\n",
    "\n",
    "### Edges, Their Features, and Labels\n",
    "\n",
    "Edges represent relationships between SNP nodes in the graph.\n",
    "\n",
    "1. **Type 1 Edges: LD-based edges**\n",
    "\n",
    "   - For each pair of SNPs (row1 and row2) that exist on the same chromosome (`#chrom`), an edge is created if the absolute difference between their positions (`pos`) is less than or equal to 1,000,000 and greater than 1 (no loops).\n",
    "   - The weight of the edge is determined by the following formula:\n",
    "     \n",
    "```\n",
    "    weights = 1 * e^(-ln(2) / 100_000 * pos_diff_abs)\n",
    "    \n",
    "```\n",
    "\n",
    "    - For each chromosome, standardize the edge weights after all weights have been computed.\n",
    "    - Prune any edges that have a weight of less than `1e-3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51485cfb-989a-430f-874b-af2d5f79d9f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68257919-e9e4-4aa7-8b45-d0de2dbd765b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edge_weight_cutoff = 1e-3  # set the cutoff value, this is an example and you can choose any suitable value\n",
    "\n",
    "\n",
    "def get_unique_snps(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Function to create mappings for SNPs to integer indices.\n",
    "    \"\"\"\n",
    "    return {snp: idx for idx, snp in enumerate(data['id'].unique())}\n",
    "\n",
    "\n",
    "def preprocess_snp_features(data: pd.DataFrame, snp_to_idx: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to create node feature vectors for SNPs and preprocess categorical and numerical features.\n",
    "    \"\"\"\n",
    "    # Ensure that 'id' exists in the data and 'id' and 'nearest_genes' are not null\n",
    "    assert 'id' in data.columns and 'nearest_genes' in data.columns, \"Columns 'id' or 'nearest_genes' do not exist in the dataframe\"\n",
    "    assert data[['id', 'nearest_genes']].isnull().sum().sum() == 0, \"Columns 'id' or 'nearest_genes' contain null values\"\n",
    "    \n",
    "    # Columns to be extracted from the original dataframe\n",
    "    cols_to_extract = ['id', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls']\n",
    "    \n",
    "    snp_features = data.loc[data['id'].isin(snp_to_idx.keys()), cols_to_extract].set_index('id').sort_index()\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Frequency encoding for 'nearest_genes' using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    snp_features['nearest_genes'] = label_encoder.fit_transform(snp_features['nearest_genes'])\n",
    "    \n",
    "    categorical_cols = ['ref', 'alt']\n",
    "\n",
    "    # CountEncoder for 'ref', 'alt'\n",
    "    count_encoder = ce.CountEncoder(cols=categorical_cols)\n",
    "    snp_features = count_encoder.fit_transform(snp_features)\n",
    "\n",
    "    numerical_cols = list(set(snp_features.columns) - set(categorical_cols))\n",
    "    snp_features[numerical_cols] = scaler.fit_transform(snp_features[numerical_cols])\n",
    "    \n",
    "    # Filling 0 values for all columns\n",
    "    snp_features = snp_features.fillna(0)\n",
    "\n",
    "    return snp_features\n",
    "\n",
    "\n",
    "def preprocess_positive_edges(data: pd.DataFrame, snp_to_idx: dict) -> torch.Tensor:\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Sort data once before grouping\n",
    "    data = data.sort_values(by=['#chrom', 'pos'])\n",
    "\n",
    "    # Create new column for SNP index\n",
    "    data['snp_idx'] = data['id'].map(snp_to_idx)\n",
    "\n",
    "    positive_edges_snp_snp = []\n",
    "    snp_weights = []\n",
    "\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        # Skip if group is empty\n",
    "        if group.empty:\n",
    "            continue\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "            pos_diff_abs = abs(group['pos'] - row['pos'])\n",
    "            mask = (pos_diff_abs > 1) & (pos_diff_abs <= 1_000_000)\n",
    "            filtered_group = group[mask]\n",
    "\n",
    "            if filtered_group.empty:\n",
    "                continue\n",
    "\n",
    "            weights = np.exp(-np.log(2) / 100_000 * pos_diff_abs[mask])\n",
    "            \n",
    "            # Skip if weights are empty\n",
    "            if len(weights) == 0:\n",
    "                continue\n",
    "\n",
    "            positive_edges_snp_snp.extend(zip([row['snp_idx']] * len(filtered_group), filtered_group['snp_idx']))\n",
    "            snp_weights.extend(weights)\n",
    "\n",
    "    # Normalize weights\n",
    "    snp_weights = scaler.fit_transform(np.array(snp_weights).reshape(-1, 1)).flatten()\n",
    "    mask_cutoff = snp_weights >= edge_weight_cutoff\n",
    "    filtered_edges = np.array(positive_edges_snp_snp)[mask_cutoff]\n",
    "    filtered_weights = snp_weights[mask_cutoff]\n",
    "\n",
    "    return torch.tensor(filtered_edges, dtype=torch.long).t().contiguous(), torch.tensor(filtered_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "def create_pytorch_graph(features: torch.Tensor, edges: torch.Tensor, edge_weights: torch.Tensor) -> Data:\n",
    "    return Data(x=features, edge_index=edges, edge_attr=edge_weights)\n",
    "\n",
    "\n",
    "# Add profiling\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Main\n",
    "snp_to_idx = get_unique_snps(data)\n",
    "labels = data['finemapped'].map(lambda x: 1 if x > 0 else 0)\n",
    "snp_features = preprocess_snp_features(data, snp_to_idx)\n",
    "features = torch.tensor(snp_features.values, dtype=torch.float)\n",
    "\n",
    "positive_edges_snp_snp, snp_weights = preprocess_positive_edges(data, snp_to_idx)\n",
    "graph = create_pytorch_graph(features, positive_edges_snp_snp, snp_weights)\n",
    "graph.y = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats(3)  # Only print the top 5 lines\n",
    "print(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save/Load graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9e725-2545-4820-903c-a68a9a9d73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PyTorch Geometric graph\n",
    "\n",
    "torch.save(graph, \"pytorch_geometric_graph.pt\")\n",
    "#graph = torch.load(\"pytorch_geometric_graph.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4462c-4130-4242-9a7d-09c6ed71b613",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52c6ce-3f9b-43f5-b3d9-b12fef9ef3ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "def print_graph_stats(graph, positive_edges_snp_snp, features_list):\n",
    "    print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "    print(f\"Number of edges: {graph.num_edges}\")\n",
    "    print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "    print(f\"Number of finemapped nodes: {data['finemapped'].sum()}\")\n",
    "\n",
    "    # Compute and print degree-related stats\n",
    "    degrees = degree(graph.edge_index[0].long(), num_nodes=graph.num_nodes)\n",
    "    average_degree = degrees.float().mean().item()\n",
    "    median_degree = np.median(degrees.numpy())\n",
    "    std_degree = degrees.float().std().item()\n",
    "\n",
    "    print(f\"Average Degree: {average_degree}\")\n",
    "    print(f\"Median Degree: {median_degree}\")\n",
    "    print(f\"Standard Deviation of Degree: {std_degree}\")\n",
    "\n",
    "    # Density is the ratio of actual edges to the maximum number of possible edges\n",
    "    num_possible_edges = graph.num_nodes * (graph.num_nodes - 1) / 2\n",
    "    density = graph.num_edges / num_possible_edges\n",
    "\n",
    "    print(f\"Density: {density:.10f}\")\n",
    "\n",
    "    # Check for NaN values in features\n",
    "    nan_mask = torch.isnan(graph.x)\n",
    "    nan_features = []\n",
    "    for feature_idx, feature_name in enumerate(features_list):\n",
    "        if nan_mask[:, feature_idx].any():\n",
    "            nan_features.append(feature_name)\n",
    "\n",
    "    print(\"Features with NaN values:\")\n",
    "    print(nan_features)\n",
    "\n",
    "def print_edge_weight_stats(edge_weights):\n",
    "    print(f\"Number of edges: {edge_weights.size(0)}\")\n",
    "    print(f\"Average edge weight: {edge_weights.float().mean().item()}\")\n",
    "    print(f\"Median edge weight: {np.median(edge_weights.numpy())}\")\n",
    "    print(f\"Standard deviation of edge weights: {edge_weights.float().std().item()}\")\n",
    "    print(f\"Maximum edge weight: {edge_weights.max().item()}\")\n",
    "    print(f\"Minimum edge weight: {edge_weights.min().item()}\")\n",
    "\n",
    "# Print graph stats\n",
    "print(\"Graph stats:\")\n",
    "print_graph_stats(graph, positive_edges_snp_snp, snp_features.columns)\n",
    "\n",
    "print(\"Edge weight stats:\")\n",
    "print_edge_weight_stats(graph.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee2415-d3e2-4269-9a3d-85516dcd379a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_histogram_of_edge_weights(edge_weights):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    weights = edge_weights.numpy()\n",
    "    \n",
    "    # Compute the histogram and normalize the counts\n",
    "    counts, bin_edges = np.histogram(weights, bins='auto')\n",
    "    counts = counts / counts.sum()\n",
    "\n",
    "    # Plot the normalized histogram\n",
    "    plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), color='#0504aa', alpha=0.7, align=\"edge\")\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xlabel('Edge Weight')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Histogram of Edge Weights')\n",
    "    plt.show()\n",
    "\n",
    "# Plot histogram\n",
    "plot_histogram_of_edge_weights(graph.edge_attr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51e96d-660c-4b4f-8ac4-9a07dba5bcc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fb79d-4729-4d4d-8dff-229108b6cac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "transform = RandomNodeSplit(split=\"train_rest\", num_val=0.35, num_test=0.35, key='y')\n",
    "graph = transform(graph)\n",
    "\n",
    "# Count the number of nodes per class in each set\n",
    "train_class_counts = Counter(graph.y[graph.train_mask].numpy())\n",
    "val_class_counts = Counter(graph.y[graph.val_mask].numpy())\n",
    "test_class_counts = Counter(graph.y[graph.test_mask].numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of nodes per class in each set:\")\n",
    "print(\"Train set:\")\n",
    "for class_label, count in train_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Validation set:\")\n",
    "for class_label, count in val_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Test set:\")\n",
    "for class_label, count in test_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "\n",
    "# Calculate and print the percentage of class 1 vs. class 0 in the test set\n",
    "total_test_nodes = sum(test_class_counts.values())\n",
    "class_0_nodes = test_class_counts[0]\n",
    "class_1_nodes = test_class_counts[1]\n",
    "class_0_percentage = (class_0_nodes / total_test_nodes) * 100\n",
    "class_1_percentage = (class_1_nodes / total_test_nodes) * 100\n",
    "print(f\"Percentage of Class 0 in test set: {class_0_percentage:.2f}%\")\n",
    "print(f\"Percentage of Class 1 in test set: {class_1_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358aee2-41a3-4301-ab75-23ccb0ff75a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b10db9-0e01-46ad-ab28-2b26e7fa3736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn.functional import binary_cross_entropy, dropout, leaky_relu\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# suppose y is your target vector\n",
    "y = graph.y.cpu().numpy()\n",
    "\n",
    "# Count number of occurrences of each class\n",
    "class_counts = np.bincount(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = 1. / class_counts\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "\n",
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_layers, num_classes, dropout_rate=0.5):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(SAGEConv(num_node_features, hidden_layers[0]))\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.layers.append(SAGEConv(hidden_layers[i - 1], hidden_layers[i]))\n",
    "        self.layers.append(SAGEConv(hidden_layers[-1], num_classes))\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.bn_layers = torch.nn.ModuleList([torch.nn.BatchNorm1d(size) for size in hidden_layers])\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for i, conv in enumerate(self.layers[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.bn_layers[i](x)\n",
    "            x = torch.nn.functional.leaky_relu(x)\n",
    "            x = torch.nn.functional.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return torch.sigmoid(x.view(-1))\n",
    "    \n",
    "    \n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets.long()].view(-1, 1)\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "            logpt = logpt * alpha_t\n",
    "        else:\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(logpt)\n",
    "        F_loss = -((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "        \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\\\n",
    "            \n",
    "            \n",
    "def train(model, data, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)[data.train_mask].squeeze()\n",
    "    loss = loss_fn(out, data.y[data.train_mask].float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data, mask, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)[mask].squeeze()\n",
    "        preds = (out > 0.5).long()\n",
    "        loss = loss_fn(out, data.y[mask].float())\n",
    "        accuracy = preds.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        precision, recall, _ = precision_recall_curve(data.y[mask].cpu(), out.cpu())\n",
    "        auprc = auc(recall, precision)\n",
    "    return loss.item(), accuracy, auprc\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialization\n",
    "    hidden_layers = [256, 256] \n",
    "    model = GraphSAGEModel(graph.num_node_features, hidden_layers, 1, dropout_rate=0.5).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    loss_fn = FocalLoss(alpha=class_weights, gamma=2)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.0001)\n",
    "\n",
    "    # Move your graph data to device\n",
    "    graph.x = graph.x.to(device)\n",
    "    graph.edge_index = graph.edge_index.to(device)\n",
    "    graph.y = graph.y.to(device)\n",
    "    graph.train_mask = graph.train_mask.to(device)\n",
    "    graph.val_mask = graph.val_mask.to(device)\n",
    "    graph.test_mask = graph.test_mask.to(device)\n",
    "\n",
    "    for epoch in range(150):\n",
    "        train_loss = train(model, graph, loss_fn, optimizer)\n",
    "        _, train_acc, _ = evaluate(model, graph, graph.train_mask, loss_fn)\n",
    "        val_loss, val_acc, val_auprc = evaluate(model, graph, graph.val_mask, loss_fn)\n",
    "        print(f'Epoch: {epoch+1}, Loss: {train_loss:.10f}, Train Acc: {train_acc:.10f}, Val Loss: {val_loss:.10f}, Val Acc: {val_acc:.10f}, Val AUPRC: {val_auprc:.10f}')\n",
    "\n",
    "        # Reduce learning rate when validation loss plateaus\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping.step(-val_auprc)  # Pass -val_auprc because we want to maximize it\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "        # Save checkpoint if is a new best\n",
    "        if -val_auprc == early_stopping.best_score:  # Save the best model based on AUPRC\n",
    "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "\n",
    "    # Load the best model back in\n",
    "    if os.path.isfile('checkpoint.pt'):\n",
    "        model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    else:\n",
    "        print(\"No checkpoint found. Creating a new file checkpoint.pt.\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "    test_loss, test_acc, test_auprc = evaluate(model, graph, graph.test_mask, loss_fn)\n",
    "    print(f'After early stopping, Test AUPRC: {test_auprc:.10f}')\n",
    "\n",
    "# Call the main function\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
