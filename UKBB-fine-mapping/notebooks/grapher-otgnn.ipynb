{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Grapher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd8be8-14cc-4d80-a944-76c5235c078b",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bc773-edf8-4567-8767-a742662cb8ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### UKBB_94traits_release1.{tsv|bed}.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef1038-e49a-4607-9c7d-5ce6daf1d94b",
   "metadata": {},
   "source": [
    "This file contains genetic variant data used in a study investigating 94 complex diseases and traits from the UK Biobank. Each row represents a variant with columns detailing characteristics such as its genomic location, allele details, association statistics, and more. It also includes indicators for linkage disequilibrium with variants failing Hardy Weinberg equilibrium or with common structural variants. This file is particularly valuable for those interested in the genetic association results and the fine-mapping of these traits and diseases.\n",
    "\n",
    "Columns:\n",
    "\n",
    "1. Chromosome: hg19 autosomes only\n",
    "2. Start: 0-indexed hg19 start position\n",
    "3. End: 0-indexed hg19 end position\n",
    "4. Variant: unique variant identifier (chr:pos:ref:alt)\n",
    "5. rsid: rsid identifier\n",
    "6. Allele1: reference allele in hg19\n",
    "7. Allele2: alternative allele in hg19\n",
    "8. Minor allele: minor allele in cohort\n",
    "9. Cohort: GWAS cohort\n",
    "10. Model_marginal: type of regression model used\n",
    "11. Method: fine-mapping method used\n",
    "12. Trait: abbreviation for phenotype in genetic association tests\n",
    "13. Region: fine-mapping region in hg19\n",
    "14. MAF: minor allele frequency in cohort\n",
    "15. Beta_marginal: marginal association effect size (effect allele: alternative)\n",
    "16. SE_marginal: standard error on marginal association effect size\n",
    "17. Chisq_marginal: test statistic for marginal association\n",
    "18. PIP: posterior probability of association from fine-mapping\n",
    "19. CS_ID: ID of 95% credible set (-1 if variant not in 95% CS)\n",
    "20. Beta_posterior: posterior expectation of true effect size (effect allele: alternative)\n",
    "21. SD_posterior: posterior standard deviation of true effect size\n",
    "22. LD_HWE: indicator for LD (R^2 > 0.6) with a variant that failed HWE (p < 10^-12) in UK10K LD\n",
    "23. LD_SV: indicator for LD (R^2 > 0.8) with a common structural variant in gnomAD European samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import ot\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "from torch_geometric.nn import global_sort_pool\n",
    "\n",
    "\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "import networkx as nx\n",
    "from ogb.io import DatasetSaver\n",
    "from ogb.linkproppred import LinkPropPredDataset\n",
    "\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f7ab7-4bf0-4dba-9428-9580338d9aa7",
   "metadata": {},
   "source": [
    "## Perform checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b729b64d-2166-4dbb-9c4c-4173e0c9e807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0+cu118\n",
      "PyTorch Geometric version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f213c8b-5dc0-468b-b7d0-72c08a6e5282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce RTX 3060 Ti (cuda)\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Desktop/geometric-omics/UKBB-fine-mapping/data/UKBB_94traits_release1.csv')\n",
    "data = data.sample(frac=0.01, random_state=42)\n",
    "\n",
    "hg19_gene_positions = pd.read_csv(\"~/Desktop/geometric-omics/UKBB-fine-mapping/data/hg19-gene-positions.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3de12a0-5352-4bbc-9d24-baf89c6b6570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['position'] = data['variant'].str.split(':').str[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e3e7c-413a-4014-a103-1762a804bc50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get geneSymbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e80872-03c8-4ab3-9a8c-7f702ec23a8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The `%%time` command at the start is a magic command in Jupyter notebooks. It times the execution of the entire cell.\n",
    "\n",
    "- The dataframes `data` and `hg19_gene_positions` are sorted by chromosome and start position. Chromosome columns are converted to string type for both dataframes.\n",
    "\n",
    "- A variable `leniency` is defined and set to 100000. This will be used later to find nearby genes within this distance.\n",
    "\n",
    "- The 'chromosome' column in the `data` dataframe and the 'chrom' column in the `hg19_gene_positions` dataframe are converted to a 'category' data type. This is done to reduce memory usage and improve performance.\n",
    "\n",
    "- An empty dictionary `gene_symbols_dict` is created. This will be used to store gene symbols corresponding to each row in the `data` dataframe.\n",
    "\n",
    "- The script then loops over each unique chromosome.\n",
    "\n",
    "  - For each chromosome, subsets of `data` and `hg19_gene_positions` are created that include only the rows corresponding to the current chromosome.\n",
    "  \n",
    "  - A KDTree is built using the start positions (`txStart`) of genes in the `hg19_gene_positions_chromosome` dataframe. KDTree is a space-partitioning data structure that allows for efficient nearest-neighbor searches.\n",
    "  \n",
    "  - The KDTree is queried for each position in the `data_chromosome` dataframe to find the nearest gene within the specified leniency. The function returns two arrays: `distances`, which holds the distances to the nearest neighbors, and `indices`, which holds the indices of these neighbors in the `hg19_gene_positions_chromosome` dataframe.\n",
    "  \n",
    "  - A list `gene_symbols` is created to store the gene symbols of the nearest genes. If no gene is found within the leniency, 'N/A' is added to the list.\n",
    "  \n",
    "  - The `gene_symbols` list is then used to populate the `gene_symbols_dict` dictionary with the index from `data_chromosome` as the key and the corresponding gene symbol as the value.\n",
    "\n",
    "- Finally, the `gene_symbols_dict` dictionary is converted to a Pandas Series and added as a new column 'geneSymbol' to the `data` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d4d183-cc94-4d72-9c95-e5347d43f760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.14 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Sort the dataframes and convert columns to string type\n",
    "data = data.sort_values(by=['chromosome', 'start'])\n",
    "data['chromosome'] = data['chromosome'].astype(str)\n",
    "\n",
    "hg19_gene_positions = hg19_gene_positions.sort_values(by=['chrom', 'txStart'])\n",
    "hg19_gene_positions['chrom'] = hg19_gene_positions['chrom'].astype(str)\n",
    "\n",
    "# Define leniency\n",
    "leniency = 100000\n",
    "\n",
    "# Convert the 'chrom' column to category type for efficient memory usage\n",
    "data['chromosome'] = data['chromosome'].astype('category')\n",
    "hg19_gene_positions['chrom'] = hg19_gene_positions['chrom'].astype('category')\n",
    "\n",
    "# Create an empty dictionary to store geneSymbols\n",
    "gene_symbols_dict = {}\n",
    "\n",
    "# Iterate over unique chromosome\n",
    "for chromosome in data['chromosome'].cat.categories:\n",
    "    # Subset data for current chromosome\n",
    "    data_chromosome = data[data['chromosome'] == chromosome]\n",
    "    hg19_gene_positions_chromosome = hg19_gene_positions[hg19_gene_positions['chrom'] == chromosome]\n",
    "\n",
    "    # Build KDTree for efficient nearest-neighbor search\n",
    "    tree = cKDTree(np.expand_dims(hg19_gene_positions_chromosome['txStart'].values, axis=1))\n",
    "\n",
    "    # Query the KDTree for nearest neighbors within the leniency\n",
    "    distances, indices = tree.query(np.expand_dims(data_chromosome['start'].values, axis=1), distance_upper_bound=leniency)\n",
    "\n",
    "    # Create a list of gene symbols\n",
    "    gene_symbols = []\n",
    "    for idx, distance in zip(indices, distances):\n",
    "        if distance == np.inf:\n",
    "            gene_symbols.append('N/A')  # or any other default value you want\n",
    "        else:\n",
    "            gene_symbols.append(hg19_gene_positions_chromosome.iloc[idx]['geneSymbol'])\n",
    "\n",
    "    # Assign geneSymbols to data dictionary\n",
    "    for idx, gene_symbol in zip(data_chromosome.index, gene_symbols):\n",
    "        gene_symbols_dict[idx] = gene_symbol\n",
    "\n",
    "# Convert the dictionary to a Series and assign it to a new column in 'data'\n",
    "data['geneSymbol'] = pd.Series(gene_symbols_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2594b60-01fa-49ad-86b0-b38c0ee4c80f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in data['geneSymbol']:  0\n",
      "Number of unique elements in data['geneSymbol']:  13158\n",
      "Number of unique elements in hg19_gene_positions['geneSymbol']:  29014\n"
     ]
    }
   ],
   "source": [
    "null_values = data['geneSymbol'].isnull().sum()\n",
    "print(\"Number of null values in data['geneSymbol']: \", null_values)\n",
    "\n",
    "unique_elements1 = data['geneSymbol'].nunique()\n",
    "print(\"Number of unique elements in data['geneSymbol']: \", unique_elements1)\n",
    "\n",
    "unique_elements2 = hg19_gene_positions['geneSymbol'].nunique()\n",
    "print(\"Number of unique elements in hg19_gene_positions['geneSymbol']: \", unique_elements2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "### Task \n",
    "Link Prediction\n",
    "\n",
    "### Node Features\n",
    "All columns from `data` dataframe:\n",
    "\n",
    "Phenotype nodes features:\n",
    "- `trait` column\n",
    "\n",
    "Gene nodes features:\n",
    "- `geneSymbol` column\n",
    "- `chromosome` column\n",
    "- `start` column\n",
    "- `end` column\n",
    "\n",
    "SNP node features:\n",
    "- `rsid` column\n",
    "- `chromosome` column\n",
    "- `position` column\n",
    "- `allele1` column\n",
    "- `allele2` column\n",
    "\n",
    "Edge features:\n",
    "- undirected\n",
    "- unweighted \n",
    "- positive if associations exist in graph\n",
    "- negative if not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf981ea3-0b4b-437f-9fc2-1210050d1ce1",
   "metadata": {},
   "source": [
    "- The `%%time` magic command is used at the start of the code block to track the execution time of the code.\n",
    "\n",
    "- The `random` library is imported, which will be used to generate random integers for creating negative edges later in the code.\n",
    "\n",
    "- Unique phenotypes, genes, and SNPs (Single Nucleotide Polymorphisms) are extracted from the given data and assigned to respective variables.\n",
    "\n",
    "- Integer indices are assigned to phenotypes, genes, and SNPs, and mappings are created in the form of dictionaries (e.g., `phenotype_to_idx`).\n",
    "\n",
    "- The node feature vectors for phenotypes, genes, and SNPs are created using the original data, by filtering, dropping duplicates, sorting values, and resetting indices.\n",
    "\n",
    "- Node type labels are created using the PyTorch tensor, where phenotypes, genes, and SNPs are marked as 0, 1, and 2 respectively.\n",
    "\n",
    "- Positive edges are created between SNPs and genes, and between genes and phenotypes. Here, an edge represents a connection or relationship between two entities (nodes). The mappings created earlier are used to convert rsid and geneSymbol to their respective indices.\n",
    "\n",
    "- Negative edges are created for SNP-Gene and Gene-Phenotype pairs. A negative edge represents a non-existing connection between two entities. Negative edges are generated randomly while ensuring that they don't coincide with the positive edges.\n",
    "\n",
    "- All positive and negative edges are combined together into a single PyTorch tensor.\n",
    "\n",
    "- Edge attributes are created as a tensor of ones with the same size as the number of edges.\n",
    "\n",
    "- The feature vectors of phenotypes, genes, and SNPs are combined together. NaN values are filled with suitable replacements, and Label Encoding is applied to categorical columns.\n",
    "\n",
    "- Numerical features are standardized using the `StandardScaler` from sklearn, and categorical columns are converted to category codes.\n",
    "\n",
    "- A PyTorch Geometric graph is created with the combined feature vectors as node features, the combined edges as the graph structure, and the tensor of ones as edge attributes.\n",
    "\n",
    "- Lastly, summary statistics about the created graph are printed, including the number of nodes, the number of positive and negative edges for SNP-Gene and Gene-Phenotype pairs, the total number of edges, node feature dimension, and node types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "849b5a45-34bc-441b-a20d-c78ff8a92d87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 105135\n",
      "Number of positive edges between SNPs and genes: 52520\n",
      "Number of positive edges between genes and phenotypes: 31791\n",
      "Number of negative edges for SNPs and genes: 525200\n",
      "Number of negative edges for genes and phenotypes: 3179100\n",
      "Number of edges: 3788611\n",
      "Node feature dimension: 6\n",
      "Node types: tensor([0, 0, 0,  ..., 2, 2, 2])\n",
      "CPU times: total: 1.86 s\n",
      "Wall time: 4.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# Create mappings for phenotypes, genes, and SNPs to integer indices\n",
    "phenotypes = data['trait'].unique()\n",
    "genes = data['geneSymbol'].unique()\n",
    "snps = data['rsid'].unique()\n",
    "phenotype_to_idx = {phenotype: idx for idx, phenotype in enumerate(phenotypes)}\n",
    "gene_to_idx = {gene: idx + len(phenotypes) for idx, gene in enumerate(genes)}\n",
    "snp_to_idx = {snp: idx + len(phenotypes) + len(genes) for idx, snp in enumerate(snps)}\n",
    "\n",
    "\n",
    "# Create node feature vectors for phenotypes, genes, and SNPs\n",
    "phenotype_features = data.loc[data['trait'].isin(phenotypes)][['trait']].drop_duplicates().sort_values(by='trait').reset_index(drop=True)\n",
    "gene_features = data.loc[data['geneSymbol'].isin(genes)][['geneSymbol', 'chromosome', 'start', 'end']].drop_duplicates().sort_values(by='geneSymbol').reset_index(drop=True)\n",
    "snp_features = data.loc[data['rsid'].isin(snps)][['rsid', 'chromosome', 'position', 'allele1', 'allele2']].drop_duplicates().sort_values(by='rsid').reset_index(drop=True)\n",
    "\n",
    "# Create node type labels\n",
    "node_types = torch.tensor([0] * len(phenotypes) + [1] * len(genes) + [2] * len(snps), dtype=torch.long)\n",
    "\n",
    "# Create positive edges between SNPs and genes\n",
    "positive_edges_snp_gene = data.loc[:, ['rsid', 'geneSymbol']].drop_duplicates()\n",
    "positive_edges_snp_gene['snp_idx'] = positive_edges_snp_gene['rsid'].map(snp_to_idx)\n",
    "positive_edges_snp_gene['gene_idx'] = positive_edges_snp_gene['geneSymbol'].map(gene_to_idx)\n",
    "positive_edges_snp_gene = positive_edges_snp_gene[['snp_idx', 'gene_idx']].values\n",
    "positive_edges_snp_gene = torch.tensor(positive_edges_snp_gene, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create positive edges between genes and phenotypes\n",
    "positive_edges_gene_phenotype = data.loc[:, ['geneSymbol', 'trait']].drop_duplicates()\n",
    "positive_edges_gene_phenotype['gene_idx'] = positive_edges_gene_phenotype['geneSymbol'].map(gene_to_idx)\n",
    "positive_edges_gene_phenotype['phenotype_idx'] = positive_edges_gene_phenotype['trait'].map(phenotype_to_idx)\n",
    "positive_edges_gene_phenotype = positive_edges_gene_phenotype[['gene_idx', 'phenotype_idx']].values\n",
    "positive_edges_gene_phenotype = torch.tensor(positive_edges_gene_phenotype, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create negative edges for SNP-Gene\n",
    "negative_edges_snp_gene = []\n",
    "positive_edges_set = set([tuple(x) for x in positive_edges_snp_gene.t().tolist()])\n",
    "for _ in range(10 * len(positive_edges_snp_gene[0])):\n",
    "    while True:\n",
    "        random_snp = random.randint(0, len(snps) - 1)\n",
    "        random_gene = random.randint(0, len(genes) - 1)\n",
    "        negative_edge = (random_snp, random_gene)\n",
    "        if negative_edge not in positive_edges_set:\n",
    "            negative_edges_snp_gene.append(negative_edge)\n",
    "            break\n",
    "negative_edges_snp_gene = torch.tensor(negative_edges_snp_gene, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create negative edges for Gene-Phenotype\n",
    "negative_edges_gene_phenotype = []\n",
    "positive_edges_set = set([tuple(x) for x in positive_edges_gene_phenotype.t().tolist()])\n",
    "for _ in range(100 * len(positive_edges_gene_phenotype[0])):\n",
    "    while True:\n",
    "        random_gene = random.randint(0, len(genes) - 1)\n",
    "        random_phenotype = random.randint(0, len(phenotypes) - 1)\n",
    "        negative_edge = (random_gene, random_phenotype)\n",
    "        if negative_edge not in positive_edges_set:\n",
    "            negative_edges_gene_phenotype.append(negative_edge)\n",
    "            break\n",
    "negative_edges_gene_phenotype = torch.tensor(negative_edges_gene_phenotype, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Combine positive and negative edges\n",
    "edges = torch.cat([positive_edges_snp_gene, positive_edges_gene_phenotype, negative_edges_snp_gene, negative_edges_gene_phenotype], dim=1)\n",
    "\n",
    "# Create edge attributes\n",
    "edge_attr = torch.ones(edges.size(1), dtype=torch.float)\n",
    "\n",
    "# Combine the feature vectors\n",
    "combined_features = pd.concat([phenotype_features, gene_features, snp_features], ignore_index=True).drop(['trait', 'geneSymbol', 'rsid'], axis=1)\n",
    "\n",
    "# Now you can fill NaNs with 'N/A'\n",
    "nan_replacements = {'chromosome': 'N/A', 'start': 0, 'end': 0, 'position': 0, 'allele1': 'N/A', 'allele2': 'N/A'}\n",
    "for col, replacement in nan_replacements.items():\n",
    "    if col in combined_features:\n",
    "        if combined_features[col].dtype.name == 'category' and replacement not in combined_features[col].cat.categories:\n",
    "            combined_features[col] = combined_features[col].cat.add_categories([replacement])\n",
    "        combined_features[col].fillna(replacement, inplace=True)\n",
    "\n",
    "# Label encoding for categorical columns\n",
    "le = LabelEncoder()\n",
    "combined_features = combined_features.apply(lambda col: le.fit_transform(col.astype(str)) if col.dtype == 'object' else col)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['start', 'end', 'position']\n",
    "categorical_columns = ['chromosome', 'allele1', 'allele2']\n",
    "for col in categorical_columns:\n",
    "    combined_features[col] = combined_features[col].astype('category').cat.codes\n",
    "\n",
    "features = torch.tensor(combined_features.values, dtype=torch.float)\n",
    "\n",
    "# Create the PyTorch Geometric graph\n",
    "graph = Data(x=features, edge_index=edges, edge_attr=edge_attr)\n",
    "graph.node_types = node_types\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of positive edges between SNPs and genes: {positive_edges_snp_gene.size(1)}\")\n",
    "print(f\"Number of positive edges between genes and phenotypes: {positive_edges_gene_phenotype.size(1)}\")\n",
    "print(f\"Number of negative edges for SNPs and genes: {negative_edges_snp_gene.size(1)}\")\n",
    "print(f\"Number of negative edges for genes and phenotypes: {negative_edges_gene_phenotype.size(1)}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "print(f\"Node types: {graph.node_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cae52-2493-43ae-9e27-e7dc67c3a7f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96d7d0e2-80bf-4a82-9c1c-6aac6307aab4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any NaN values in features? False\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in features\n",
    "nan_in_features = torch.isnan(graph.x).any().item()\n",
    "print(f\"Are there any NaN values in features? {nan_in_features}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2a08f05-3012-4d85-85eb-3f5cb6a6955c",
   "metadata": {
    "tags": []
   },
   "source": [
    "def print_graph_stats(graph, phenotypes, snps):\n",
    "    G = nx.Graph()\n",
    "    edge_weights = graph.edge_attr.view(-1)  # ensure that edge_attr is a 1D tensor\n",
    "    for edge, weight in zip(graph.edge_index.t().numpy(), edge_weights):\n",
    "        G.add_edge(edge[0], edge[1], weight=weight.item())  # use the pip value as the edge weight\n",
    "\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_phenotypes = len(phenotypes)\n",
    "    num_genes = len(genes)\n",
    "    num_snps = len(snps)\n",
    "    num_positive_edges = sum(weight > 0 for weight in edge_weights)\n",
    "    num_negative_edges = sum(weight == 0 for weight in edge_weights)\n",
    "    num_edges = G.number_of_edges()\n",
    "    num_connected_components = nx.number_connected_components(G)\n",
    "    average_degree = np.mean([degree for _, degree in G.degree()])\n",
    "    median_degree = np.median([degree for _, degree in G.degree()])\n",
    "    std_degree = np.std([degree for _, degree in G.degree()])\n",
    "    density = nx.density(G)\n",
    "    assortativity = nx.degree_assortativity_coefficient(G)\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]\n",
    "    average_weight = np.mean(edge_weights)\n",
    "    median_weight = np.median(edge_weights)\n",
    "    std_weight = np.std(edge_weights)\n",
    "\n",
    "    print(f\"Number of nodes: {num_nodes}\")\n",
    "    print(\"Number of SNP nodes:\", num_snps)\n",
    "    print(\"Number of Gene nodes:\", num_genes)\n",
    "    print(\"Number of Phenotype nodes:\", num_phenotypes)\n",
    "    print(f\"Number of positive edges: {num_positive_edges}\")\n",
    "    print(f\"Number of negative edges: {num_negative_edges}\")\n",
    "    print(f\"Number of edges: {num_edges}\")\n",
    "    print(f\"Number of connected components: {num_connected_components}\")\n",
    "    print(f\"Average degree: {average_degree:.2f}\")\n",
    "    print(f\"Median degree: {median_degree}\")\n",
    "    print(f\"Standard deviation of degree: {std_degree:.2f}\")\n",
    "    print(f\"Density: {density:.10f}\")\n",
    "    print(f\"Assortativity: {assortativity:.10f}\")\n",
    "    print(f\"Average edge weight: {average_weight:.2f}\")\n",
    "    print(f\"Median edge weight: {median_weight}\")\n",
    "    print(f\"Standard deviation of edge weight: {std_weight:.2f}\")\n",
    "\n",
    "# Print\n",
    "print(\"Graph stats:\")\n",
    "print_graph_stats(graph, phenotypes, snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ab9a9e-7a28-4d0e-b17f-05a64f58e3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum degree: 1.0\n",
      "Maximum degree: 324.0\n",
      "Average degree: 57.60218811035156\n",
      "Number of nodes with degree 1: 13254\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "# Compute the degree of each node\n",
    "degrees = degree(graph.edge_index[0])\n",
    "\n",
    "# Print statistics about the degrees\n",
    "print(f\"Minimum degree: {degrees.min().item()}\")\n",
    "print(f\"Maximum degree: {degrees.max().item()}\")\n",
    "print(f\"Average degree: {degrees.float().mean().item()}\")\n",
    "\n",
    "# Count the number of nodes with degree 1\n",
    "num_nodes_degree_1 = (degrees == 1).sum().item()\n",
    "print(f\"Number of nodes with degree 1: {num_nodes_degree_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51e96d-660c-4b4f-8ac4-9a07dba5bcc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d110955-0ad0-41d7-81fc-e41226316964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[105135, 6], edge_index=[2, 31048], edge_attr=[31048], node_types=[65772], edge_label=[31048], edge_label_index=[2, 31048])\n",
      "Data(x=[105135, 6], edge_index=[2, 31048], edge_attr=[31048], node_types=[65772], edge_label=[62090], edge_label_index=[2, 62090])\n",
      "Data(x=[105135, 6], edge_index=[2, 93138], edge_attr=[93138], node_types=[65772], edge_label=[62090], edge_label_index=[2, 62090])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit \n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.4, num_test=0.4, is_undirected=True)\n",
    "graph_train, graph_val, graph_test = transform(graph)\n",
    "\n",
    "print(graph_train)\n",
    "print(graph_val)\n",
    "print(graph_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8eaae7-0ff7-4ce7-bb7c-7fffc8632d00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc897c35-4b85-4a9a-8b02-e6553f6bb264",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The script begins with a comment stating the task (Link Prediction) and providing information about node types and the dimension of the node feature vector.\n",
    "\n",
    "- PyTorch's GPU memory cache is emptied using `torch.cuda.empty_cache()` to free up space for computation.\n",
    "\n",
    "- A Graph Convolutional Network (GCN) model is defined as a subclass of `torch.nn.Module`. This network consists of two convolutional layers (`GCNConv`). The `forward` method defines how input data passes through these layers, which includes a ReLU activation function and a dropout layer for regularization.\n",
    "\n",
    "- The script checks if CUDA is available for GPU computation. If it is, the device is set to \"cuda\"; otherwise, it's set to \"cpu\".\n",
    "\n",
    "- An instance of the GCN model is created with 16 hidden channels and is moved to the chosen device.\n",
    "\n",
    "- The training, validation, and test graphs are also moved to the chosen device.\n",
    "\n",
    "- An Adam optimizer is initialized with the model parameters, a learning rate of 0.01, and a weight decay of 5e-4.\n",
    "\n",
    "- A training function is defined, which trains the model on the training graph. It computes a binary cross-entropy loss based on the predictions for positive and negative edges and backpropagates the loss to update the model parameters.\n",
    "\n",
    "- An evaluation function is defined, which evaluates the model on a given graph. It computes the ROC-AUC, Mean Reciprocal Rank (MRR), and Hits@5 metrics based on the model's predictions.\n",
    "\n",
    "- The MRR and Hits@5 computation functions are also defined. They compute these metrics based on the model's predictions and the true labels.\n",
    "\n",
    "- Maximum values for ROC-AUC, MRR, and Hits@5 on both validation and test sets are initialized to negative infinity.\n",
    "\n",
    "- The model is then trained for 150 epochs. In each epoch, the training function is called to train the model, and the evaluation function is called to evaluate it on the validation graph. The maximum ROC-AUC, MRR, and Hits@5 on the validation set are updated if the current epoch's scores are higher.\n",
    "\n",
    "- The loss, ROC-AUC, MRR, and Hits@5 for each epoch are printed for tracking the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744860e-e107-4495-9e88-79fe72ac7db7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 20.8158, Val ROC-AUC: 0.5000000000, Val MRR: 1.0000000000, Val Hits@5: 1\n",
      "Epoch: 2, Loss: 20.6790, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val Hits@5: 1\n",
      "Epoch: 3, Loss: 20.5951, Val ROC-AUC: 0.5000000000, Val MRR: 1.0000000000, Val Hits@5: 1\n",
      "Epoch: 4, Loss: 20.5504, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val Hits@5: 1\n",
      "Epoch: 5, Loss: 20.5312, Val ROC-AUC: 0.5000000000, Val MRR: 0.1428571429, Val Hits@5: 0\n",
      "Epoch: 6, Loss: 20.5267, Val ROC-AUC: 0.5000000000, Val MRR: 1.0000000000, Val Hits@5: 1\n",
      "Epoch: 7, Loss: 20.5291, Val ROC-AUC: 0.5000000000, Val MRR: 0.3333333333, Val Hits@5: 1\n",
      "Epoch: 8, Loss: 20.5333, Val ROC-AUC: 0.5000000000, Val MRR: 1.0000000000, Val Hits@5: 1\n",
      "Epoch: 9, Loss: 20.5366, Val ROC-AUC: 0.5000000000, Val MRR: 1.0000000000, Val Hits@5: 1\n",
      "Epoch: 10, Loss: 20.5377, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val Hits@5: 1\n"
     ]
    }
   ],
   "source": [
    "# The number of hidden channels (features) for each point in the point cloud.\n",
    "HIDDEN_CHANNELS = 6\n",
    "\n",
    "# The number of point clouds to be learned by the model.\n",
    "NUM_POINT_CLOUDS = 94\n",
    "\n",
    "# The number of points in each point cloud.\n",
    "PC_SIZE = 6\n",
    "\n",
    "# Define the device for computation (GPU if available, else CPU).\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# The learning rate for the Adam optimizer.\n",
    "LR = 0.01\n",
    "\n",
    "# The weight decay (L2 regularization) for the Adam optimizer.\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "# The number of epochs (full passes over the training data) to train the model.\n",
    "EPOCHS = 150\n",
    "\n",
    "\n",
    "def normalize_point_clouds(pc):\n",
    "    mean = torch.mean(pc, dim=0)\n",
    "    pc = pc - mean.unsqueeze(0)  # subtract the mean\n",
    "    std = torch.std(pc, dim=0)\n",
    "    std = torch.max(std, torch.tensor(1e-6).to(pc.device))  # ensure the denominator is non-zero\n",
    "    pc = pc / std.unsqueeze(0)  # divide by standard deviation\n",
    "    return pc\n",
    "\n",
    "class OT_GNN(nn.Module):\n",
    "    def __init__(self, hidden_channels, num_point_clouds, pc_size, device):\n",
    "        super(OT_GNN, self).__init__()\n",
    "        self.pc_list = nn.ParameterList([\n",
    "            Parameter(torch.nn.init.orthogonal_(torch.empty(pc_size, hidden_channels, device=device)))\n",
    "            for _ in range(num_point_clouds)\n",
    "        ])\n",
    "        # You should replace this loss with a suitable PyTorch native loss function\n",
    "        self.loss = nn.MSELoss()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        pc_embeddings = torch.cat([\n",
    "            normalize_point_clouds(pc).unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "            for pc in self.pc_list\n",
    "        ], dim=1)\n",
    "\n",
    "        # Compute your embeddings here instead of calculating Wasserstein distance.\n",
    "        return pc_embeddings\n",
    "\n",
    "device = DEVICE\n",
    "model = OT_GNN(hidden_channels=HIDDEN_CHANNELS, num_point_clouds=NUM_POINT_CLOUDS, pc_size=PC_SIZE, device=DEVICE).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def train(graph, noise_contrastive_weight=0.1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, edge_index = graph.x.float().to(device), graph.edge_index.long().to(device)\n",
    "    z = model(x)\n",
    "    pos_edge_index = edge_index\n",
    "\n",
    "    pos = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=-1)\n",
    "    neg_edge_index = negative_sampling(edge_index, num_nodes=z.size(0), num_neg_samples=edge_index.size(1), method=\"sparse\", force_undirected=False)\n",
    "    neg = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    logits = torch.cat([pos, neg], dim=0)\n",
    "    targets = torch.cat([torch.ones(pos.size(0), dtype=torch.float32),\n",
    "                         torch.zeros(neg.size(0), dtype=torch.float32)], dim=0).to(device)\n",
    "\n",
    "    logits = logits.view(-1) if logits.dim() == 1 else logits\n",
    "    targets = targets.view(-1) if targets.dim() == 1 else targets\n",
    "    logits[torch.isnan(logits)] = 0.0\n",
    "\n",
    "    # Reshape targets to match the size of logits\n",
    "    targets = targets.unsqueeze(1).expand_as(logits)\n",
    "\n",
    "    # Compute the standard loss\n",
    "    loss = model.loss(logits, targets)\n",
    "\n",
    "    # Compute the noise contrastive loss\n",
    "    noise_contrastive_loss = -torch.log(torch.sigmoid(logits)).mean()\n",
    "\n",
    "    # Combine the two losses\n",
    "    total_loss = loss + noise_contrastive_weight * noise_contrastive_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return total_loss.item()\n",
    "\n",
    "\n",
    "def evaluate(graph):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, edge_index = graph.x.float().to(device), graph.edge_index.long().to(device)\n",
    "        z = model(x)\n",
    "\n",
    "        pos = torch.sigmoid((z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "        neg_edge_index = negative_sampling(edge_index, num_nodes=graph.num_nodes, num_neg_samples=edge_index.size(1), method=\"sparse\")\n",
    "        neg = torch.sigmoid((z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "\n",
    "        # sort labels in descending order\n",
    "        pos_target, neg_target = torch.ones_like(pos).cpu(), torch.zeros_like(neg).cpu()\n",
    "        target = torch.cat([pos_target, neg_target], dim=0).cpu()\n",
    "        pred = torch.cat([pos, neg], dim=0).cpu()\n",
    "\n",
    "        # calculate AUC, MRR, and Hits@5\n",
    "        auc = roc_auc_score(target.numpy(), pred.numpy())\n",
    "        mrr = compute_mrr(pred.numpy(), target.numpy())\n",
    "        hits_at_5 = compute_hits_at_k(pred.numpy(), target.numpy(), 5)\n",
    "\n",
    "        return auc, mrr, hits_at_5\n",
    "\n",
    "\n",
    "\n",
    "def compute_mrr(preds, true_labels):\n",
    "    pos_preds = preds[:len(true_labels)]\n",
    "    sorted_idx = np.argsort(pos_preds)[::-1]\n",
    "    for i, idx in enumerate(sorted_idx):\n",
    "        if true_labels[idx] == 1:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def compute_hits_at_k(preds, true_labels, k=5):\n",
    "    pos_preds = preds[:len(true_labels)]\n",
    "    sorted_idx = np.argsort(pos_preds)[::-1]\n",
    "    hits = 0\n",
    "    for idx in sorted_idx[:k]:\n",
    "        if true_labels[idx] == 1:\n",
    "            hits = 1\n",
    "            break\n",
    "    return hits\n",
    "\n",
    "# performance monitoring\n",
    "val_metrics = {\n",
    "    \"roc_auc\": [],\n",
    "    \"mrr\": [],\n",
    "    \"hits_at_5\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(graph_train)\n",
    "    val_roc_auc, val_mrr, val_hits_at_5 = evaluate(graph_val)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss:.4f}, Val ROC-AUC: {val_roc_auc:.10f}, Val MRR: {val_mrr:.10f}, Val Hits@5: {val_hits_at_5}\")\n",
    "\n",
    "    # store metrics\n",
    "    val_metrics[\"roc_auc\"].append(val_roc_auc)\n",
    "    val_metrics[\"mrr\"].append(val_mrr)\n",
    "    val_metrics[\"hits_at_5\"].append(val_hits_at_5)\n",
    "\n",
    "max_val_roc_auc = max(val_metrics[\"roc_auc\"])\n",
    "max_val_mrr = max(val_metrics[\"mrr\"])\n",
    "max_val_hits5 = max(val_metrics[\"hits_at_5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d432600-1aa5-41ac-9c0d-a6d833acf505",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55930699-10ed-4220-8144-139098ecf0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_roc_auc, val_mrr, val_hits5 = evaluate(graph_val.edge_index, graph_val)\n",
    "test_roc_auc, test_mrr, test_hits5 = evaluate(graph_test.edge_index, graph_test)\n",
    "\n",
    "max_test_roc_auc = max(max_test_roc_auc, test_roc_auc)\n",
    "max_test_mrr = max(max_test_mrr, test_mrr)\n",
    "max_test_hits5 = max(max_test_hits5, test_hits5)\n",
    "\n",
    "print(f\"Maximum Validation ROC-AUC: {max_val_roc_auc:.10f}\")\n",
    "print(f\"Maximum Validation MRR: {max_val_mrr:.10f}\")\n",
    "print(f\"Maximum Validation Hits@5: {max_val_hits5:.10f}\")\n",
    "\n",
    "print(f\"Maximum Test ROC-AUC: {max_test_roc_auc:.10f}\")\n",
    "print(f\"Maximum Test MRR: {max_test_mrr:.10f}\")\n",
    "print(f\"Maximum Test Hits@5: {max_test_hits5:.10f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
