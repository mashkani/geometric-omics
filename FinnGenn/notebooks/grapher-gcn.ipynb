{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Grapher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd8be8-14cc-4d80-a944-76c5235c078b",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bc773-edf8-4567-8767-a742662cb8ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [FinnGen](https://finngen.gitbook.io/documentation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef1038-e49a-4607-9c7d-5ce6daf1d94b",
   "metadata": {},
   "source": [
    "- The FinnGen research project is an expedition to the frontier of genomics and medicine, with significant discoveries potentially arising from any one of Finland’s 500,000 biomedical pioneers.\n",
    "- The project brings together a nation-wide network of Finnish biobanks, with every Finn able to participate in the study by giving biobank consent.\n",
    "- As of the last update, there were 589,000 samples available, with a goal to reach 520,000 by 2023. The latest data freeze included combined genotype and health registry data from 473,681 individuals.\n",
    "- The study utilizes samples collected by a nationwide network of Finnish biobanks and combines genome information with digital health care data from national health registries【8†source】.\n",
    "- There's a need for samples from all over Finland as solutions in the field of personalized healthcare can be found only by looking at large populations. Every Finn can be a part of the FinnGen study by giving a biobank consent.\n",
    "- The genome data produced during the project is owned by the Finnish biobanks and remains available for research purposes. The medical breakthroughs that arise from the project are expected to benefit health care systems and patients globally.\n",
    "- The FinnGen research project is collaborative, involving all the same actors as drug development, with the aim to speed up the emergence of new innovations.\n",
    "- The project's data freeze 9 results and summary statistics are now available, consisting of over 377,200 individuals, almost 20.2 M variants, and 2,272 disease endpoints. Results can be browsed online using the FinnGen web browser, and the summary statistics downloaded.\n",
    "- The University of Helsinki is the organization responsible for the study, and the nationwide network of Finnish biobanks is participating in the study, thus covering the whole of Finland. The Helsinki Biobank coordinates the sample collection.\n",
    "- For more information, the project can be contacted at finngen-info@helsinki.fi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1805a-55dc-409c-9ccf-512eb1f88667",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8ec1d-73be-4b8c-95dc-79489e84998b",
   "metadata": {},
   "source": [
    "Here's the summary documentation for the DataFrame in bullet format:\n",
    "\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located.\n",
    "\n",
    "- `pos`: This is the position of the genetic variant on the chromosome.\n",
    "\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant.\n",
    "\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population.\n",
    "\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group.\n",
    "\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group.\n",
    "\n",
    "- `causal`: This binary column indicates whether the variant is determined to be causal (1) or not (0).\n",
    "\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "\n",
    "import networkx as nx\n",
    "from ogb.io import DatasetSaver\n",
    "from ogb.linkproppred import LinkPropPredDataset\n",
    "\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f7ab7-4bf0-4dba-9428-9580338d9aa7",
   "metadata": {},
   "source": [
    "## Perform checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b729b64d-2166-4dbb-9c4c-4173e0c9e807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0+cu118\n",
      "PyTorch Geometric version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f213c8b-5dc0-468b-b7d0-72c08a6e5282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce RTX 3060 Ti (cuda)\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data and create new rows for each gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Desktop/geometric-omics/FinnGenn/data/gwas-causal.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assuming your DataFrame is called data and the relevant column is 'nearest_genes'\n",
    "# First, let's split the gene names in the 'nearest_genes' column\n",
    "split_genes = data['nearest_genes'].str.split(',')\n",
    "\n",
    "# Flatten the list of split gene names\n",
    "flat_genes = [item for sublist in split_genes for item in sublist]\n",
    "\n",
    "# Then, we create a new DataFrame by repeating rows and substituting the gene names\n",
    "data_new = (data.loc[data.index.repeat(split_genes.str.len())]\n",
    "            .assign(nearest_genes=flat_genes))\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513cafa0-f3f8-47b5-9d22-96062449edde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>rsids</th>\n",
       "      <th>nearest_genes</th>\n",
       "      <th>pval</th>\n",
       "      <th>mlogp</th>\n",
       "      <th>beta</th>\n",
       "      <th>sebeta</th>\n",
       "      <th>af_alt</th>\n",
       "      <th>af_alt_cases</th>\n",
       "      <th>af_alt_controls</th>\n",
       "      <th>causal</th>\n",
       "      <th>trait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13668</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>rs2691328</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.106658</td>\n",
       "      <td>0.972006</td>\n",
       "      <td>-0.114822</td>\n",
       "      <td>0.071168</td>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0</td>\n",
       "      <td>I9_HYPTENS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14773</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>rs878915777</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.620115</td>\n",
       "      <td>0.207528</td>\n",
       "      <td>-0.021548</td>\n",
       "      <td>0.043470</td>\n",
       "      <td>0.013501</td>\n",
       "      <td>0.013448</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0</td>\n",
       "      <td>I9_HYPTENS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15585</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>rs533630043</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.065689</td>\n",
       "      <td>-0.023716</td>\n",
       "      <td>0.134105</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0</td>\n",
       "      <td>I9_HYPTENS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>16549</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>rs1262014613</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.321844</td>\n",
       "      <td>0.492355</td>\n",
       "      <td>-0.215787</td>\n",
       "      <td>0.217818</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0</td>\n",
       "      <td>I9_HYPTENS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>16567</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>rs1194064194</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.764225</td>\n",
       "      <td>0.116779</td>\n",
       "      <td>0.021523</td>\n",
       "      <td>0.071757</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0.004207</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>0</td>\n",
       "      <td>I9_HYPTENS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #chrom    pos ref alt         rsids nearest_genes      pval     mlogp   \n",
       "0       1  13668   G   A     rs2691328         OR4F5  0.106658  0.972006  \\\n",
       "1       1  14773   C   T   rs878915777         OR4F5  0.620115  0.207528   \n",
       "2       1  15585   G   A   rs533630043         OR4F5  0.859628  0.065689   \n",
       "3       1  16549   T   C  rs1262014613         OR4F5  0.321844  0.492355   \n",
       "4       1  16567   G   C  rs1194064194         OR4F5  0.764225  0.116779   \n",
       "\n",
       "       beta    sebeta    af_alt  af_alt_cases  af_alt_controls  causal   \n",
       "0 -0.114822  0.071168  0.005846      0.005683         0.005914       0  \\\n",
       "1 -0.021548  0.043470  0.013501      0.013448         0.013524       0   \n",
       "2 -0.023716  0.134105  0.001112      0.001117         0.001109       0   \n",
       "3 -0.215787  0.217818  0.000563      0.000556         0.000566       0   \n",
       "4  0.021523  0.071757  0.004192      0.004207         0.004186       0   \n",
       "\n",
       "        trait  \n",
       "0  I9_HYPTENS  \n",
       "1  I9_HYPTENS  \n",
       "2  I9_HYPTENS  \n",
       "3  I9_HYPTENS  \n",
       "4  I9_HYPTENS  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "Task\n",
    "- causal link prediction \n",
    "- can a GNN learn how to predict the causal SNP?\n",
    "\n",
    "Phenotype nodes features:\n",
    "- `trait` column\n",
    "\n",
    "SNP node features:\n",
    "- `rsids` column\n",
    "- `nearest_genes` column\n",
    "- `#chrom` column\n",
    "- `pos` column\n",
    "- `ref` column\n",
    "- `alt` column\n",
    "- `beta` column\n",
    "- `sebeta` column\n",
    "- `af_alt` column\n",
    "- `af_alt_cases` column\n",
    "\n",
    "Edge features:\n",
    "- undirected \n",
    "- positive if `data['causal'] = 1`\n",
    "- negative if `data['causal'] = 0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849b5a45-34bc-441b-a20d-c78ff8a92d87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 11s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create mappings for phenotypes and SNPs to integer indices\n",
    "phenotypes = data['trait'].unique()\n",
    "snps = data['rsids'].unique()\n",
    "phenotype_to_idx = {phenotype: idx for idx, phenotype in enumerate(phenotypes)}\n",
    "snp_to_idx = {snp: idx + len(phenotypes) for idx, snp in enumerate(snps)}\n",
    "\n",
    "# Create node feature vectors for phenotypes and SNPs\n",
    "phenotype_features = data.loc[data['trait'].isin(phenotypes)][['trait']].drop_duplicates().sort_values(by='trait').reset_index(drop=True)\n",
    "snp_features = data.loc[data['rsids'].isin(snps)][['rsids', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'beta', 'sebeta', 'af_alt', 'af_alt_cases']].drop_duplicates().sort_values(by='rsids').reset_index(drop=True)\n",
    "\n",
    "# Create node type labels\n",
    "node_types = torch.tensor([0] * len(phenotypes) + [1] * len(snps), dtype=torch.long)\n",
    "\n",
    "# Edge creation based on 'causal' column\n",
    "edges = data[['rsids', 'trait', 'causal']].drop_duplicates()\n",
    "\n",
    "edges['snp_idx'] = edges['rsids'].map(snp_to_idx)\n",
    "edges['phenotype_idx'] = edges['trait'].map(phenotype_to_idx)\n",
    "\n",
    "# Create positive and negative edges for SNP-Phenotype\n",
    "positive_edges_snp_phenotype = edges.loc[edges['causal'] == 1, ['snp_idx', 'phenotype_idx']].values\n",
    "negative_edges_snp_phenotype = edges.loc[edges['causal'] == 0, ['snp_idx', 'phenotype_idx']].values\n",
    "\n",
    "positive_edges_snp_phenotype = torch.tensor(positive_edges_snp_phenotype, dtype=torch.long).t().contiguous()\n",
    "negative_edges_snp_phenotype = torch.tensor(negative_edges_snp_phenotype, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Combine edges\n",
    "edges = torch.cat([positive_edges_snp_phenotype, negative_edges_snp_phenotype], dim=1)\n",
    "\n",
    "# Create edge attributes\n",
    "edge_attr = torch.ones(edges.size(1), dtype=torch.float)\n",
    "edge_attr[len(positive_edges_snp_phenotype[0]):] *= -1  # Make non-causal edges negative\n",
    "\n",
    "# Combine the feature vectors\n",
    "combined_features = pd.concat([phenotype_features, snp_features], ignore_index=True).drop(['trait', 'rsids'], axis=1)\n",
    "\n",
    "# Now you can fill NaNs with 'N/A' or 0 for numerical columns\n",
    "nan_replacements = {'nearest_genes': 'N/A', '#chrom': 'N/A', 'pos': 0, 'ref': 'N/A', 'alt': 'N/A', 'beta': 0, 'sebeta': 0, 'af_alt': 0, 'af_alt_cases': 0}\n",
    "for col, replacement in nan_replacements.items():\n",
    "    if col in combined_features:\n",
    "        combined_features[col].fillna(replacement, inplace=True)\n",
    "\n",
    "# Label encoding for categorical columns\n",
    "le = LabelEncoder()\n",
    "categorical_columns = ['nearest_genes', '#chrom', 'ref', 'alt']\n",
    "for col in categorical_columns:\n",
    "    combined_features[col] = le.fit_transform(combined_features[col].astype(str))\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['pos', 'beta', 'sebeta', 'af_alt', 'af_alt_cases']\n",
    "for col in numerical_columns:\n",
    "    combined_features[col] = scaler.fit_transform(combined_features[[col]])\n",
    "\n",
    "features = torch.tensor(combined_features.values, dtype=torch.float)\n",
    "\n",
    "# Create the PyTorch Geometric graph\n",
    "graph = Data(x=features, edge_index=edges, edge_attr=edge_attr)\n",
    "graph.node_types = node_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cae52-2493-43ae-9e27-e7dc67c3a7f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fbe125d-9b67-4a48-a8f0-6aa8542249e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive edges: 276\n",
      "Number of negative edges: 18709344\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative edges\n",
    "num_positive_edges = (edge_attr == 1).sum().item()\n",
    "num_negative_edges = (edge_attr == -1).sum().item()\n",
    "\n",
    "print(\"Number of positive edges:\", num_positive_edges)\n",
    "print(\"Number of negative edges:\", num_negative_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0709aa36-0d8a-45de-b5c5-1b287fad15ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph stats:\n",
      "Number of nodes: 20565859\n",
      "Number of 0 nodes: 1\n",
      "Number of 1 nodes: 18709619\n",
      "Number of positive edges between SNPs and phenotypes: 276\n",
      "Number of negative edges between SNPs and phenotypes: 18709344\n",
      "Number of edges: 18709620\n",
      "Node feature dimension: 9\n",
      "\n",
      "0 node stats:\n",
      "Average degree: 0.00\n",
      "Median degree: 0.00\n",
      "Standard deviation of degree: nan\n",
      "\n",
      "1 node stats:\n",
      "Average degree: 1.00\n",
      "Median degree: 1.00\n",
      "Standard deviation of degree: 0.00\n",
      "Density: 0.0000000885\n",
      "Are there any NaN values in features? False\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "def print_graph_stats(graph, positive_edges_snp_phenotype, negative_edges_snp_phenotype):\n",
    "    node_types = np.unique(graph.node_types.numpy(), return_counts=True)\n",
    "\n",
    "    print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "    for node_type, count in zip(*node_types):\n",
    "        print(f\"Number of {node_type} nodes: {count}\")\n",
    "    print(f\"Number of positive edges between SNPs and phenotypes: {positive_edges_snp_phenotype.size(1)}\")\n",
    "    print(f\"Number of negative edges between SNPs and phenotypes: {negative_edges_snp_phenotype.size(1)}\")\n",
    "    print(f\"Number of edges: {graph.num_edges}\")\n",
    "    print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "    # Compute and print degree-related stats for each node type\n",
    "    for node_type in node_types[0]:\n",
    "        node_indices = np.where(graph.node_types.numpy() == node_type)[0]\n",
    "        degrees = degree(graph.edge_index[0], num_nodes=graph.num_nodes)[node_indices]\n",
    "        average_degree = degrees.float().mean().item()\n",
    "        median_degree = np.median(degrees.numpy())\n",
    "        std_degree = degrees.float().std().item()\n",
    "\n",
    "        print(f\"\\n{node_type} node stats:\")\n",
    "        print(f\"Average degree: {average_degree:.2f}\")\n",
    "        print(f\"Median degree: {median_degree:.2f}\")\n",
    "        print(f\"Standard deviation of degree: {std_degree:.2f}\")\n",
    "\n",
    "    # Density is the ratio of actual edges to the maximum number of possible edges\n",
    "    num_possible_edges = graph.num_nodes * (graph.num_nodes - 1) / 2\n",
    "    density = graph.num_edges / num_possible_edges\n",
    "    print(f\"Density: {density:.10f}\")\n",
    "\n",
    "    # Check for NaN values in features\n",
    "    nan_in_features = torch.isnan(graph.x).any().item()\n",
    "    print(f\"Are there any NaN values in features? {nan_in_features}\")\n",
    "\n",
    "# Print\n",
    "print(\"Graph stats:\")\n",
    "print_graph_stats(graph, positive_edges_snp_phenotype, negative_edges_snp_phenotype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51e96d-660c-4b4f-8ac4-9a07dba5bcc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9ecd3-36c5-45d2-8038-493e762b20dc",
   "metadata": {},
   "source": [
    "- **Importing Modules**\n",
    "\n",
    "  The script begins by importing the necessary Python libraries. It uses `random` for shuffling data, `torch` for handling tensors, and `torch_geometric.data` for its `Data` class, which is used to represent graph data.\n",
    "\n",
    "- **Constants**\n",
    "\n",
    "  The ratios for splitting the data into training, validation, and testing sets are defined. Both positive and negative edges are split equally into three parts.\n",
    "\n",
    "- **Calculating Sample Size for Each Split**\n",
    "\n",
    "  The number of samples for each set (training, validation, testing) are calculated separately for SNP-Gene and Gene-Trait edges. This is done for both positive and negative edges.\n",
    "\n",
    "- **Shuffling the Edges**\n",
    "\n",
    "  The edges for both positive and negative SNP-Gene and Gene-Trait data are shuffled. This ensures that the training, validation, and testing sets get a fair representation of the entire dataset.\n",
    "\n",
    "- **Splitting the Edges**\n",
    "\n",
    "  Both SNP-Gene and Gene-Trait edges are split according to the previously calculated sample sizes. The split is performed separately for both positive and negative edges.\n",
    "\n",
    "- **Combining SNP-Gene and Gene-Trait Edges**\n",
    "\n",
    "  After the split, the SNP-Gene and Gene-Trait edges are combined back together to form the final sets of edges for the training, validation, and testing sets.\n",
    "\n",
    "- **Converting Edges back to Tensors**\n",
    "\n",
    "  The lists of edges are then converted back into torch tensors. This conversion prepares the data for future operations with PyTorch's machine learning functionalities.\n",
    "\n",
    "- **Creating Graphs**\n",
    "\n",
    "  Graphs for training, validation, and testing sets are created. These graphs are instances of the `Data` class from the `torch_geometric.data` module. The graphs contain node features, edge indices, and edge attributes.\n",
    "\n",
    "- **Setting Node Types**\n",
    "\n",
    "  The node types for each graph are set. The node type information can be used for tasks such as node classification.\n",
    "\n",
    "- **Printing the Graphs**\n",
    "\n",
    "  Finally, the script prints out the graphs for the training, validation, and testing sets. This helps to ensure that the data has been correctly processed and is ready for the machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d110955-0ad0-41d7-81fc-e41226316964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Train:\n",
      "Data(x=[20565859, 9], edge_index=[2, 3118270], edge_attr=[18709620], node_types=[18709620])\n",
      "\n",
      "Graph Validation:\n",
      "Data(x=[20565859, 9], edge_index=[2, 6236540], edge_attr=[18709620], node_types=[18709620])\n",
      "\n",
      "Graph Test:\n",
      "Data(x=[20565859, 9], edge_index=[2, 9354810], edge_attr=[18709620], node_types=[18709620])\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "pos_train_ratio = 1/6\n",
    "pos_val_ratio = 2/6\n",
    "pos_test_ratio = 3/6\n",
    "\n",
    "neg_train_ratio = 1/6\n",
    "neg_val_ratio = 2/6\n",
    "neg_test_ratio = 3/6\n",
    "\n",
    "# Calculate the number of samples for each split\n",
    "num_positive_snp_phenotype_train = int(positive_edges_snp_phenotype.size(1) * pos_train_ratio)\n",
    "num_positive_snp_phenotype_val = int(positive_edges_snp_phenotype.size(1) * pos_val_ratio)\n",
    "num_positive_snp_phenotype_test = positive_edges_snp_phenotype.size(1) - num_positive_snp_phenotype_train - num_positive_snp_phenotype_val\n",
    "\n",
    "num_negative_snp_phenotype_train = int(negative_edges_snp_phenotype.size(1) * neg_train_ratio)\n",
    "num_negative_snp_phenotype_val = int(negative_edges_snp_phenotype.size(1) * neg_val_ratio)\n",
    "num_negative_snp_phenotype_test = negative_edges_snp_phenotype.size(1) - num_negative_snp_phenotype_train - num_negative_snp_phenotype_val\n",
    "\n",
    "# Shuffle the positive and negative edges\n",
    "positive_edges_snp_phenotype = positive_edges_snp_phenotype.t().tolist()\n",
    "random.shuffle(positive_edges_snp_phenotype)\n",
    "\n",
    "negative_edges_snp_phenotype = negative_edges_snp_phenotype.t().tolist()\n",
    "random.shuffle(negative_edges_snp_phenotype)\n",
    "\n",
    "# Split SNP-Phenotype positive edges\n",
    "positive_snp_phenotype_train_edges = positive_edges_snp_phenotype[:num_positive_snp_phenotype_train]\n",
    "positive_snp_phenotype_val_edges = positive_edges_snp_phenotype[num_positive_snp_phenotype_train:num_positive_snp_phenotype_train + num_positive_snp_phenotype_val]\n",
    "positive_snp_phenotype_test_edges = positive_edges_snp_phenotype[num_positive_snp_phenotype_train + num_positive_snp_phenotype_val:]\n",
    "\n",
    "# Split SNP-Phenotype negative edges\n",
    "negative_snp_phenotype_train_edges = negative_edges_snp_phenotype[:num_negative_snp_phenotype_train]\n",
    "negative_snp_phenotype_val_edges = negative_edges_snp_phenotype[num_negative_snp_phenotype_train:num_negative_snp_phenotype_train + num_negative_snp_phenotype_val]\n",
    "negative_snp_phenotype_test_edges = negative_edges_snp_phenotype[num_negative_snp_phenotype_train + num_negative_snp_phenotype_val:]\n",
    "\n",
    "# Convert edges back to tensors\n",
    "positive_train_edges = torch.tensor(positive_snp_phenotype_train_edges, dtype=torch.long).t().contiguous()\n",
    "positive_val_edges = torch.tensor(positive_snp_phenotype_val_edges, dtype=torch.long).t().contiguous()\n",
    "positive_test_edges = torch.tensor(positive_snp_phenotype_test_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "negative_train_edges = torch.tensor(negative_snp_phenotype_train_edges, dtype=torch.long).t().contiguous()\n",
    "negative_val_edges = torch.tensor(negative_snp_phenotype_val_edges, dtype=torch.long).t().contiguous()\n",
    "negative_test_edges = torch.tensor(negative_snp_phenotype_test_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create train, validation, and test graphs\n",
    "graph_train = Data(x=features, edge_index=torch.cat([positive_train_edges, negative_train_edges], dim=1), edge_attr=edge_attr)\n",
    "graph_val = Data(x=features, edge_index=torch.cat([positive_val_edges, negative_val_edges], dim=1), edge_attr=edge_attr)\n",
    "graph_test = Data(x=features, edge_index=torch.cat([positive_test_edges, negative_test_edges], dim=1), edge_attr=edge_attr)\n",
    "\n",
    "# Set node types for train, validation, and test graphs\n",
    "graph_train.node_types = node_types\n",
    "graph_val.node_types = node_types\n",
    "graph_test.node_types = node_types\n",
    "\n",
    "# Print the graphs\n",
    "print(\"Graph Train:\")\n",
    "print(graph_train)\n",
    "print(\"\\nGraph Validation:\")\n",
    "print(graph_val)\n",
    "print(\"\\nGraph Test:\")\n",
    "print(graph_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c4b91-e7cc-4e64-9e19-492e6e939e96",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65dfb1-c71a-432a-a385-8d25094b76e1",
   "metadata": {},
   "source": [
    "### Define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06d86bc-49dd-41a3-a19e-6439c8b26c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def compute_recall(preds, true_labels):\n",
    "    # Count the number of positive labels\n",
    "    num_pos = np.sum(true_labels == 1)\n",
    "    # Rank the predictions\n",
    "    sorted_preds_idx = np.argsort(preds)[::-1]\n",
    "    # Consider the top-k predictions to be positive\n",
    "    pos_preds_binary = np.zeros_like(preds)\n",
    "    pos_preds_binary[sorted_preds_idx[:num_pos]] = 1\n",
    "    # Calculate the number of true positives and false negatives\n",
    "    true_positives = np.sum((pos_preds_binary == 1) & (true_labels == 1))\n",
    "    false_negatives = np.sum((pos_preds_binary == 0) & (true_labels == 1))\n",
    "    # Calculate recall\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall\n",
    "\n",
    "def compute_precision(preds, true_labels):\n",
    "    # Count the number of positive labels\n",
    "    num_pos = np.sum(true_labels == 1)\n",
    "    # Rank the predictions\n",
    "    sorted_preds_idx = np.argsort(preds)[::-1]\n",
    "    # Consider the top-k predictions to be positive\n",
    "    pos_preds_binary = np.zeros_like(preds)\n",
    "    pos_preds_binary[sorted_preds_idx[:num_pos]] = 1\n",
    "    # Calculate the number of true positives and false positives\n",
    "    true_positives = np.sum((pos_preds_binary == 1) & (true_labels == 1))\n",
    "    false_positives = np.sum((pos_preds_binary == 1) & (true_labels == 0))\n",
    "    # Calculate precision\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "def compute_mrr(preds, true_labels):\n",
    "    # Find the predicted scores for positive examples\n",
    "    pos_preds = preds[:len(true_labels)]\n",
    "    # Rank the positive examples by predicted score in descending order\n",
    "    sorted_idx = np.argsort(pos_preds)[::-1]\n",
    "    # Find the rank of the first true positive\n",
    "    for i, idx in enumerate(sorted_idx):\n",
    "        if true_labels[idx] == 1:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "def compute_hits_at_k(preds, true_labels, k=1):\n",
    "    # Find the predicted scores for positive examples\n",
    "    pos_preds = preds[:len(true_labels)]\n",
    "    # Rank the positive examples by predicted score in descending order\n",
    "    sorted_idx = np.argsort(pos_preds)[::-1]\n",
    "    # Check if the first k predictions contain at least one true positive\n",
    "    hits = 0\n",
    "    for idx in sorted_idx[:k]:\n",
    "        if true_labels[idx] == 1:\n",
    "            hits = 1\n",
    "            break\n",
    "    return hits\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3761f7f-9f6c-4e53-ad2c-d45d877ef8b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a2972fd-305a-4ccf-b939-418eb88971fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.7289, Val ROC-AUC: 0.4970584651, Val MRR: 0.0000272546, Val hits@1: 0, Val Recall: 0.9941167699, Val Precision: 0.9941167699, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 2, Loss: 0.6949, Val ROC-AUC: 0.4999010669, Val MRR: 0.0008097166, Val hits@1: 0, Val Recall: 0.9998019735, Val Precision: 0.9998019735, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 3, Loss: 0.6932, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 4, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 5, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 6, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 7, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 8, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 9, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 10, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 11, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 12, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 13, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 14, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 15, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 16, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 17, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 18, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 19, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 20, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 21, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 22, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 23, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 24, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n",
      "Epoch: 25, Loss: 0.6931, Val ROC-AUC: 0.5000000000, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9999996793, Val Precision: 0.9999996793, Val Pos Accuracy: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "# Define the Logistic Regression model\n",
    "class LogReg(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# Train and evaluate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LogReg(input_dim=9).to(device)  # 9-dimensional edge feature vectors\n",
    "\n",
    "# I'm assuming you have these datasets ready\n",
    "graph_train = graph_train.to(device)\n",
    "graph_val = graph_val.to(device)\n",
    "graph_test = graph_test.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(graph_train.x.float())\n",
    "\n",
    "    pos_edge_index = graph_train.edge_index\n",
    "    neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=z.size(0), num_neg_samples=pos_edge_index.size(1))\n",
    "\n",
    "    pos_logits = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=-1)\n",
    "    neg_logits = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    logits = torch.cat([pos_logits, neg_logits], dim=0)\n",
    "    targets = torch.tensor([1] * pos_edge_index.size(1) + [0] * neg_edge_index.size(1), dtype=torch.float32).to(device)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function without model\n",
    "def evaluate(edge_index, graph):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(graph.x.float())\n",
    "        pos = torch.sigmoid((z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "        neg_edge_index = negative_sampling(edge_index, num_nodes=graph.num_nodes, num_neg_samples=edge_index.size(1))\n",
    "        neg = torch.sigmoid((z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "\n",
    "        preds = np.concatenate([pos.cpu().numpy(), neg.cpu().numpy()])\n",
    "        true_labels = np.concatenate([np.ones_like(pos.cpu().numpy()), np.zeros_like(neg.cpu().numpy())])\n",
    "\n",
    "        # Compute Accuracy for positive edges\n",
    "        pos_preds = preds[:len(pos)]  # positive predictions\n",
    "        pos_labels = true_labels[:len(pos)]  # actual positive labels\n",
    "        pos_accuracy = np.mean((pos_preds > 0.9) == pos_labels)\n",
    "\n",
    "        # Other metrics\n",
    "        roc_auc = roc_auc_score(true_labels, preds)\n",
    "        mrr = compute_mrr(preds, true_labels)\n",
    "        hits_at_5 = compute_hits_at_k(preds, true_labels, k=5)\n",
    "        recall = compute_recall(preds, true_labels)\n",
    "        precision = compute_precision(preds, true_labels)\n",
    "\n",
    "        return roc_auc, mrr, hits_at_5, recall, precision, pos_accuracy\n",
    "\n",
    "max_val_roc_auc = -np.inf\n",
    "max_val_mrr = -np.inf\n",
    "max_val_hits1 = -np.inf\n",
    "max_val_recall = -np.inf\n",
    "max_val_precision = -np.inf\n",
    "max_val_pos_accuracy = -np.inf\n",
    "\n",
    "max_test_roc_auc = -np.inf\n",
    "max_test_mrr = -np.inf\n",
    "max_test_hits1 = -np.inf\n",
    "max_test_recall = -np.inf\n",
    "max_test_precision = -np.inf\n",
    "max_test_pos_accuracy = -np.inf\n",
    "\n",
    "# Assuming the evaluate function is properly defined somewhere else\n",
    "for epoch in range(25):\n",
    "    loss = train()\n",
    "    val_roc_auc, val_mrr, val_hits_at_5, val_recall, val_precision, val_pos_accuracy = evaluate(graph_val.edge_index, graph_val)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss:.4f}, Val ROC-AUC: {val_roc_auc:.10f}, Val MRR: {val_mrr:.10f}, Val hits@1: {val_hits_at_5}, Val Recall: {val_recall:.10f}, Val Precision: {val_precision:.10f}, Val Pos Accuracy: {val_pos_accuracy:.10f}\")\n",
    "    max_val_roc_auc = max(max_val_roc_auc, val_roc_auc)\n",
    "    max_val_mrr = max(max_val_mrr, val_mrr)\n",
    "    max_val_hits1 = max(max_val_hits1, val_hits_at_5)\n",
    "    max_val_recall = max(max_val_recall, val_recall)\n",
    "    max_val_precision = max(max_val_precision, val_precision)\n",
    "    max_val_pos_accuracy = max(max_val_pos_accuracy, val_pos_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7ffa845-3f01-4177-967a-69d58e303e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Validation ROC-AUC: 0.5000000000\n",
      "Maximum Validation MRR: 0.5000000000\n",
      "Maximum Validation hits@1: 1.0000000000\n",
      "Maximum Validation Recall: 0.9999996793\n",
      "Maximum Validation Precision: 0.9999996793\n",
      "Maximum Validation Positive Edge Accuracy: 0.0000000000\n",
      "Maximum Test ROC-AUC: 0.5000000000\n",
      "Maximum Test MRR: 0.5000000000\n",
      "Maximum Test hits@1: 1.0000000000\n",
      "Maximum Test Recall: 0.9999997862\n",
      "Maximum Test Precision: 0.9999997862\n",
      "Maximum Test Positive Edge Accuracy: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "# For each epoch\n",
    "val_roc_auc, val_mrr, val_hits1, val_recall, val_precision, val_pos_accuracy = evaluate(graph_val.edge_index, graph_val)\n",
    "test_roc_auc, test_mrr, test_hits1, test_recall, test_precision, test_pos_accuracy = evaluate(graph_test.edge_index, graph_test)\n",
    "\n",
    "max_val_roc_auc = max(max_val_roc_auc, val_roc_auc)\n",
    "max_val_mrr = max(max_val_mrr, val_mrr)\n",
    "max_val_hits1 = max(max_val_hits1, val_hits1)\n",
    "max_val_recall = max(max_val_recall, val_recall)\n",
    "max_val_precision = max(max_val_precision, val_precision)\n",
    "max_val_pos_accuracy = max(max_val_pos_accuracy, val_pos_accuracy) # Add this line\n",
    "\n",
    "max_test_roc_auc = max(max_test_roc_auc, test_roc_auc)\n",
    "max_test_mrr = max(max_test_mrr, test_mrr)\n",
    "max_test_hits1 = max(max_test_hits1, test_hits1)\n",
    "max_test_recall = max(max_test_recall, test_recall)\n",
    "max_test_precision = max(max_test_precision, test_precision)\n",
    "max_test_pos_accuracy = max(max_test_pos_accuracy, test_pos_accuracy) # Add this line\n",
    "\n",
    "# Print the maximum scores for each metric\n",
    "print(f\"Maximum Validation ROC-AUC: {max_val_roc_auc:.10f}\")\n",
    "print(f\"Maximum Validation MRR: {max_val_mrr:.10f}\")\n",
    "print(f\"Maximum Validation hits@1: {max_val_hits1:.10f}\")\n",
    "print(f\"Maximum Validation Recall: {max_val_recall:.10f}\")\n",
    "print(f\"Maximum Validation Precision: {max_val_precision:.10f}\")\n",
    "print(f\"Maximum Validation Positive Edge Accuracy: {max_val_pos_accuracy:.10f}\") # Add this line\n",
    "\n",
    "print(f\"Maximum Test ROC-AUC: {max_test_roc_auc:.10f}\")\n",
    "print(f\"Maximum Test MRR: {max_test_mrr:.10f}\")\n",
    "print(f\"Maximum Test hits@1: {max_test_hits1:.10f}\")\n",
    "print(f\"Maximum Test Recall: {max_test_recall:.10f}\")\n",
    "print(f\"Maximum Test Precision: {max_test_precision:.10f}\")\n",
    "print(f\"Maximum Test Positive Edge Accuracy: {max_test_pos_accuracy:.10f}\") # Add this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10e208-06ab-4242-80cc-fa0004b49ef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbf66b-a6a4-407b-b4f9-7dd93eb24e49",
   "metadata": {},
   "source": [
    "- The GCN (Graph Convolutional Network) model used in this script is a simple 2-layer GCN. It transforms the original 9-dimensional node feature vectors into 2-dimensional hidden representations, using the adjacency matrix (encoded by `edge_index`) to propagate information across the graph.\n",
    "- The model is trained using Focal Loss, which is designed to address class imbalance problems. The training function computes the Focal Loss between the model's predictions on positive and negative edge examples, and the true edge labels. Negative edges are generated using a negative sampling method.\n",
    "- During evaluation, the model's embeddings are used to predict whether an edge exists between each pair of nodes, and these predictions are compared to the actual edges in the validation or test graph. Several evaluation metrics are computed, including ROC AUC, Mean Reciprocal Rank (MRR), hits@1, Recall, and Precision.\n",
    "- The training process iterates for 100 epochs. In each epoch, the model parameters are updated to minimize the Focal Loss on the training data, and the model's performance is evaluated on the validation data. The best validation scores on the ROC AUC, MRR, hits@1, Recall, and Precision metrics are tracked throughout training.\n",
    "- After training, the model can be used to predict whether causal edges exist between nodes in a graph. This makes it suitable for tasks like link prediction in biological networks, where the nodes represent entities like genes or phenotypes and the edges represent relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1744860e-e107-4495-9e88-79fe72ac7db7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 6850164736.0000, Val ROC-AUC: 0.6060425682, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9785440324, Val Precision: 0.9785440324, Val Pos Accuracy: 0.9785441928\n",
      "Epoch: 2, Loss: 6051239424.0000, Val ROC-AUC: 0.6049639748, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9781471778, Val Precision: 0.9781471778, Val Pos Accuracy: 0.9781473381\n",
      "Epoch: 3, Loss: 5332168704.0000, Val ROC-AUC: 0.6064335187, Val MRR: 1.0000000000, Val hits@1: 1, Val Recall: 0.9775773105, Val Precision: 0.9775773105, Val Pos Accuracy: 0.9775773105\n",
      "Epoch: 4, Loss: 4673915904.0000, Val ROC-AUC: 0.6079260365, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9768570393, Val Precision: 0.9768570393, Val Pos Accuracy: 0.9768571997\n",
      "Epoch: 5, Loss: 4074292480.0000, Val ROC-AUC: 0.6101341509, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9785231875, Val Precision: 0.9785231875, Val Pos Accuracy: 0.9785231875\n",
      "Epoch: 6, Loss: 3546605568.0000, Val ROC-AUC: 0.6123064964, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9781398019, Val Precision: 0.9781398019, Val Pos Accuracy: 0.9781399622\n",
      "Epoch: 7, Loss: 3084379136.0000, Val ROC-AUC: 0.6144816260, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9779568479, Val Precision: 0.9779568479, Val Pos Accuracy: 0.9779570082\n",
      "Epoch: 8, Loss: 2659532032.0000, Val ROC-AUC: 0.6165321430, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9779481892, Val Precision: 0.9779481892, Val Pos Accuracy: 0.9779483496\n",
      "Epoch: 9, Loss: 2291409408.0000, Val ROC-AUC: 0.6199505814, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9773645323, Val Precision: 0.9773645323, Val Pos Accuracy: 0.9773646926\n",
      "Epoch: 10, Loss: 1966498432.0000, Val ROC-AUC: 0.6221222617, Val MRR: 1.0000000000, Val hits@1: 1, Val Recall: 0.9759608693, Val Precision: 0.9759608693, Val Pos Accuracy: 0.9759608693\n",
      "Epoch: 11, Loss: 1678977280.0000, Val ROC-AUC: 0.6248703011, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9745960100, Val Precision: 0.9745960100, Val Pos Accuracy: 0.9745961703\n",
      "Epoch: 12, Loss: 1429417856.0000, Val ROC-AUC: 0.6267469565, Val MRR: 1.0000000000, Val hits@1: 1, Val Recall: 0.9734633306, Val Precision: 0.9734633306, Val Pos Accuracy: 0.9734633306\n",
      "Epoch: 13, Loss: 1210701952.0000, Val ROC-AUC: 0.6287497647, Val MRR: 1.0000000000, Val hits@1: 1, Val Recall: 0.9725472778, Val Precision: 0.9725472778, Val Pos Accuracy: 0.9725472778\n",
      "Epoch: 14, Loss: 1026968576.0000, Val ROC-AUC: 0.6308613148, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9716552768, Val Precision: 0.9716552768, Val Pos Accuracy: 0.9716552768\n",
      "Epoch: 15, Loss: 865446592.0000, Val ROC-AUC: 0.6328854440, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9706592117, Val Precision: 0.9706592117, Val Pos Accuracy: 0.9706593720\n",
      "Epoch: 16, Loss: 728043008.0000, Val ROC-AUC: 0.6345201602, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9736530191, Val Precision: 0.9736530191, Val Pos Accuracy: 0.9736531795\n",
      "Epoch: 17, Loss: 607582400.0000, Val ROC-AUC: 0.6381206698, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9771697127, Val Precision: 0.9771697127, Val Pos Accuracy: 0.9771698730\n",
      "Epoch: 18, Loss: 506180512.0000, Val ROC-AUC: 0.6404727107, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9770630831, Val Precision: 0.9770630831, Val Pos Accuracy: 0.9770632434\n",
      "Epoch: 19, Loss: 419359840.0000, Val ROC-AUC: 0.6428787167, Val MRR: 1.0000000000, Val hits@1: 1, Val Recall: 0.9769121981, Val Precision: 0.9769121981, Val Pos Accuracy: 0.9769121981\n",
      "Epoch: 20, Loss: 345526272.0000, Val ROC-AUC: 0.6443496662, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9765730678, Val Precision: 0.9765730678, Val Pos Accuracy: 0.9765730678\n",
      "Epoch: 21, Loss: 281792384.0000, Val ROC-AUC: 0.6462274728, Val MRR: 1.0000000000, Val hits@1: 1, Val Recall: 0.9764794261, Val Precision: 0.9764794261, Val Pos Accuracy: 0.9764794261\n",
      "Epoch: 22, Loss: 227909392.0000, Val ROC-AUC: 0.6476016964, Val MRR: 1.0000000000, Val hits@1: 1, Val Recall: 0.9764654760, Val Precision: 0.9764654760, Val Pos Accuracy: 0.9764654760\n",
      "Epoch: 23, Loss: 181744256.0000, Val ROC-AUC: 0.6495012183, Val MRR: 1.0000000000, Val hits@1: 1, Val Recall: 0.9764228242, Val Precision: 0.9764228242, Val Pos Accuracy: 0.9764228242\n",
      "Epoch: 24, Loss: 143068224.0000, Val ROC-AUC: 0.6514667960, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9763646188, Val Precision: 0.9763646188, Val Pos Accuracy: 0.9763647792\n",
      "Epoch: 25, Loss: 109204816.0000, Val ROC-AUC: 0.6528109502, Val MRR: 0.5000000000, Val hits@1: 1, Val Recall: 0.9752506999, Val Precision: 0.9752506999, Val Pos Accuracy: 0.9752506999\n"
     ]
    }
   ],
   "source": [
    "# Task: Link prediction: does a causal edge exist between two nodes?\n",
    "# Node Types: 0 = phenotypes, 1 = snps\n",
    "# Node Feature Vector: 10-dimensional\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(9, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Train and evaluate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(hidden_channels=2).to(device)\n",
    "\n",
    "graph_train = graph_train.to(device)\n",
    "graph_val = graph_val.to(device)\n",
    "graph_test = graph_test.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Instantiate the loss function\n",
    "focal_loss = FocalLoss(alpha=0.9, gamma=2.0).to(device)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(graph_train.x.float(), graph_train.edge_index)\n",
    "\n",
    "    # Only consider positive edges for the positive score calculation\n",
    "    pos_edge_index = graph_train.edge_index\n",
    "    pos = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    # Use negative_sampling to generate negative edges\n",
    "    neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=z.size(0), num_neg_samples=pos_edge_index.size(1))\n",
    "    neg = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    logits = torch.cat([pos, neg], dim=0)\n",
    "    targets = torch.tensor([1] * pos.size(0) + [0] * neg.size(0), dtype=torch.float32).to(device)\n",
    "\n",
    "    loss = focal_loss(logits, targets) # replace BCE with focal loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(edge_index, graph):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(graph.x.float(), graph.edge_index)\n",
    "        pos = torch.sigmoid((z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "        neg_edge_index = negative_sampling(edge_index, num_nodes=graph.num_nodes, num_neg_samples=edge_index.size(1))\n",
    "        neg = torch.sigmoid((z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "\n",
    "        preds = np.concatenate([pos.cpu().numpy(), neg.cpu().numpy()])\n",
    "        true_labels = np.concatenate([np.ones_like(pos.cpu().numpy()), np.zeros_like(neg.cpu().numpy())])\n",
    "\n",
    "        # Compute Accuracy for positive edges\n",
    "        pos_preds = preds[:len(pos)]  # positive predictions\n",
    "        pos_labels = true_labels[:len(pos)]  # actual positive labels\n",
    "        pos_accuracy = np.mean((pos_preds > 0.9) == pos_labels)\n",
    "\n",
    "        # Other metrics\n",
    "        roc_auc = roc_auc_score(true_labels, preds)\n",
    "        mrr = compute_mrr(preds, true_labels)\n",
    "        hits_at_5 = compute_hits_at_k(preds, true_labels, k=5)\n",
    "        recall = compute_recall(preds, true_labels)\n",
    "        precision = compute_precision(preds, true_labels)\n",
    "\n",
    "        return roc_auc, mrr, hits_at_5, recall, precision, pos_accuracy\n",
    "\n",
    "\n",
    "max_val_roc_auc = -np.inf\n",
    "max_val_mrr = -np.inf\n",
    "max_val_hits1 = -np.inf\n",
    "max_val_recall = -np.inf\n",
    "max_val_precision = -np.inf\n",
    "max_val_pos_accuracy = -np.inf\n",
    "\n",
    "max_test_roc_auc = -np.inf\n",
    "max_test_mrr = -np.inf\n",
    "max_test_hits1 = -np.inf\n",
    "max_test_recall = -np.inf\n",
    "max_test_precision = -np.inf\n",
    "max_test_pos_accuracy = -np.inf\n",
    "\n",
    "for epoch in range(25):\n",
    "    loss = train()\n",
    "    val_roc_auc, val_mrr, val_hits_at_5, val_recall, val_precision, val_pos_accuracy = evaluate(graph_val.edge_index, graph_val)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss:.4f}, Val ROC-AUC: {val_roc_auc:.10f}, Val MRR: {val_mrr:.10f}, Val hits@1: {val_hits_at_5}, Val Recall: {val_recall:.10f}, Val Precision: {val_precision:.10f}, Val Pos Accuracy: {val_pos_accuracy:.10f}\")\n",
    "    max_val_roc_auc = max(max_val_roc_auc, val_roc_auc)\n",
    "    max_val_mrr = max(max_val_mrr, val_mrr)\n",
    "    max_val_hits1 = max(max_val_hits1, val_hits_at_5)\n",
    "    max_val_recall = max(max_val_recall, val_recall)\n",
    "    max_val_precision = max(max_val_precision, val_precision)\n",
    "    max_val_pos_accuracy = max(max_val_pos_accuracy, val_pos_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55930699-10ed-4220-8144-139098ecf0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Validation ROC-AUC: 0.6529438637\n",
      "Maximum Validation MRR: 1.0000000000\n",
      "Maximum Validation hits@1: 1.0000000000\n",
      "Maximum Validation Recall: 0.9785440324\n",
      "Maximum Validation Precision: 0.9785440324\n",
      "Maximum Validation Positive Edge Accuracy: 0.9785441928\n",
      "Maximum Test ROC-AUC: 0.6529052981\n",
      "Maximum Test MRR: 1.0000000000\n",
      "Maximum Test hits@1: 1.0000000000\n",
      "Maximum Test Recall: 0.9752578620\n",
      "Maximum Test Precision: 0.9752578620\n",
      "Maximum Test Positive Edge Accuracy: 0.9752579689\n"
     ]
    }
   ],
   "source": [
    "# For each epoch\n",
    "val_roc_auc, val_mrr, val_hits1, val_recall, val_precision, val_pos_accuracy = evaluate(graph_val.edge_index, graph_val)\n",
    "test_roc_auc, test_mrr, test_hits1, test_recall, test_precision, test_pos_accuracy = evaluate(graph_test.edge_index, graph_test)\n",
    "\n",
    "max_val_roc_auc = max(max_val_roc_auc, val_roc_auc)\n",
    "max_val_mrr = max(max_val_mrr, val_mrr)\n",
    "max_val_hits1 = max(max_val_hits1, val_hits1)\n",
    "max_val_recall = max(max_val_recall, val_recall)\n",
    "max_val_precision = max(max_val_precision, val_precision)\n",
    "max_val_pos_accuracy = max(max_val_pos_accuracy, val_pos_accuracy) # Add this line\n",
    "\n",
    "max_test_roc_auc = max(max_test_roc_auc, test_roc_auc)\n",
    "max_test_mrr = max(max_test_mrr, test_mrr)\n",
    "max_test_hits1 = max(max_test_hits1, test_hits1)\n",
    "max_test_recall = max(max_test_recall, test_recall)\n",
    "max_test_precision = max(max_test_precision, test_precision)\n",
    "max_test_pos_accuracy = max(max_test_pos_accuracy, test_pos_accuracy) # Add this line\n",
    "\n",
    "# Print the maximum scores for each metric\n",
    "print(f\"Maximum Validation ROC-AUC: {max_val_roc_auc:.10f}\")\n",
    "print(f\"Maximum Validation MRR: {max_val_mrr:.10f}\")\n",
    "print(f\"Maximum Validation hits@1: {max_val_hits1:.10f}\")\n",
    "print(f\"Maximum Validation Recall: {max_val_recall:.10f}\")\n",
    "print(f\"Maximum Validation Precision: {max_val_precision:.10f}\")\n",
    "print(f\"Maximum Validation Positive Edge Accuracy: {max_val_pos_accuracy:.10f}\") # Add this line\n",
    "\n",
    "print(f\"Maximum Test ROC-AUC: {max_test_roc_auc:.10f}\")\n",
    "print(f\"Maximum Test MRR: {max_test_mrr:.10f}\")\n",
    "print(f\"Maximum Test hits@1: {max_test_hits1:.10f}\")\n",
    "print(f\"Maximum Test Recall: {max_test_recall:.10f}\")\n",
    "print(f\"Maximum Test Precision: {max_test_precision:.10f}\")\n",
    "print(f\"Maximum Test Positive Edge Accuracy: {max_test_pos_accuracy:.10f}\") # Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11e0560b-c904-40ba-8bfb-03746b949ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 26 parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model)} parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
