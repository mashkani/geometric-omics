{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Grapher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd8be8-14cc-4d80-a944-76c5235c078b",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bc773-edf8-4567-8767-a742662cb8ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [FinnGen](https://finngen.gitbook.io/documentation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef1038-e49a-4607-9c7d-5ce6daf1d94b",
   "metadata": {},
   "source": [
    "- The FinnGen research project is an expedition to the frontier of genomics and medicine, with significant discoveries potentially arising from any one of Finland’s 500,000 biomedical pioneers.\n",
    "- The project brings together a nation-wide network of Finnish biobanks, with every Finn able to participate in the study by giving biobank consent.\n",
    "- As of the last update, there were 589,000 samples available, with a goal to reach 520,000 by 2023. The latest data freeze included combined genotype and health registry data from 473,681 individuals.\n",
    "- The study utilizes samples collected by a nationwide network of Finnish biobanks and combines genome information with digital health care data from national health registries【8†source】.\n",
    "- There's a need for samples from all over Finland as solutions in the field of personalized healthcare can be found only by looking at large populations. Every Finn can be a part of the FinnGen study by giving a biobank consent.\n",
    "- The genome data produced during the project is owned by the Finnish biobanks and remains available for research purposes. The medical breakthroughs that arise from the project are expected to benefit health care systems and patients globally.\n",
    "- The FinnGen research project is collaborative, involving all the same actors as drug development, with the aim to speed up the emergence of new innovations.\n",
    "- The project's data freeze 9 results and summary statistics are now available, consisting of over 377,200 individuals, almost 20.2 M variants, and 2,272 disease endpoints. Results can be browsed online using the FinnGen web browser, and the summary statistics downloaded.\n",
    "- The University of Helsinki is the organization responsible for the study, and the nationwide network of Finnish biobanks is participating in the study, thus covering the whole of Finland. The Helsinki Biobank coordinates the sample collection.\n",
    "- For more information, the project can be contacted at finngen-info@helsinki.fi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1805a-55dc-409c-9ccf-512eb1f88667",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8ec1d-73be-4b8c-95dc-79489e84998b",
   "metadata": {},
   "source": [
    "Here's the summary documentation for the DataFrame in bullet format:\n",
    "\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located.\n",
    "\n",
    "- `pos`: This is the position of the genetic variant on the chromosome.\n",
    "\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant.\n",
    "\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population.\n",
    "\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group.\n",
    "\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group.\n",
    "\n",
    "- `causal`: This binary column indicates whether the variant is determined to be causal (1) or not (0).\n",
    "\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "\n",
    "import networkx as nx\n",
    "from ogb.io import DatasetSaver\n",
    "from ogb.linkproppred import LinkPropPredDataset\n",
    "\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f7ab7-4bf0-4dba-9428-9580338d9aa7",
   "metadata": {},
   "source": [
    "## Perform checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b729b64d-2166-4dbb-9c4c-4173e0c9e807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0+cu118\n",
      "PyTorch Geometric version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f213c8b-5dc0-468b-b7d0-72c08a6e5282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce RTX 3060 Ti (cuda)\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data and create new rows for each gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Desktop/geometric-omics/FinnGenn/data/gwas-causal.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assuming your DataFrame is called data and the relevant column is 'nearest_genes'\n",
    "# First, let's split the gene names in the 'nearest_genes' column\n",
    "split_genes = data['nearest_genes'].str.split(',')\n",
    "\n",
    "# Flatten the list of split gene names\n",
    "flat_genes = [item for sublist in split_genes for item in sublist]\n",
    "\n",
    "# Then, we create a new DataFrame by repeating rows and substituting the gene names\n",
    "data_new = (data.loc[data.index.repeat(split_genes.str.len())]\n",
    "            .assign(nearest_genes=flat_genes))\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513cafa0-f3f8-47b5-9d22-96062449edde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>rsids</th>\n",
       "      <th>nearest_genes</th>\n",
       "      <th>pval</th>\n",
       "      <th>mlogp</th>\n",
       "      <th>beta</th>\n",
       "      <th>sebeta</th>\n",
       "      <th>af_alt</th>\n",
       "      <th>af_alt_cases</th>\n",
       "      <th>af_alt_controls</th>\n",
       "      <th>causal</th>\n",
       "      <th>LD</th>\n",
       "      <th>lead</th>\n",
       "      <th>trait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13668</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>rs2691328</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.944365</td>\n",
       "      <td>0.024860</td>\n",
       "      <td>-0.005926</td>\n",
       "      <td>0.084918</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14773</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>rs878915777</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.844305</td>\n",
       "      <td>0.073501</td>\n",
       "      <td>0.010088</td>\n",
       "      <td>0.051369</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.013547</td>\n",
       "      <td>0.013485</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15585</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>rs533630043</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.841908</td>\n",
       "      <td>0.074735</td>\n",
       "      <td>0.031464</td>\n",
       "      <td>0.157751</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>16549</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>rs1262014613</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.343308</td>\n",
       "      <td>0.464316</td>\n",
       "      <td>0.241377</td>\n",
       "      <td>0.254711</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>16567</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>rs1194064194</td>\n",
       "      <td>OR4F5</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0.886447</td>\n",
       "      <td>0.130736</td>\n",
       "      <td>0.086319</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20565622</th>\n",
       "      <td>23</td>\n",
       "      <td>155697920</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>1.566790</td>\n",
       "      <td>-0.013475</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>0.290961</td>\n",
       "      <td>0.286054</td>\n",
       "      <td>0.291879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20565623</th>\n",
       "      <td>23</td>\n",
       "      <td>155698443</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.178417</td>\n",
       "      <td>0.748564</td>\n",
       "      <td>-0.069907</td>\n",
       "      <td>0.051951</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20565624</th>\n",
       "      <td>23</td>\n",
       "      <td>155698490</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.279640</td>\n",
       "      <td>0.553400</td>\n",
       "      <td>-0.020245</td>\n",
       "      <td>0.018725</td>\n",
       "      <td>0.024406</td>\n",
       "      <td>0.024312</td>\n",
       "      <td>0.024423</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20565625</th>\n",
       "      <td>23</td>\n",
       "      <td>155699751</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.078864</td>\n",
       "      <td>1.103120</td>\n",
       "      <td>-0.011284</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.244829</td>\n",
       "      <td>0.241257</td>\n",
       "      <td>0.245498</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20565626</th>\n",
       "      <td>23</td>\n",
       "      <td>155700291</td>\n",
       "      <td>CAA</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.071590</td>\n",
       "      <td>1.145150</td>\n",
       "      <td>-0.011563</td>\n",
       "      <td>0.006418</td>\n",
       "      <td>0.245328</td>\n",
       "      <td>0.241724</td>\n",
       "      <td>0.246003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20565627 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          #chrom        pos  ref alt         rsids nearest_genes      pval   \n",
       "0              1      13668    G   A     rs2691328         OR4F5  0.944365  \\\n",
       "1              1      14773    C   T   rs878915777         OR4F5  0.844305   \n",
       "2              1      15585    G   A   rs533630043         OR4F5  0.841908   \n",
       "3              1      16549    T   C  rs1262014613         OR4F5  0.343308   \n",
       "4              1      16567    G   C  rs1194064194         OR4F5  0.129883   \n",
       "...          ...        ...  ...  ..           ...           ...       ...   \n",
       "20565622      23  155697920    G   A           NaN           nan  0.027115   \n",
       "20565623      23  155698443    C   A           NaN           nan  0.178417   \n",
       "20565624      23  155698490    C   T           NaN           nan  0.279640   \n",
       "20565625      23  155699751    C   T           NaN           nan  0.078864   \n",
       "20565626      23  155700291  CAA   C           NaN           nan  0.071590   \n",
       "\n",
       "             mlogp      beta    sebeta    af_alt  af_alt_cases   \n",
       "0         0.024860 -0.005926  0.084918  0.005842      0.005729  \\\n",
       "1         0.073501  0.010088  0.051369  0.013495      0.013547   \n",
       "2         0.074735  0.031464  0.157751  0.001113      0.001125   \n",
       "3         0.464316  0.241377  0.254711  0.000561      0.000620   \n",
       "4         0.886447  0.130736  0.086319  0.004170      0.004250   \n",
       "...            ...       ...       ...       ...           ...   \n",
       "20565622  1.566790 -0.013475  0.006097  0.290961      0.286054   \n",
       "20565623  0.748564 -0.069907  0.051951  0.003259      0.003022   \n",
       "20565624  0.553400 -0.020245  0.018725  0.024406      0.024312   \n",
       "20565625  1.103120 -0.011284  0.006421  0.244829      0.241257   \n",
       "20565626  1.145150 -0.011563  0.006418  0.245328      0.241724   \n",
       "\n",
       "          af_alt_controls  causal  LD lead trait  \n",
       "0                0.005863       0   0  NaN   T2D  \n",
       "1                0.013485       0   0  NaN   T2D  \n",
       "2                0.001110       0   0  NaN   T2D  \n",
       "3                0.000550       0   0  NaN   T2D  \n",
       "4                0.004154       0   0  NaN   T2D  \n",
       "...                   ...     ...  ..  ...   ...  \n",
       "20565622         0.291879       0   0  NaN   T2D  \n",
       "20565623         0.003304       0   0  NaN   T2D  \n",
       "20565624         0.024423       0   0  NaN   T2D  \n",
       "20565625         0.245498       0   0  NaN   T2D  \n",
       "20565626         0.246003       0   0  NaN   T2D  \n",
       "\n",
       "[20565627 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011a6c79-4026-4e07-a032-0042f21e9a21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20565627"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "**Task Overview**\n",
    "- The objective is to design and implement a multi-class link prediction model for analyzing relationships between SNP nodes and Phenotype nodes.\n",
    "\n",
    "**Nodes and Their Features**\n",
    "- There are two types of nodes: SNP Nodes and Phenotype Nodes.\n",
    "- *Phenotype Nodes*: Each Phenotype Node represents a particular trait. This information comes from the `trait` column in the data.\n",
    "- *SNP Nodes*: Each SNP Node is characterized by various features, including `rsids`, `nearest_genes`, `#chrom`, `pos`, `ref`, `alt`, `beta`, `sebeta`, `af_alt`, and `af_alt_cases` columns.\n",
    "\n",
    "**Edges, Their Features, and Labels**\n",
    "- Edges represent relationships between nodes. There are two types of edges: SNP-Phenotype and SNP-SNP.\n",
    "- *SNP-Phenotype Edges*:\n",
    "  - These edges are undirected, linking SNP Nodes and Phenotype Nodes.\n",
    "  - The label for each edge is determined by the `causal` column in the data:\n",
    "    - A label of +1 is assigned when `data['causal']` is 1, indicating a causal relationship.\n",
    "    - A label of -1 is assigned when `data['causal']` is 0, indicating the absence of a causal relationship.\n",
    "- *SNP-SNP Edges*:\n",
    "  - These edges are undirected, linking an SNP Node (as identified by the `rsids` column) to another SNP Node (as identified by the `lead` column) in the same data row.\n",
    "  - The label for each edge is determined by the `LD` column in the data:\n",
    "    - A label of +2 is assigned when `data['LD']` is 1, signifying that the two SNPs are in linkage disequilibrium.\n",
    "    - A label of -2 is assigned when `data['LD']` is 0, indicating that the two SNPs are not in linkage disequilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "849b5a45-34bc-441b-a20d-c78ff8a92d87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:37\u001b[0m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create mappings for phenotypes and SNPs to integer indices\n",
    "phenotypes = data['trait'].unique()\n",
    "snps = data['rsids'].unique()\n",
    "phenotype_to_idx = {phenotype: idx for idx, phenotype in enumerate(phenotypes)}\n",
    "snp_to_idx = {snp: idx + len(phenotypes) for idx, snp in enumerate(snps)}\n",
    "\n",
    "# Create node feature vectors for phenotypes and SNPs\n",
    "phenotype_features = data.loc[data['trait'].isin(phenotypes)][['trait']].drop_duplicates().sort_values(by='trait').reset_index(drop=True)\n",
    "snp_features = data.loc[data['rsids'].isin(snps)][['rsids', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'beta', 'sebeta', 'af_alt', 'af_alt_cases']].drop_duplicates().sort_values(by='rsids').reset_index(drop=True)\n",
    "\n",
    "# Create node type labels\n",
    "node_types = torch.tensor([0] * len(phenotypes) + [1] * len(snps), dtype=torch.long)\n",
    "\n",
    "# SNP-Phenotype edge creation based on 'causal' column\n",
    "edges = data[['rsids', 'trait', 'causal']].drop_duplicates()\n",
    "\n",
    "edges['snp_idx'] = edges['rsids'].map(snp_to_idx)\n",
    "edges['phenotype_idx'] = edges['trait'].map(phenotype_to_idx)\n",
    "\n",
    "# Create positive and negative edges for SNP-Phenotype\n",
    "positive_edges_snp_phenotype = edges.loc[edges['causal'] == 1, ['snp_idx', 'phenotype_idx']].values\n",
    "negative_edges_snp_phenotype = edges.loc[edges['causal'] == 0, ['snp_idx', 'phenotype_idx']].values\n",
    "\n",
    "positive_edges_snp_phenotype = torch.tensor(positive_edges_snp_phenotype, dtype=torch.long).t().contiguous()\n",
    "negative_edges_snp_phenotype = torch.tensor(negative_edges_snp_phenotype, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# SNP-SNP edge creation based on 'LD' column\n",
    "snp_snp_edges = data[['rsids', 'lead', 'LD']].dropna().drop_duplicates()\n",
    "\n",
    "snp_snp_edges['snp_idx'] = snp_snp_edges['rsids'].map(snp_to_idx)\n",
    "snp_snp_edges['lead_snp_idx'] = snp_snp_edges['lead'].map(snp_to_idx)\n",
    "\n",
    "# Create positive and negative edges for SNP-SNP\n",
    "positive_edges_snp_snp = torch.tensor(positive_edges_snp_snp, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create a set of all SNP-SNP edges (both positive and negative)\n",
    "all_snp_snp_edges = set(map(tuple, np.concatenate((positive_edges_snp_snp, negative_edges_snp_snp), axis=1)))\n",
    "\n",
    "# Create a set of all SNPs\n",
    "all_snps = set(range(len(phenotypes), len(phenotypes) + len(snps)))\n",
    "\n",
    "# Initialize a list for negative SNP-SNP edges\n",
    "negative_edges_snp_snp = []\n",
    "\n",
    "# Create 1000 negative SNP-SNP edges for every positive edge\n",
    "for _ in range(1000 * positive_edges_snp_snp.shape[1]):\n",
    "    while True:\n",
    "        # Randomly select two SNPs\n",
    "        snp1, snp2 = random.sample(all_snps, 2)\n",
    "\n",
    "        # If the pair does not exist in the set of all SNP-SNP edges, add it to the list of negative edges\n",
    "        if (snp1, snp2) not in all_snp_snp_edges and (snp2, snp1) not in all_snp_snp_edges:\n",
    "            negative_edges_snp_snp.append((snp1, snp2))\n",
    "            break\n",
    "\n",
    "negative_edges_snp_snp = torch.tensor(negative_edges_snp_snp, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Combine SNP-Phenotype and SNP-SNP edges\n",
    "edges = torch.cat([positive_edges_snp_phenotype, negative_edges_snp_phenotype, positive_edges_snp_snp, negative_edges_snp_snp], dim=1)\n",
    "\n",
    "# Create edge attributes\n",
    "edge_attr = torch.cat([torch.ones(positive_edges_snp_phenotype.size(1), dtype=torch.float),\n",
    "                       -1 * torch.ones(negative_edges_snp_phenotype.size(1), dtype=torch.float),\n",
    "                       2 * torch.ones(positive_edges_snp_snp.size(1), dtype=torch.float),\n",
    "                       -2 * torch.ones(negative_edges_snp_snp.size(1), dtype=torch.float)])\n",
    "\n",
    "# Combine the feature vectors\n",
    "combined_features = pd.concat([phenotype_features, snp_features], ignore_index=True).drop(['trait', 'rsids'], axis=1)\n",
    "\n",
    "# Now you can fill NaNs with 'N/A' or 0 for numerical columns\n",
    "nan_replacements = {'nearest_genes': 'N/A', '#chrom': 'N/A', 'pos': 0, 'ref': 'N/A', 'alt': 'N/A', 'beta': 0, 'sebeta': 0, 'af_alt': 0, 'af_alt_cases': 0}\n",
    "for col, replacement in nan_replacements.items():\n",
    "    if col in combined_features:\n",
    "        combined_features[col].fillna(replacement, inplace=True)\n",
    "\n",
    "# Label encoding for categorical columns\n",
    "le = LabelEncoder()\n",
    "categorical_columns = ['nearest_genes', '#chrom', 'ref', 'alt']\n",
    "for col in categorical_columns:\n",
    "    combined_features[col] = le.fit_transform(combined_features[col].astype(str))\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['pos', 'beta', 'sebeta', 'af_alt', 'af_alt_cases']\n",
    "for col in numerical_columns:\n",
    "    combined_features[col] = scaler.fit_transform(combined_features[[col]])\n",
    "\n",
    "features = torch.tensor(combined_features.values, dtype=torch.float)\n",
    "\n",
    "# Create the PyTorch Geometric graph\n",
    "graph = Data(x=features, edge_index=edges, edge_attr=edge_attr)\n",
    "graph.node_types = node_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cae52-2493-43ae-9e27-e7dc67c3a7f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0709aa36-0d8a-45de-b5c5-1b287fad15ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph stats:\n",
      "Number of nodes: 20565628\n",
      "Number of 0 nodes: 1\n",
      "Number of 1 nodes: 18709437\n",
      "Number of positive edges between SNPs and phenotypes: 37\n",
      "Number of negative edges between SNPs and phenotypes: 18709400\n",
      "Number of positive edges between SNPs: 3934\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Print\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph stats:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mprint_graph_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_edges_snp_phenotype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_edges_snp_phenotype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_edges_snp_snp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_edges_snp_snp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m, in \u001b[0;36mprint_graph_stats\u001b[1;34m(graph, positive_edges_snp_phenotype, negative_edges_snp_phenotype, positive_edges_snp_snp, negative_edges_snp_snp)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of negative edges between SNPs and phenotypes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnegative_edges_snp_phenotype\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of positive edges between SNPs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpositive_edges_snp_snp\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of negative edges between SNPs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnegative_edges_snp_snp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of edges: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph\u001b[38;5;241m.\u001b[39mnum_edges\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode feature dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph\u001b[38;5;241m.\u001b[39mnum_node_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "def print_graph_stats(graph, positive_edges_snp_phenotype, negative_edges_snp_phenotype, positive_edges_snp_snp, negative_edges_snp_snp):\n",
    "    node_types = np.unique(graph.node_types.numpy(), return_counts=True)\n",
    "\n",
    "    print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "    for node_type, count in zip(*node_types):\n",
    "        print(f\"Number of {node_type} nodes: {count}\")\n",
    "    print(f\"Number of positive edges between SNPs and phenotypes: {positive_edges_snp_phenotype.size(1)}\")\n",
    "    print(f\"Number of negative edges between SNPs and phenotypes: {negative_edges_snp_phenotype.size(1)}\")\n",
    "    print(f\"Number of positive edges between SNPs: {positive_edges_snp_snp.size(1)}\")\n",
    "    print(f\"Number of negative edges between SNPs: {negative_edges_snp_snp.size(1)}\")\n",
    "    print(f\"Number of edges: {graph.num_edges}\")\n",
    "    print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "    # Compute and print degree-related stats for each node type\n",
    "    for node_type in node_types[0]:\n",
    "        node_indices = np.where(graph.node_types.numpy() == node_type)[0]\n",
    "        degrees = degree(graph.edge_index[0], num_nodes=graph.num_nodes)[node_indices]\n",
    "        average_degree = degrees.float().mean().item()\n",
    "        median_degree = np.median(degrees.numpy())\n",
    "        std_degree = degrees.float().std().item()\n",
    "\n",
    "        print(f\"\\n{node_type} node stats:\")\n",
    "        print(f\"Average degree: {average_degree:.2f}\")\n",
    "        print(f\"Median degree: {median_degree:.2f}\")\n",
    "        print(f\"Standard deviation of degree: {std_degree:.2f}\")\n",
    "\n",
    "    # Density is the ratio of actual edges to the maximum number of possible edges\n",
    "    num_possible_edges = graph.num_nodes * (graph.num_nodes - 1) / 2\n",
    "    density = graph.num_edges / num_possible_edges\n",
    "    print(f\"Density: {density:.10f}\")\n",
    "\n",
    "    # Check for NaN values in features\n",
    "    nan_in_features = torch.isnan(graph.x).any().item()\n",
    "    print(f\"Are there any NaN values in features? {nan_in_features}\")\n",
    "\n",
    "# Print\n",
    "print(\"Graph stats:\")\n",
    "print_graph_stats(graph, positive_edges_snp_phenotype, negative_edges_snp_phenotype, positive_edges_snp_snp, negative_edges_snp_snp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51e96d-660c-4b4f-8ac4-9a07dba5bcc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9ecd3-36c5-45d2-8038-493e762b20dc",
   "metadata": {},
   "source": [
    "- **Importing Modules**\n",
    "\n",
    "  The script begins by importing the necessary Python libraries. It uses `random` for shuffling data, `torch` for handling tensors, and `torch_geometric.data` for its `Data` class, which is used to represent graph data.\n",
    "\n",
    "- **Constants**\n",
    "\n",
    "  The ratios for splitting the data into training, validation, and testing sets are defined. Both positive and negative edges are split equally into three parts.\n",
    "\n",
    "- **Calculating Sample Size for Each Split**\n",
    "\n",
    "  The number of samples for each set (training, validation, testing) are calculated separately for SNP-Gene and Gene-Trait edges. This is done for both positive and negative edges.\n",
    "\n",
    "- **Shuffling the Edges**\n",
    "\n",
    "  The edges for both positive and negative SNP-Gene and Gene-Trait data are shuffled. This ensures that the training, validation, and testing sets get a fair representation of the entire dataset.\n",
    "\n",
    "- **Splitting the Edges**\n",
    "\n",
    "  Both SNP-Gene and Gene-Trait edges are split according to the previously calculated sample sizes. The split is performed separately for both positive and negative edges.\n",
    "\n",
    "- **Combining SNP-Gene and Gene-Trait Edges**\n",
    "\n",
    "  After the split, the SNP-Gene and Gene-Trait edges are combined back together to form the final sets of edges for the training, validation, and testing sets.\n",
    "\n",
    "- **Converting Edges back to Tensors**\n",
    "\n",
    "  The lists of edges are then converted back into torch tensors. This conversion prepares the data for future operations with PyTorch's machine learning functionalities.\n",
    "\n",
    "- **Creating Graphs**\n",
    "\n",
    "  Graphs for training, validation, and testing sets are created. These graphs are instances of the `Data` class from the `torch_geometric.data` module. The graphs contain node features, edge indices, and edge attributes.\n",
    "\n",
    "- **Setting Node Types**\n",
    "\n",
    "  The node types for each graph are set. The node type information can be used for tasks such as node classification.\n",
    "\n",
    "- **Printing the Graphs**\n",
    "\n",
    "  Finally, the script prints out the graphs for the training, validation, and testing sets. This helps to ensure that the data has been correctly processed and is ready for the machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5753b-0f33-47b7-891e-11c19f6039fc",
   "metadata": {},
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d110955-0ad0-41d7-81fc-e41226316964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "pos_train_ratio = 1/6\n",
    "pos_val_ratio = 2/6\n",
    "pos_test_ratio = 3/6\n",
    "\n",
    "neg_train_ratio = 1/6\n",
    "neg_val_ratio = 2/6\n",
    "neg_test_ratio = 3/6\n",
    "\n",
    "# Calculate the number of samples for each split\n",
    "num_positive_snp_phenotype_train = int(positive_edges_snp_phenotype.size(1) * pos_train_ratio)\n",
    "num_positive_snp_phenotype_val = int(positive_edges_snp_phenotype.size(1) * pos_val_ratio)\n",
    "num_positive_snp_phenotype_test = positive_edges_snp_phenotype.size(1) - num_positive_snp_phenotype_train - num_positive_snp_phenotype_val\n",
    "\n",
    "num_negative_snp_phenotype_train = int(negative_edges_snp_phenotype.size(1) * neg_train_ratio)\n",
    "num_negative_snp_phenotype_val = int(negative_edges_snp_phenotype.size(1) * neg_val_ratio)\n",
    "num_negative_snp_phenotype_test = negative_edges_snp_phenotype.size(1) - num_negative_snp_phenotype_train - num_negative_snp_phenotype_val\n",
    "\n",
    "# Similar calculations for SNP-SNP edges\n",
    "num_positive_snp_snp_train = int(positive_edges_snp_snp.size(1) * pos_train_ratio)\n",
    "num_positive_snp_snp_val = int(positive_edges_snp_snp.size(1) * pos_val_ratio)\n",
    "num_positive_snp_snp_test = positive_edges_snp_snp.size(1) - num_positive_snp_snp_train - num_positive_snp_snp_val\n",
    "\n",
    "num_negative_snp_snp_train = int(negative_edges_snp_snp.size(1) * neg_train_ratio)\n",
    "num_negative_snp_snp_val = int(negative_edges_snp_snp.size(1) * neg_val_ratio)\n",
    "num_negative_snp_snp_test = negative_edges_snp_snp.size(1) - num_negative_snp_snp_train - num_negative_snp_snp_val\n",
    "\n",
    "# Shuffle the positive and negative edges\n",
    "positive_edges_snp_phenotype = positive_edges_snp_phenotype.t().tolist()\n",
    "random.shuffle(positive_edges_snp_phenotype)\n",
    "\n",
    "negative_edges_snp_phenotype = negative_edges_snp_phenotype.t().tolist()\n",
    "random.shuffle(negative_edges_snp_phenotype)\n",
    "\n",
    "positive_edges_snp_snp = positive_edges_snp_snp.t().tolist()\n",
    "random.shuffle(positive_edges_snp_snp)\n",
    "\n",
    "negative_edges_snp_snp = negative_edges_snp_snp.t().tolist()\n",
    "random.shuffle(negative_edges_snp_snp)\n",
    "\n",
    "# Splitting code for SNP-Phenotype positive and negative edges remains the same.\n",
    "\n",
    "# Split SNP-SNP positive edges\n",
    "positive_snp_snp_train_edges = positive_edges_snp_snp[:num_positive_snp_snp_train]\n",
    "positive_snp_snp_val_edges = positive_edges_snp_snp[num_positive_snp_snp_train:num_positive_snp_snp_train + num_positive_snp_snp_val]\n",
    "positive_snp_snp_test_edges = positive_edges_snp_snp[num_positive_snp_snp_train + num_positive_snp_snp_val:]\n",
    "\n",
    "# Split SNP-SNP negative edges\n",
    "negative_snp_snp_train_edges = negative_edges_snp_snp[:num_negative_snp_snp_train]\n",
    "negative_snp_snp_val_edges = negative_edges_snp_snp[num_negative_snp_snp_train:num_negative_snp_snp_train + num_negative_snp_snp_val]\n",
    "negative_snp_snp_test_edges = negative_edges_snp_snp[num_negative_snp_snp_train + num_negative_snp_snp_val:]\n",
    "\n",
    "# Convert edges back to tensors\n",
    "positive_train_edges = torch.tensor(positive_snp_phenotype_train_edges + positive_snp_snp_train_edges, dtype=torch.long).t().contiguous()\n",
    "positive_val_edges = torch.tensor(positive_snp_phenotype_val_edges + positive_snp_snp_val_edges, dtype=torch.long).t().contiguous()\n",
    "positive_test_edges = torch.tensor(positive_snp_phenotype_test_edges + positive_snp_snp_test_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "negative_train_edges = torch.tensor(negative_snp_phenotype_train_edges + negative_snp_snp_train_edges, dtype=torch.long).t().contiguous()\n",
    "negative_val_edges = torch.tensor(negative_snp_phenotype_val_edges + negative_snp_snp_val_edges, dtype=torch.long).t().contiguous()\n",
    "negative_test_edges = torch.tensor(negative_snp_phenotype_test_edges + negative_snp_snp_test_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create train, validation, and test graphs\n",
    "graph_train = Data(x=features, edge_index=torch.cat([positive_train_edges, negative_train_edges], dim=1), edge_attr=edge_attr)\n",
    "graph_val = Data(x=features, edge_index=torch.cat([positive_val_edges, negative_val_edges], dim=1), edge_attr=edge_attr)\n",
    "graph_test = Data(x=features, edge_index=torch.cat([positive_test_edges, negative_test_edges], dim=1), edge_attr=edge_attr)\n",
    "\n",
    "# Set node types for train, validation, and test graphs\n",
    "graph_train.node_types = node_types\n",
    "graph_val.node_types = node_types\n",
    "graph_test.node_types = node_types\n",
    "\n",
    "# Print the graphs\n",
    "print(\"Graph Train:\")\n",
    "print(graph_train)\n",
    "print(\"\\nGraph Validation:\")\n",
    "print(graph_val)\n",
    "print(\"\\nGraph Test:\")\n",
    "print(graph_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c4b91-e7cc-4e64-9e19-492e6e939e96",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65dfb1-c71a-432a-a385-8d25094b76e1",
   "metadata": {},
   "source": [
    "### Define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d86bc-49dd-41a3-a19e-6439c8b26c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def compute_recall(preds, true_labels):\n",
    "    # Count the number of positive labels\n",
    "    num_pos = np.sum(true_labels == 1)\n",
    "    # Rank the predictions\n",
    "    sorted_preds_idx = np.argsort(preds)[::-1]\n",
    "    # Consider the top-k predictions to be positive\n",
    "    pos_preds_binary = np.zeros_like(preds)\n",
    "    pos_preds_binary[sorted_preds_idx[:num_pos]] = 1\n",
    "    # Calculate the number of true positives and false negatives\n",
    "    true_positives = np.sum((pos_preds_binary == 1) & (true_labels == 1))\n",
    "    false_negatives = np.sum((pos_preds_binary == 0) & (true_labels == 1))\n",
    "    # Calculate recall\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall\n",
    "\n",
    "def compute_precision(preds, true_labels):\n",
    "    # Count the number of positive labels\n",
    "    num_pos = np.sum(true_labels == 1)\n",
    "    # Rank the predictions\n",
    "    sorted_preds_idx = np.argsort(preds)[::-1]\n",
    "    # Consider the top-k predictions to be positive\n",
    "    pos_preds_binary = np.zeros_like(preds)\n",
    "    pos_preds_binary[sorted_preds_idx[:num_pos]] = 1\n",
    "    # Calculate the number of true positives and false positives\n",
    "    true_positives = np.sum((pos_preds_binary == 1) & (true_labels == 1))\n",
    "    false_positives = np.sum((pos_preds_binary == 1) & (true_labels == 0))\n",
    "    # Calculate precision\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "def compute_mrr(preds, true_labels):\n",
    "    # Find the predicted scores for positive examples\n",
    "    pos_preds = preds[:len(true_labels)]\n",
    "    # Rank the positive examples by predicted score in descending order\n",
    "    sorted_idx = np.argsort(pos_preds)[::-1]\n",
    "    # Find the rank of the first true positive\n",
    "    for i, idx in enumerate(sorted_idx):\n",
    "        if true_labels[idx] == 1:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "def compute_hits_at_k(preds, true_labels, k=1):\n",
    "    # Find the predicted scores for positive examples\n",
    "    pos_preds = preds[:len(true_labels)]\n",
    "    # Rank the positive examples by predicted score in descending order\n",
    "    sorted_idx = np.argsort(pos_preds)[::-1]\n",
    "    # Check if the first k predictions contain at least one true positive\n",
    "    hits = 0\n",
    "    for idx in sorted_idx[:k]:\n",
    "        if true_labels[idx] == 1:\n",
    "            hits = 1\n",
    "            break\n",
    "    return hits\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3761f7f-9f6c-4e53-ad2c-d45d877ef8b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2972fd-305a-4ccf-b939-418eb88971fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Logistic Regression model\n",
    "class LogReg(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# Train and evaluate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LogReg(input_dim=9).to(device)  # 9-dimensional edge feature vectors\n",
    "\n",
    "# I'm assuming you have these datasets ready\n",
    "graph_train = graph_train.to(device)\n",
    "graph_val = graph_val.to(device)\n",
    "graph_test = graph_test.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(graph_train.x.float())\n",
    "\n",
    "    pos_edge_index = graph_train.edge_index\n",
    "    neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=z.size(0), num_neg_samples=pos_edge_index.size(1))\n",
    "\n",
    "    pos_logits = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=-1)\n",
    "    neg_logits = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    logits = torch.cat([pos_logits, neg_logits], dim=0)\n",
    "    targets = torch.tensor([1] * pos_edge_index.size(1) + [0] * neg_edge_index.size(1), dtype=torch.float32).to(device)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function without model\n",
    "def evaluate(edge_index, graph):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(graph.x.float())\n",
    "        pos = torch.sigmoid((z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "        neg_edge_index = negative_sampling(edge_index, num_nodes=graph.num_nodes, num_neg_samples=edge_index.size(1))\n",
    "        neg = torch.sigmoid((z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "\n",
    "        preds = np.concatenate([pos.cpu().numpy(), neg.cpu().numpy()])\n",
    "        true_labels = np.concatenate([np.ones_like(pos.cpu().numpy()), np.zeros_like(neg.cpu().numpy())])\n",
    "\n",
    "        # Compute Accuracy for positive edges\n",
    "        pos_preds = preds[:len(pos)]  # positive predictions\n",
    "        pos_labels = true_labels[:len(pos)]  # actual positive labels\n",
    "        pos_accuracy = np.mean((pos_preds > 0.9) == pos_labels)\n",
    "\n",
    "        # Other metrics\n",
    "        roc_auc = roc_auc_score(true_labels, preds)\n",
    "        mrr = compute_mrr(preds, true_labels)\n",
    "        hits_at_5 = compute_hits_at_k(preds, true_labels, k=5)\n",
    "        recall = compute_recall(preds, true_labels)\n",
    "        precision = compute_precision(preds, true_labels)\n",
    "\n",
    "        return roc_auc, mrr, hits_at_5, recall, precision, pos_accuracy\n",
    "\n",
    "max_val_roc_auc = -np.inf\n",
    "max_val_mrr = -np.inf\n",
    "max_val_hits1 = -np.inf\n",
    "max_val_recall = -np.inf\n",
    "max_val_precision = -np.inf\n",
    "max_val_pos_accuracy = -np.inf\n",
    "\n",
    "max_test_roc_auc = -np.inf\n",
    "max_test_mrr = -np.inf\n",
    "max_test_hits1 = -np.inf\n",
    "max_test_recall = -np.inf\n",
    "max_test_precision = -np.inf\n",
    "max_test_pos_accuracy = -np.inf\n",
    "\n",
    "# Assuming the evaluate function is properly defined somewhere else\n",
    "for epoch in range(25):\n",
    "    loss = train()\n",
    "    val_roc_auc, val_mrr, val_hits_at_5, val_recall, val_precision, val_pos_accuracy = evaluate(graph_val.edge_index, graph_val)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss:.4f}, Val ROC-AUC: {val_roc_auc:.10f}, Val MRR: {val_mrr:.10f}, Val hits@1: {val_hits_at_5}, Val Recall: {val_recall:.10f}, Val Precision: {val_precision:.10f}, Val Pos Accuracy: {val_pos_accuracy:.10f}\")\n",
    "    max_val_roc_auc = max(max_val_roc_auc, val_roc_auc)\n",
    "    max_val_mrr = max(max_val_mrr, val_mrr)\n",
    "    max_val_hits1 = max(max_val_hits1, val_hits_at_5)\n",
    "    max_val_recall = max(max_val_recall, val_recall)\n",
    "    max_val_precision = max(max_val_precision, val_precision)\n",
    "    max_val_pos_accuracy = max(max_val_pos_accuracy, val_pos_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffa845-3f01-4177-967a-69d58e303e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each epoch\n",
    "val_roc_auc, val_mrr, val_hits1, val_recall, val_precision, val_pos_accuracy = evaluate(graph_val.edge_index, graph_val)\n",
    "test_roc_auc, test_mrr, test_hits1, test_recall, test_precision, test_pos_accuracy = evaluate(graph_test.edge_index, graph_test)\n",
    "\n",
    "max_val_roc_auc = max(max_val_roc_auc, val_roc_auc)\n",
    "max_val_mrr = max(max_val_mrr, val_mrr)\n",
    "max_val_hits1 = max(max_val_hits1, val_hits1)\n",
    "max_val_recall = max(max_val_recall, val_recall)\n",
    "max_val_precision = max(max_val_precision, val_precision)\n",
    "max_val_pos_accuracy = max(max_val_pos_accuracy, val_pos_accuracy) # Add this line\n",
    "\n",
    "max_test_roc_auc = max(max_test_roc_auc, test_roc_auc)\n",
    "max_test_mrr = max(max_test_mrr, test_mrr)\n",
    "max_test_hits1 = max(max_test_hits1, test_hits1)\n",
    "max_test_recall = max(max_test_recall, test_recall)\n",
    "max_test_precision = max(max_test_precision, test_precision)\n",
    "max_test_pos_accuracy = max(max_test_pos_accuracy, test_pos_accuracy) # Add this line\n",
    "\n",
    "# Print the maximum scores for each metric\n",
    "print(f\"Maximum Validation ROC-AUC: {max_val_roc_auc:.10f}\")\n",
    "print(f\"Maximum Validation MRR: {max_val_mrr:.10f}\")\n",
    "print(f\"Maximum Validation hits@1: {max_val_hits1:.10f}\")\n",
    "print(f\"Maximum Validation Recall: {max_val_recall:.10f}\")\n",
    "print(f\"Maximum Validation Precision: {max_val_precision:.10f}\")\n",
    "print(f\"Maximum Validation Positive Edge Accuracy: {max_val_pos_accuracy:.10f}\") # Add this line\n",
    "\n",
    "print(f\"Maximum Test ROC-AUC: {max_test_roc_auc:.10f}\")\n",
    "print(f\"Maximum Test MRR: {max_test_mrr:.10f}\")\n",
    "print(f\"Maximum Test hits@1: {max_test_hits1:.10f}\")\n",
    "print(f\"Maximum Test Recall: {max_test_recall:.10f}\")\n",
    "print(f\"Maximum Test Precision: {max_test_precision:.10f}\")\n",
    "print(f\"Maximum Test Positive Edge Accuracy: {max_test_pos_accuracy:.10f}\") # Add this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10e208-06ab-4242-80cc-fa0004b49ef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbf66b-a6a4-407b-b4f9-7dd93eb24e49",
   "metadata": {},
   "source": [
    "- The GCN (Graph Convolutional Network) model used in this script is a simple 2-layer GCN. It transforms the original 9-dimensional node feature vectors into 2-dimensional hidden representations, using the adjacency matrix (encoded by `edge_index`) to propagate information across the graph.\n",
    "- The model is trained using Focal Loss, which is designed to address class imbalance problems. The training function computes the Focal Loss between the model's predictions on positive and negative edge examples, and the true edge labels. Negative edges are generated using a negative sampling method.\n",
    "- During evaluation, the model's embeddings are used to predict whether an edge exists between each pair of nodes, and these predictions are compared to the actual edges in the validation or test graph. Several evaluation metrics are computed, including ROC AUC, Mean Reciprocal Rank (MRR), hits@1, Recall, and Precision.\n",
    "- The training process iterates for 100 epochs. In each epoch, the model parameters are updated to minimize the Focal Loss on the training data, and the model's performance is evaluated on the validation data. The best validation scores on the ROC AUC, MRR, hits@1, Recall, and Precision metrics are tracked throughout training.\n",
    "- After training, the model can be used to predict whether causal edges exist between nodes in a graph. This makes it suitable for tasks like link prediction in biological networks, where the nodes represent entities like genes or phenotypes and the edges represent relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744860e-e107-4495-9e88-79fe72ac7db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task: Link prediction: does a causal edge exist between two nodes?\n",
    "# Node Types: 0 = phenotypes, 1 = snps\n",
    "# Node Feature Vector: 10-dimensional\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(9, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Train and evaluate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(hidden_channels=2).to(device)\n",
    "\n",
    "graph_train = graph_train.to(device)\n",
    "graph_val = graph_val.to(device)\n",
    "graph_test = graph_test.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Instantiate the loss function\n",
    "focal_loss = FocalLoss(alpha=0.9, gamma=2.0).to(device)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(graph_train.x.float(), graph_train.edge_index)\n",
    "\n",
    "    # Only consider positive edges for the positive score calculation\n",
    "    pos_edge_index = graph_train.edge_index\n",
    "    pos = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    # Use negative_sampling to generate negative edges\n",
    "    neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=z.size(0), num_neg_samples=pos_edge_index.size(1))\n",
    "    neg = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    logits = torch.cat([pos, neg], dim=0)\n",
    "    targets = torch.tensor([1] * pos.size(0) + [0] * neg.size(0), dtype=torch.float32).to(device)\n",
    "\n",
    "    loss = focal_loss(logits, targets) # replace BCE with focal loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(edge_index, graph):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(graph.x.float(), graph.edge_index)\n",
    "        pos = torch.sigmoid((z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "        neg_edge_index = negative_sampling(edge_index, num_nodes=graph.num_nodes, num_neg_samples=edge_index.size(1))\n",
    "        neg = torch.sigmoid((z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)).view(-1)\n",
    "\n",
    "        preds = np.concatenate([pos.cpu().numpy(), neg.cpu().numpy()])\n",
    "        true_labels = np.concatenate([np.ones_like(pos.cpu().numpy()), np.zeros_like(neg.cpu().numpy())])\n",
    "\n",
    "        # Compute Accuracy for positive edges\n",
    "        pos_preds = preds[:len(pos)]  # positive predictions\n",
    "        pos_labels = true_labels[:len(pos)]  # actual positive labels\n",
    "        pos_accuracy = np.mean((pos_preds > 0.9) == pos_labels)\n",
    "\n",
    "        # Other metrics\n",
    "        roc_auc = roc_auc_score(true_labels, preds)\n",
    "        mrr = compute_mrr(preds, true_labels)\n",
    "        hits_at_5 = compute_hits_at_k(preds, true_labels, k=5)\n",
    "        recall = compute_recall(preds, true_labels)\n",
    "        precision = compute_precision(preds, true_labels)\n",
    "\n",
    "        return roc_auc, mrr, hits_at_5, recall, precision, pos_accuracy\n",
    "\n",
    "\n",
    "max_val_roc_auc = -np.inf\n",
    "max_val_mrr = -np.inf\n",
    "max_val_hits1 = -np.inf\n",
    "max_val_recall = -np.inf\n",
    "max_val_precision = -np.inf\n",
    "max_val_pos_accuracy = -np.inf\n",
    "\n",
    "max_test_roc_auc = -np.inf\n",
    "max_test_mrr = -np.inf\n",
    "max_test_hits1 = -np.inf\n",
    "max_test_recall = -np.inf\n",
    "max_test_precision = -np.inf\n",
    "max_test_pos_accuracy = -np.inf\n",
    "\n",
    "for epoch in range(25):\n",
    "    loss = train()\n",
    "    val_roc_auc, val_mrr, val_hits_at_5, val_recall, val_precision, val_pos_accuracy = evaluate(graph_val.edge_index, graph_val)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss:.4f}, Val ROC-AUC: {val_roc_auc:.10f}, Val MRR: {val_mrr:.10f}, Val hits@1: {val_hits_at_5}, Val Recall: {val_recall:.10f}, Val Precision: {val_precision:.10f}, Val Pos Accuracy: {val_pos_accuracy:.10f}\")\n",
    "    max_val_roc_auc = max(max_val_roc_auc, val_roc_auc)\n",
    "    max_val_mrr = max(max_val_mrr, val_mrr)\n",
    "    max_val_hits1 = max(max_val_hits1, val_hits_at_5)\n",
    "    max_val_recall = max(max_val_recall, val_recall)\n",
    "    max_val_precision = max(max_val_precision, val_precision)\n",
    "    max_val_pos_accuracy = max(max_val_pos_accuracy, val_pos_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55930699-10ed-4220-8144-139098ecf0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each epoch\n",
    "val_roc_auc, val_mrr, val_hits1, val_recall, val_precision, val_pos_accuracy = evaluate(graph_val.edge_index, graph_val)\n",
    "test_roc_auc, test_mrr, test_hits1, test_recall, test_precision, test_pos_accuracy = evaluate(graph_test.edge_index, graph_test)\n",
    "\n",
    "max_val_roc_auc = max(max_val_roc_auc, val_roc_auc)\n",
    "max_val_mrr = max(max_val_mrr, val_mrr)\n",
    "max_val_hits1 = max(max_val_hits1, val_hits1)\n",
    "max_val_recall = max(max_val_recall, val_recall)\n",
    "max_val_precision = max(max_val_precision, val_precision)\n",
    "max_val_pos_accuracy = max(max_val_pos_accuracy, val_pos_accuracy) # Add this line\n",
    "\n",
    "max_test_roc_auc = max(max_test_roc_auc, test_roc_auc)\n",
    "max_test_mrr = max(max_test_mrr, test_mrr)\n",
    "max_test_hits1 = max(max_test_hits1, test_hits1)\n",
    "max_test_recall = max(max_test_recall, test_recall)\n",
    "max_test_precision = max(max_test_precision, test_precision)\n",
    "max_test_pos_accuracy = max(max_test_pos_accuracy, test_pos_accuracy) # Add this line\n",
    "\n",
    "# Print the maximum scores for each metric\n",
    "print(f\"Maximum Validation ROC-AUC: {max_val_roc_auc:.10f}\")\n",
    "print(f\"Maximum Validation MRR: {max_val_mrr:.10f}\")\n",
    "print(f\"Maximum Validation hits@1: {max_val_hits1:.10f}\")\n",
    "print(f\"Maximum Validation Recall: {max_val_recall:.10f}\")\n",
    "print(f\"Maximum Validation Precision: {max_val_precision:.10f}\")\n",
    "print(f\"Maximum Validation Positive Edge Accuracy: {max_val_pos_accuracy:.10f}\") # Add this line\n",
    "\n",
    "print(f\"Maximum Test ROC-AUC: {max_test_roc_auc:.10f}\")\n",
    "print(f\"Maximum Test MRR: {max_test_mrr:.10f}\")\n",
    "print(f\"Maximum Test hits@1: {max_test_hits1:.10f}\")\n",
    "print(f\"Maximum Test Recall: {max_test_recall:.10f}\")\n",
    "print(f\"Maximum Test Precision: {max_test_precision:.10f}\")\n",
    "print(f\"Maximum Test Positive Edge Accuracy: {max_test_pos_accuracy:.10f}\") # Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0560b-c904-40ba-8bfb-03746b949ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model)} parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
