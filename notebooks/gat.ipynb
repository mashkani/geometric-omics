{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8856a85c-cc6c-4e9c-9466-fafee6338628",
   "metadata": {
    "tags": []
   },
   "source": [
    "## README \n",
    "Website: https://www.eqtlgen.org/phase1.html\n",
    "\n",
    "Paper: https://www.nature.com/articles/s41588-021-00913-zhttps://www.nature.com/articles/s41588-021-00913-z\n",
    "\n",
    "This Jupyter notebook README covers cis-eQTL and trans-eQTL results from the eQTLGen project. The dataset includes significant files for both cis-eQTL and trans-eQTL analyses. \n",
    "\n",
    "Files contain various columns with information on P-value, SNP rs ID, SNP chromosome, SNP position, assessed and not assessed alleles, Z-score, ENSG and HGNC names of eQTL genes, gene chromosome, gene position, number of cohorts, number of samples, false discovery rate, and Bonferroni-corrected P-value.\n",
    "\n",
    "The cis-eQTL analysis includes 19,250 genes expressed in blood, with SNP-gene combinations within 1Mb from the gene center and tested in at least 2 cohorts. The trans-eQTL analysis tests 19,960 genes expressed in blood and 10,317 trait-associated SNPs based on GWAS Catalog, Immunobase, and Astle et al. study. Trans-eQTL combinations have a distance of >5Mb and were tested in at least 2 cohorts.\n",
    "\n",
    "The FDR calculation uses a pruned set of SNPs for trans-eQTL mapping and permutation-based FDR calculation. Crossmapping filters are applied to identify and remove potential artifacts in trans-eQTL results, recalculating the FDR afterward. Note that the full results file has not been filtered for cross-mapping effects, which may lead to artifacts in the data.\n",
    "\n",
    "The code below demonstrates the process of creating a graph-based representation of the combined cis and trans-eQTL data using PyTorch Geometric. This process can be broken down into several steps:\n",
    "\n",
    "1. Combine cis and trans dataframes: The code begins by concatenating the cis and trans dataframes into a single dataframe named 'data', which contains information on both cis-eQTL and trans-eQTL results. This combined dataset simplifies the process of working with the data and ensures that all relevant information is contained within a single data structure.\n",
    "\n",
    "2. Create mappings for genes and SNPs: To represent the genes and SNPs as nodes in the graph, integer indices are assigned to each unique gene and SNP. This is done using dictionaries called 'gene_to_idx' and 'snp_to_idx', which map the gene and SNP identifiers to their corresponding integer indices.\n",
    "\n",
    "3. Generate node type labels: Node type labels are created using PyTorch tensors, distinguishing between gene nodes (assigned a label of 0) and SNP nodes (assigned a label of 1). This differentiation is useful for various graph-based analyses and machine learning tasks that require knowledge of node types.\n",
    "\n",
    "4. Create edges based on gene and SNP indices: Edges in the graph represent the relationships between genes and SNPs. These edges are created by iterating over the 'data' dataframe and extracting the corresponding gene and SNP indices from the previously created mappings. The edges are then represented as a PyTorch tensor with a long data type.\n",
    "\n",
    "5. Convert edges to undirected: Since the relationships between genes and SNPs are undirected, the edges in the graph should also be undirected. This is achieved using the 'to_undirected()' function from PyTorch Geometric, which ensures that the graph correctly represents the underlying biology.\n",
    "\n",
    "6. Create a PyTorch Geometric graph: Finally, the graph is created using the PyTorch Geometric 'Data' class. The node types and edge indices are used as inputs to instantiate the graph object, which can then be utilized for further analysis and visualization.\n",
    "\n",
    "The resulting 'graph' object is a PyTorch Geometric representation of the combined cis and trans-eQTL data. The prediction task is to predict new association edges given the training edges, with the task type being link prediction. Below are a few important graph statistics:\n",
    "\n",
    "- Number of nodes: 3681495\n",
    "- Number of SNP nodes: 3664025\n",
    "- Number of Gene nodes: 17470\n",
    "- Number of edges: 10567450\n",
    "- Number of connected components: 424\n",
    "- Average degree: 5.74\n",
    "- Median degree: 2.0\n",
    "- Standard deviation of degree: 69.81\n",
    "- Density: 0.0000015594\n",
    "- Assortativity: -0.2267915607\n",
    "\n",
    "The prediction task is to predict new association edges given the training edges, with the task type being link prediction. The data splitting is random while maintaining an equal proportion of cis- and trans- associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5b6c8-9ccf-4c35-b6dc-e8755e8c251e",
   "metadata": {},
   "source": [
    "## Data Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c312ac4-08e0-46ae-b18a-10f81e6b2ddb",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f0bf7f-91fc-4010-bb06-e5e4434c3c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "\n",
    "import networkx as nx\n",
    "from ogb.io import DatasetSaver\n",
    "from ogb.linkproppred import LinkPropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c398fa63-4d51-40a5-baab-d085d2beaf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0+cu118\n",
      "PyTorch Geometric version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bdb806-8661-4798-93a7-85e08eff2304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce RTX 3060 Ti (cuda)\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5ebac-59d8-4e08-9de7-a3a51e770769",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2efe87e-d367-4fea-93f0-1502f08c9558",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3680443, 1], edge_index=[2, 21015328])\n",
      "Data(x=[10151, 1], edge_index=[2, 119572])\n",
      "Data(x=[3681495, 1], edge_index=[2, 21134900])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "cis = pd.read_csv(\"sig-cis.csv\")\n",
    "trans = pd.read_csv(\"sig-trans.csv\")\n",
    "\n",
    "\n",
    "# For the cis dataframe\n",
    "\n",
    "cis_genes = cis['Gene'].unique()\n",
    "cis_snps = cis['SNP'].unique()\n",
    "cis_gene_to_idx = {gene: idx for idx, gene in enumerate(cis_genes)}\n",
    "cis_snp_to_idx = {snp: idx + len(cis_genes) for idx, snp in enumerate(cis_snps)}\n",
    "\n",
    "cis_node_types = torch.tensor([0] * len(cis_genes) + [1] * len(cis_snps), dtype=torch.long)\n",
    "\n",
    "cis_edges = cis.apply(lambda row: (cis_gene_to_idx[row['Gene']], cis_snp_to_idx[row['SNP']]), axis=1)\n",
    "cis_edges = torch.tensor(list(cis_edges), dtype=torch.long).t().contiguous()\n",
    "\n",
    "cis_edges = to_undirected(cis_edges)\n",
    "\n",
    "cis_graph = Data(x=cis_node_types.view(-1, 1), edge_index=cis_edges)\n",
    "\n",
    "print(cis_graph)\n",
    "\n",
    "\n",
    "\n",
    "# For the trans dataframe\n",
    "trans_genes = trans['Gene'].unique()\n",
    "trans_snps = trans['SNP'].unique()\n",
    "trans_gene_to_idx = {gene: idx for idx, gene in enumerate(trans_genes)}\n",
    "trans_snp_to_idx = {snp: idx + len(trans_genes) for idx, snp in enumerate(trans_snps)}\n",
    "\n",
    "trans_node_types = torch.tensor([0] * len(trans_genes) + [1] * len(trans_snps), dtype=torch.long)\n",
    "\n",
    "trans_edges = trans.apply(lambda row: (trans_gene_to_idx[row['Gene']], trans_snp_to_idx[row['SNP']]), axis=1)\n",
    "trans_edges = torch.tensor(list(trans_edges), dtype=torch.long).t().contiguous()\n",
    "\n",
    "trans_edges = to_undirected(trans_edges)\n",
    "\n",
    "trans_graph = Data(x=trans_node_types.view(-1, 1), edge_index=trans_edges)\n",
    "\n",
    "print(trans_graph)\n",
    "\n",
    "\n",
    "\n",
    "# Combine the cis and trans dataframes\n",
    "data = pd.concat([cis, trans], ignore_index=True)\n",
    "\n",
    "# Function to filter the lowest 20% of associations\n",
    "def filter_lowest_20_percent(df):\n",
    "    threshold = np.percentile(df['BonferroniP'], 20)\n",
    "    return df[df['BonferroniP'] <= threshold]\n",
    "\n",
    "# Filter cis and trans dataframes\n",
    "cis_filtered = filter_lowest_20_percent(cis)\n",
    "trans_filtered = filter_lowest_20_percent(trans)\n",
    "\n",
    "# Remove filtered data from original dataframes\n",
    "cis_remaining = cis[~cis.index.isin(cis_filtered.index)]\n",
    "trans_remaining = trans[~trans.index.isin(trans_filtered.index)]\n",
    "\n",
    "# Test that all values in 'BonferroniP' column of cis_filtered are lower than cis_remaining\n",
    "assert all(cis_filtered['BonferroniP'] <= cis_remaining['BonferroniP'].min()), \"Values in cis_filtered are not all lower than cis_remaining\"\n",
    "\n",
    "# Test that all values in 'BonferroniP' column of trans_filtered are lower than trans_remaining\n",
    "assert all(trans_filtered['BonferroniP'] <= trans_remaining['BonferroniP'].min()), \"Values in trans_filtered are not all lower than trans_remaining\"\n",
    "\n",
    "# Create mappings for genes and SNPs to integer indices\n",
    "genes = data['Gene'].unique()\n",
    "snps = data['SNP'].unique()\n",
    "gene_to_idx = {gene: idx for idx, gene in enumerate(genes)}\n",
    "snp_to_idx = {snp: idx + len(genes) for idx, snp in enumerate(snps)}\n",
    "\n",
    "# Create node type labels\n",
    "node_types = torch.tensor([0] * len(genes) + [1] * len(snps), dtype=torch.long)\n",
    "\n",
    "# Create edges\n",
    "edges = data.apply(lambda row: (gene_to_idx[row['Gene']], snp_to_idx[row['SNP']]), axis=1)\n",
    "edges = torch.tensor(list(edges), dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Convert edges to undirected\n",
    "edges = to_undirected(edges)\n",
    "\n",
    "# Create the PyTorch Geometric graph\n",
    "graph = Data(x=node_types.view(-1, 1), edge_index=edges)\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c04a0f-6b11-4759-b562-edb64e7a3841",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6015033-e3b5-43c9-9586-64a512c1f080",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdee40ba-792a-452f-b267-ffbe6920f4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3681495, 1], edge_index=[2, 2113514])\n",
      "Data(x=[3681495, 1], edge_index=[2, 4226968])\n",
      "Data(x=[3681495, 1], edge_index=[2, 4226968])\n"
     ]
    }
   ],
   "source": [
    "# Use filtered data as training data\n",
    "train_data = pd.concat([cis_filtered, trans_filtered], ignore_index=True)\n",
    "\n",
    "# Split remaining cis and trans dataframes into validation and test sets\n",
    "cis_val, cis_test = train_test_split(cis_remaining, test_size=0.5, random_state=42)\n",
    "trans_val, trans_test = train_test_split(trans_remaining, test_size=0.5, random_state=42)\n",
    "\n",
    "# Concatenate the respective sets\n",
    "val_data = pd.concat([cis_val, trans_val], ignore_index=True)\n",
    "test_data = pd.concat([cis_test, trans_test], ignore_index=True)\n",
    "\n",
    "# Create edge indices for train, validation, and test sets\n",
    "edge_index_train = train_data.apply(lambda row: (gene_to_idx[row['Gene']], snp_to_idx[row['SNP']]), axis=1)\n",
    "edge_index_val = val_data.apply(lambda row: (gene_to_idx[row['Gene']], snp_to_idx[row['SNP']]), axis=1)\n",
    "edge_index_test = test_data.apply(lambda row: (gene_to_idx[row['Gene']], snp_to_idx[row['SNP']]), axis=1)\n",
    "\n",
    "edge_index_train = torch.tensor(list(edge_index_train), dtype=torch.long).t().contiguous()\n",
    "edge_index_val = torch.tensor(list(edge_index_val), dtype=torch.long).t().contiguous()\n",
    "edge_index_test = torch.tensor(list(edge_index_test), dtype=torch.long).t().contiguous()\n",
    "\n",
    "graph_train = Data(x=graph.x, edge_index=edge_index_train)\n",
    "graph_val = Data(x=graph.x, edge_index=edge_index_val)\n",
    "graph_test = Data(x=graph.x, edge_index=edge_index_test)\n",
    "\n",
    "print(graph_train)\n",
    "print(graph_val)\n",
    "print(graph_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3febf7-7f72-459b-a8d0-110912c27aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c8a1fd-93fb-4707-9c04-e6711d7b3d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "GAT(\n",
      "  (conv1): GATConv(1, 3, heads=6)\n",
      "  (conv2): GATConv(18, 3, heads=6)\n",
      ")\n",
      "Total Parameters: 450\n",
      "None\n",
      "Epoch: 1, Loss: 1.1692\n",
      "Epoch: 2, Loss: 0.9775\n",
      "Epoch: 3, Loss: 0.8518\n",
      "Epoch: 4, Loss: 0.7733\n",
      "Epoch: 5, Loss: 0.7265\n",
      "Epoch: 6, Loss: 0.6980\n",
      "Epoch: 7, Loss: 0.6819\n",
      "Epoch: 8, Loss: 0.6719\n",
      "Epoch: 9, Loss: 0.6647\n",
      "Epoch: 10, Loss: 0.6589\n",
      "Epoch: 11, Loss: 0.6531\n",
      "Epoch: 12, Loss: 0.6463\n",
      "Epoch: 13, Loss: 0.6383\n",
      "Epoch: 14, Loss: 0.6297\n",
      "Epoch: 15, Loss: 0.6170\n",
      "Epoch: 16, Loss: 0.6045\n",
      "Epoch: 17, Loss: 0.5915\n",
      "Epoch: 18, Loss: 0.5747\n",
      "Epoch: 19, Loss: 0.5599\n",
      "Epoch: 20, Loss: 0.5453\n",
      "Epoch: 21, Loss: 0.5288\n",
      "Epoch: 22, Loss: 0.5137\n",
      "Epoch: 23, Loss: 0.4993\n",
      "Epoch: 24, Loss: 0.4865\n",
      "Epoch: 25, Loss: 0.4747\n",
      "Epoch: 26, Loss: 0.4664\n",
      "Epoch: 27, Loss: 0.4582\n",
      "Epoch: 28, Loss: 0.4538\n",
      "Epoch: 29, Loss: 0.4499\n",
      "Epoch: 30, Loss: 0.4475\n",
      "Epoch: 31, Loss: 0.4475\n",
      "Epoch: 32, Loss: 0.4484\n",
      "Epoch: 33, Loss: 0.4483\n",
      "Epoch: 34, Loss: 0.4494\n",
      "Epoch: 35, Loss: 0.4496\n",
      "Epoch: 36, Loss: 0.4499\n",
      "Epoch: 37, Loss: 0.4496\n",
      "Epoch: 38, Loss: 0.4492\n",
      "Epoch: 39, Loss: 0.4477\n",
      "Epoch: 40, Loss: 0.4465\n",
      "Epoch: 41, Loss: 0.4439\n",
      "Epoch: 42, Loss: 0.4431\n",
      "Epoch: 43, Loss: 0.4414\n",
      "Epoch: 44, Loss: 0.4391\n",
      "Epoch: 45, Loss: 0.4388\n",
      "Epoch: 46, Loss: 0.4368\n",
      "Epoch: 47, Loss: 0.4378\n",
      "Epoch: 48, Loss: 0.4362\n",
      "Epoch: 49, Loss: 0.4350\n",
      "Epoch: 50, Loss: 0.4351\n",
      "Epoch: 51, Loss: 0.4348\n",
      "Epoch: 52, Loss: 0.4338\n",
      "Epoch: 53, Loss: 0.4348\n",
      "Epoch: 54, Loss: 0.4339\n",
      "Epoch: 55, Loss: 0.4343\n",
      "Epoch: 56, Loss: 0.4342\n",
      "Epoch: 57, Loss: 0.4336\n",
      "Epoch: 58, Loss: 0.4330\n",
      "Epoch: 59, Loss: 0.4323\n",
      "Epoch: 60, Loss: 0.4322\n",
      "Epoch: 61, Loss: 0.4328\n",
      "Epoch: 62, Loss: 0.4319\n",
      "Epoch: 63, Loss: 0.4319\n",
      "Epoch: 64, Loss: 0.4314\n",
      "Epoch: 65, Loss: 0.4301\n",
      "Epoch: 66, Loss: 0.4297\n",
      "Epoch: 67, Loss: 0.4301\n",
      "Epoch: 68, Loss: 0.4302\n",
      "Epoch: 69, Loss: 0.4293\n",
      "Epoch: 70, Loss: 0.4291\n",
      "Epoch: 71, Loss: 0.4291\n",
      "Epoch: 72, Loss: 0.4296\n",
      "Epoch: 73, Loss: 0.4290\n",
      "Epoch: 74, Loss: 0.4287\n",
      "Epoch: 75, Loss: 0.4285\n",
      "Epoch: 76, Loss: 0.4281\n",
      "Epoch: 77, Loss: 0.4275\n",
      "Epoch: 78, Loss: 0.4275\n",
      "Epoch: 79, Loss: 0.4268\n",
      "Epoch: 80, Loss: 0.4271\n",
      "Epoch: 81, Loss: 0.4258\n",
      "Epoch: 82, Loss: 0.4256\n",
      "Epoch: 83, Loss: 0.4261\n",
      "Epoch: 84, Loss: 0.4262\n",
      "Epoch: 85, Loss: 0.4254\n",
      "Epoch: 86, Loss: 0.4243\n",
      "Epoch: 87, Loss: 0.4248\n",
      "Epoch: 88, Loss: 0.4241\n",
      "Epoch: 89, Loss: 0.4249\n",
      "Epoch: 90, Loss: 0.4248\n",
      "Epoch: 91, Loss: 0.4239\n",
      "Epoch: 92, Loss: 0.4235\n",
      "Epoch: 93, Loss: 0.4239\n",
      "Epoch: 94, Loss: 0.4230\n",
      "Epoch: 95, Loss: 0.4237\n",
      "Epoch: 96, Loss: 0.4223\n",
      "Epoch: 97, Loss: 0.4224\n",
      "Epoch: 98, Loss: 0.4221\n",
      "Epoch: 99, Loss: 0.4219\n",
      "Epoch: 100, Loss: 0.4212\n",
      "Epoch: 101, Loss: 0.4213\n",
      "Epoch: 102, Loss: 0.4205\n",
      "Epoch: 103, Loss: 0.4206\n",
      "Epoch: 104, Loss: 0.4202\n",
      "Epoch: 105, Loss: 0.4191\n",
      "Epoch: 106, Loss: 0.4201\n",
      "Epoch: 107, Loss: 0.4199\n",
      "Epoch: 108, Loss: 0.4197\n",
      "Epoch: 109, Loss: 0.4185\n",
      "Epoch: 110, Loss: 0.4190\n",
      "Epoch: 111, Loss: 0.4182\n",
      "Epoch: 112, Loss: 0.4178\n",
      "Epoch: 113, Loss: 0.4173\n",
      "Epoch: 114, Loss: 0.4161\n",
      "Epoch: 115, Loss: 0.4159\n",
      "Epoch: 116, Loss: 0.4157\n",
      "Epoch: 117, Loss: 0.4156\n",
      "Epoch: 118, Loss: 0.4157\n",
      "Epoch: 119, Loss: 0.4154\n",
      "Epoch: 120, Loss: 0.4144\n",
      "Epoch: 121, Loss: 0.4145\n",
      "Epoch: 122, Loss: 0.4142\n",
      "Epoch: 123, Loss: 0.4128\n",
      "Epoch: 124, Loss: 0.4133\n",
      "Epoch: 125, Loss: 0.4125\n",
      "Epoch: 126, Loss: 0.4125\n",
      "Epoch: 127, Loss: 0.4117\n",
      "Epoch: 128, Loss: 0.4115\n",
      "Epoch: 129, Loss: 0.4105\n",
      "Epoch: 130, Loss: 0.4105\n",
      "Epoch: 131, Loss: 0.4099\n",
      "Epoch: 132, Loss: 0.4096\n",
      "Epoch: 133, Loss: 0.4087\n",
      "Epoch: 134, Loss: 0.4086\n",
      "Epoch: 135, Loss: 0.4084\n",
      "Epoch: 136, Loss: 0.4073\n",
      "Epoch: 137, Loss: 0.4077\n",
      "Epoch: 138, Loss: 0.4065\n",
      "Epoch: 139, Loss: 0.4059\n",
      "Epoch: 140, Loss: 0.4058\n",
      "Epoch: 141, Loss: 0.4053\n",
      "Epoch: 142, Loss: 0.4039\n",
      "Epoch: 143, Loss: 0.4048\n",
      "Epoch: 144, Loss: 0.4036\n",
      "Epoch: 145, Loss: 0.4045\n",
      "Epoch: 146, Loss: 0.4029\n",
      "Epoch: 147, Loss: 0.4024\n",
      "Epoch: 148, Loss: 0.4023\n",
      "Epoch: 149, Loss: 0.4006\n",
      "Epoch: 150, Loss: 0.4009\n",
      "Epoch: 151, Loss: 0.4001\n",
      "Epoch: 152, Loss: 0.4000\n",
      "Epoch: 153, Loss: 0.3995\n",
      "Epoch: 154, Loss: 0.3986\n",
      "Epoch: 155, Loss: 0.3980\n",
      "Epoch: 156, Loss: 0.3980\n",
      "Epoch: 157, Loss: 0.3969\n",
      "Epoch: 158, Loss: 0.3966\n",
      "Epoch: 159, Loss: 0.3957\n",
      "Epoch: 160, Loss: 0.3956\n",
      "Epoch: 161, Loss: 0.3948\n",
      "Epoch: 162, Loss: 0.3947\n",
      "Epoch: 163, Loss: 0.3938\n",
      "Epoch: 164, Loss: 0.3941\n",
      "Epoch: 165, Loss: 0.3933\n",
      "Epoch: 166, Loss: 0.3924\n",
      "Epoch: 167, Loss: 0.3919\n",
      "Epoch: 168, Loss: 0.3914\n",
      "Epoch: 169, Loss: 0.3905\n",
      "Epoch: 170, Loss: 0.3905\n",
      "Epoch: 171, Loss: 0.3906\n",
      "Epoch: 172, Loss: 0.3898\n",
      "Epoch: 173, Loss: 0.3899\n",
      "Epoch: 174, Loss: 0.3889\n",
      "Epoch: 175, Loss: 0.3892\n",
      "Epoch: 176, Loss: 0.3883\n",
      "Epoch: 177, Loss: 0.3878\n",
      "Epoch: 178, Loss: 0.3877\n",
      "Epoch: 179, Loss: 0.3876\n",
      "Epoch: 180, Loss: 0.3864\n",
      "Epoch: 181, Loss: 0.3867\n",
      "Epoch: 182, Loss: 0.3866\n",
      "Epoch: 183, Loss: 0.3855\n",
      "Epoch: 184, Loss: 0.3853\n",
      "Epoch: 185, Loss: 0.3851\n",
      "Epoch: 186, Loss: 0.3847\n",
      "Epoch: 187, Loss: 0.3843\n",
      "Epoch: 188, Loss: 0.3833\n",
      "Epoch: 189, Loss: 0.3837\n",
      "Epoch: 190, Loss: 0.3837\n",
      "Epoch: 191, Loss: 0.3831\n",
      "Epoch: 192, Loss: 0.3831\n",
      "Epoch: 193, Loss: 0.3831\n",
      "Epoch: 194, Loss: 0.3827\n",
      "Epoch: 195, Loss: 0.3819\n",
      "Epoch: 196, Loss: 0.3813\n",
      "Epoch: 197, Loss: 0.3815\n",
      "Epoch: 198, Loss: 0.3812\n",
      "Epoch: 199, Loss: 0.3809\n",
      "Epoch: 200, Loss: 0.3808\n",
      "Validation Hits@6: 0.5000000000\n",
      "Test Hits@6: 0.5000000000\n",
      "\n",
      "Validation MRR: 0.5000000000\n",
      "Test MRR: 0.5000000000\n",
      "\n",
      "Validation ROC-AUC: 0.5000000000\n",
      "Test ROC-AUC: 0.5000000000\n",
      "\n",
      "Validation AP: 0.5000000000\n",
      "Test AP: 0.5000000000\n",
      "\n",
      "Validation Precision: 0.5000000000\n",
      "Test Precision: 0.5000000000\n",
      "\n",
      "Validation Recall: 1.0000000000\n",
      "Test Recall: 1.0000000000\n",
      "\n",
      "Validation F1 Score: 0.6666666667\n",
      "Test F1 Score: 0.6666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(1, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=heads)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "def model_summary(model):\n",
    "    print(\"Model Summary:\")\n",
    "    print(model)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "\n",
    "# Train and evaluate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GAT(hidden_channels=3, heads=6).to(device)\n",
    "print(model_summary(model)) \n",
    "\n",
    "graph_train = graph_train.to(device)\n",
    "graph_val = graph_val.to(device)\n",
    "graph_test = graph_test.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(graph_train.x.float(), graph_train.edge_index)\n",
    "    pos_inner_product = (z[edge_index_train[0]] * z[edge_index_train[1]]).sum(dim=-1)\n",
    "    neg_edge_index = negative_sampling(graph_train.edge_index, num_nodes=graph_train.num_nodes, num_neg_samples=graph_train.edge_index.size(1))\n",
    "    neg_inner_product = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    logits = torch.cat([pos_inner_product, neg_inner_product], dim=0)\n",
    "    targets = torch.tensor([1] * pos_inner_product.size(0) + [0] * neg_inner_product.size(0), dtype=torch.float32).to(device)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def hits_at_k(preds, true_labels, k=6):\n",
    "    k = min(k, len(preds))\n",
    "    top_k_indices = np.argpartition(preds, -k)[-k:]\n",
    "    hits = np.sum(true_labels[top_k_indices])\n",
    "    return hits / k\n",
    "\n",
    "def mean_reciprocal_rank(preds, true_labels):\n",
    "    sorted_indices = np.argsort(preds)[::-1]\n",
    "    sorted_true_labels = true_labels[sorted_indices]\n",
    "    non_zero_indices = np.nonzero(sorted_true_labels)[0]\n",
    "    return np.mean(1 / (non_zero_indices + 1))\n",
    "\n",
    "def evaluate(edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(graph_test.x.float(), graph_test.edge_index)\n",
    "        pos_pred = torch.sigmoid(torch.sum(z[edge_index[0]] * z[edge_index[1]])).view(-1)\n",
    "        neg_edge_index = negative_sampling(edge_index, num_nodes=graph_test.num_nodes, num_neg_samples=edge_index.size(1))\n",
    "        neg_pred = torch.sigmoid(torch.sum(z[neg_edge_index[0]] * z[neg_edge_index[1]])).view(-1)\n",
    "        pos_pred = pos_pred.cpu().numpy()\n",
    "        neg_pred = neg_pred.cpu().numpy()\n",
    "\n",
    "        preds = np.concatenate([pos_pred, neg_pred])\n",
    "        true_labels = np.concatenate([np.ones_like(pos_pred), np.zeros_like(neg_pred)])\n",
    "\n",
    "        hits_6 = hits_at_k(preds, true_labels)\n",
    "        mrr = mean_reciprocal_rank(preds, true_labels)\n",
    "        roc_auc = roc_auc_score(true_labels, preds)\n",
    "        ap = average_precision_score(true_labels, preds)\n",
    "\n",
    "        binary_preds = np.round(preds)\n",
    "        precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, binary_preds, average='binary')\n",
    "\n",
    "        return hits_6, mrr, roc_auc, ap, precision, recall, f1_score\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "val_metrics = evaluate(graph_val.edge_index)\n",
    "test_metrics = evaluate(graph_test.edge_index)\n",
    "\n",
    "metric_names = ['Hits@6', 'MRR', 'ROC-AUC', 'AP', 'Precision', 'Recall', 'F1 Score']\n",
    "for name, val_metric, test_metric in zip(metric_names, val_metrics, test_metrics):\n",
    "    print(f\"Validation {name}: {val_metric:.10f}\")\n",
    "    print(f\"Test {name}: {test_metric:.10f}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
